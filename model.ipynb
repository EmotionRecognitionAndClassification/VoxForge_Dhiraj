{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab79f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5937471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>gender</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Male</td>\n",
       "      <td>1998.167321</td>\n",
       "      <td>3826.669730</td>\n",
       "      <td>0.009790</td>\n",
       "      <td>0.176262</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1542.501380</td>\n",
       "      <td>-166.959571</td>\n",
       "      <td>83.464402</td>\n",
       "      <td>...</td>\n",
       "      <td>15.272717</td>\n",
       "      <td>14.990555</td>\n",
       "      <td>14.630045</td>\n",
       "      <td>29.689999</td>\n",
       "      <td>-0.061420</td>\n",
       "      <td>-0.160284</td>\n",
       "      <td>-0.135946</td>\n",
       "      <td>-0.155298</td>\n",
       "      <td>0.039997</td>\n",
       "      <td>0.034367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1465.930938</td>\n",
       "      <td>2964.031040</td>\n",
       "      <td>0.007881</td>\n",
       "      <td>0.146610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1412.481786</td>\n",
       "      <td>-207.286773</td>\n",
       "      <td>100.778216</td>\n",
       "      <td>...</td>\n",
       "      <td>16.908002</td>\n",
       "      <td>16.401552</td>\n",
       "      <td>18.983034</td>\n",
       "      <td>28.853853</td>\n",
       "      <td>-0.028131</td>\n",
       "      <td>0.010150</td>\n",
       "      <td>0.008699</td>\n",
       "      <td>0.069619</td>\n",
       "      <td>0.015815</td>\n",
       "      <td>0.035503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>1819.720329</td>\n",
       "      <td>3758.195466</td>\n",
       "      <td>0.046866</td>\n",
       "      <td>0.264266</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1822.544171</td>\n",
       "      <td>-108.999489</td>\n",
       "      <td>88.477946</td>\n",
       "      <td>...</td>\n",
       "      <td>15.495786</td>\n",
       "      <td>17.275785</td>\n",
       "      <td>15.989216</td>\n",
       "      <td>31.911001</td>\n",
       "      <td>0.005678</td>\n",
       "      <td>-0.000520</td>\n",
       "      <td>0.042118</td>\n",
       "      <td>-0.065317</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>-0.027502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>1522.400676</td>\n",
       "      <td>3197.733275</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1469.451624</td>\n",
       "      <td>-335.279025</td>\n",
       "      <td>88.043952</td>\n",
       "      <td>...</td>\n",
       "      <td>16.119826</td>\n",
       "      <td>16.877280</td>\n",
       "      <td>20.289910</td>\n",
       "      <td>20.613310</td>\n",
       "      <td>-0.028095</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.065693</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.043067</td>\n",
       "      <td>-0.009393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Male</td>\n",
       "      <td>1313.624225</td>\n",
       "      <td>1879.899364</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.456163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>877.905397</td>\n",
       "      <td>-152.596151</td>\n",
       "      <td>101.331179</td>\n",
       "      <td>...</td>\n",
       "      <td>15.182697</td>\n",
       "      <td>15.116126</td>\n",
       "      <td>15.652804</td>\n",
       "      <td>29.761262</td>\n",
       "      <td>-0.014816</td>\n",
       "      <td>-0.004522</td>\n",
       "      <td>-0.071889</td>\n",
       "      <td>0.013992</td>\n",
       "      <td>0.016066</td>\n",
       "      <td>-0.004285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 174 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 gender            0            1         2         3     4  \\\n",
       "0           0   Male  1998.167321  3826.669730  0.009790  0.176262   3.0   \n",
       "1           1   Male  1465.930938  2964.031040  0.007881  0.146610   0.0   \n",
       "2           2   Male  1819.720329  3758.195466  0.046866  0.264266  19.0   \n",
       "3           3   Male  1522.400676  3197.733275  0.005549  0.054348  32.0   \n",
       "4           4   Male  1313.624225  1879.899364  0.001486  0.456163   0.0   \n",
       "\n",
       "             5           6           7  ...        162        163        164  \\\n",
       "0  1542.501380 -166.959571   83.464402  ...  15.272717  14.990555  14.630045   \n",
       "1  1412.481786 -207.286773  100.778216  ...  16.908002  16.401552  18.983034   \n",
       "2  1822.544171 -108.999489   88.477946  ...  15.495786  17.275785  15.989216   \n",
       "3  1469.451624 -335.279025   88.043952  ...  16.119826  16.877280  20.289910   \n",
       "4   877.905397 -152.596151  101.331179  ...  15.182697  15.116126  15.652804   \n",
       "\n",
       "         165       166       167       168       169       170       171  \n",
       "0  29.689999 -0.061420 -0.160284 -0.135946 -0.155298  0.039997  0.034367  \n",
       "1  28.853853 -0.028131  0.010150  0.008699  0.069619  0.015815  0.035503  \n",
       "2  31.911001  0.005678 -0.000520  0.042118 -0.065317  0.002688 -0.027502  \n",
       "3  20.613310 -0.028095  0.028000  0.065693  0.088900  0.043067 -0.009393  \n",
       "4  29.761262 -0.014816 -0.004522 -0.071889  0.013992  0.016066 -0.004285  \n",
       "\n",
       "[5 rows x 174 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/prem/Desktop/Speech_Corpus/feature.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b0d3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>1998.167321</td>\n",
       "      <td>3826.669730</td>\n",
       "      <td>0.009790</td>\n",
       "      <td>0.176262</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1542.501380</td>\n",
       "      <td>-166.959571</td>\n",
       "      <td>83.464402</td>\n",
       "      <td>25.002596</td>\n",
       "      <td>...</td>\n",
       "      <td>15.272717</td>\n",
       "      <td>14.990555</td>\n",
       "      <td>14.630045</td>\n",
       "      <td>29.689999</td>\n",
       "      <td>-0.061420</td>\n",
       "      <td>-0.160284</td>\n",
       "      <td>-0.135946</td>\n",
       "      <td>-0.155298</td>\n",
       "      <td>0.039997</td>\n",
       "      <td>0.034367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>1465.930938</td>\n",
       "      <td>2964.031040</td>\n",
       "      <td>0.007881</td>\n",
       "      <td>0.146610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1412.481786</td>\n",
       "      <td>-207.286773</td>\n",
       "      <td>100.778216</td>\n",
       "      <td>-3.030930</td>\n",
       "      <td>...</td>\n",
       "      <td>16.908002</td>\n",
       "      <td>16.401552</td>\n",
       "      <td>18.983034</td>\n",
       "      <td>28.853853</td>\n",
       "      <td>-0.028131</td>\n",
       "      <td>0.010150</td>\n",
       "      <td>0.008699</td>\n",
       "      <td>0.069619</td>\n",
       "      <td>0.015815</td>\n",
       "      <td>0.035503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>1819.720329</td>\n",
       "      <td>3758.195466</td>\n",
       "      <td>0.046866</td>\n",
       "      <td>0.264266</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1822.544171</td>\n",
       "      <td>-108.999489</td>\n",
       "      <td>88.477946</td>\n",
       "      <td>1.765099</td>\n",
       "      <td>...</td>\n",
       "      <td>15.495786</td>\n",
       "      <td>17.275785</td>\n",
       "      <td>15.989216</td>\n",
       "      <td>31.911001</td>\n",
       "      <td>0.005678</td>\n",
       "      <td>-0.000520</td>\n",
       "      <td>0.042118</td>\n",
       "      <td>-0.065317</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>-0.027502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>1522.400676</td>\n",
       "      <td>3197.733275</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1469.451624</td>\n",
       "      <td>-335.279025</td>\n",
       "      <td>88.043952</td>\n",
       "      <td>-13.385408</td>\n",
       "      <td>...</td>\n",
       "      <td>16.119826</td>\n",
       "      <td>16.877280</td>\n",
       "      <td>20.289910</td>\n",
       "      <td>20.613310</td>\n",
       "      <td>-0.028095</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.065693</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.043067</td>\n",
       "      <td>-0.009393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>1313.624225</td>\n",
       "      <td>1879.899364</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.456163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>877.905397</td>\n",
       "      <td>-152.596151</td>\n",
       "      <td>101.331179</td>\n",
       "      <td>4.128521</td>\n",
       "      <td>...</td>\n",
       "      <td>15.182697</td>\n",
       "      <td>15.116126</td>\n",
       "      <td>15.652804</td>\n",
       "      <td>29.761262</td>\n",
       "      <td>-0.014816</td>\n",
       "      <td>-0.004522</td>\n",
       "      <td>-0.071889</td>\n",
       "      <td>0.013992</td>\n",
       "      <td>0.016066</td>\n",
       "      <td>-0.004285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 173 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender            0            1         2         3     4            5  \\\n",
       "0   Male  1998.167321  3826.669730  0.009790  0.176262   3.0  1542.501380   \n",
       "1   Male  1465.930938  2964.031040  0.007881  0.146610   0.0  1412.481786   \n",
       "2   Male  1819.720329  3758.195466  0.046866  0.264266  19.0  1822.544171   \n",
       "3   Male  1522.400676  3197.733275  0.005549  0.054348  32.0  1469.451624   \n",
       "4   Male  1313.624225  1879.899364  0.001486  0.456163   0.0   877.905397   \n",
       "\n",
       "            6           7          8  ...        162        163        164  \\\n",
       "0 -166.959571   83.464402  25.002596  ...  15.272717  14.990555  14.630045   \n",
       "1 -207.286773  100.778216  -3.030930  ...  16.908002  16.401552  18.983034   \n",
       "2 -108.999489   88.477946   1.765099  ...  15.495786  17.275785  15.989216   \n",
       "3 -335.279025   88.043952 -13.385408  ...  16.119826  16.877280  20.289910   \n",
       "4 -152.596151  101.331179   4.128521  ...  15.182697  15.116126  15.652804   \n",
       "\n",
       "         165       166       167       168       169       170       171  \n",
       "0  29.689999 -0.061420 -0.160284 -0.135946 -0.155298  0.039997  0.034367  \n",
       "1  28.853853 -0.028131  0.010150  0.008699  0.069619  0.015815  0.035503  \n",
       "2  31.911001  0.005678 -0.000520  0.042118 -0.065317  0.002688 -0.027502  \n",
       "3  20.613310 -0.028095  0.028000  0.065693  0.088900  0.043067 -0.009393  \n",
       "4  29.761262 -0.014816 -0.004522 -0.071889  0.013992  0.016066 -0.004285  \n",
       "\n",
       "[5 rows x 173 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91f5f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8354 entries, 0 to 8353\n",
      "Columns: 173 entries, gender to 171\n",
      "dtypes: float64(172), object(1)\n",
      "memory usage: 11.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6d7a376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8354, 173)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd6a2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='gender', ylabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4641eff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1998.167321</td>\n",
       "      <td>3826.669730</td>\n",
       "      <td>0.009790</td>\n",
       "      <td>0.176262</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1542.501380</td>\n",
       "      <td>-166.959571</td>\n",
       "      <td>83.464402</td>\n",
       "      <td>25.002596</td>\n",
       "      <td>...</td>\n",
       "      <td>15.272717</td>\n",
       "      <td>14.990555</td>\n",
       "      <td>14.630045</td>\n",
       "      <td>29.689999</td>\n",
       "      <td>-0.061420</td>\n",
       "      <td>-0.160284</td>\n",
       "      <td>-0.135946</td>\n",
       "      <td>-0.155298</td>\n",
       "      <td>0.039997</td>\n",
       "      <td>0.034367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1465.930938</td>\n",
       "      <td>2964.031040</td>\n",
       "      <td>0.007881</td>\n",
       "      <td>0.146610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1412.481786</td>\n",
       "      <td>-207.286773</td>\n",
       "      <td>100.778216</td>\n",
       "      <td>-3.030930</td>\n",
       "      <td>...</td>\n",
       "      <td>16.908002</td>\n",
       "      <td>16.401552</td>\n",
       "      <td>18.983034</td>\n",
       "      <td>28.853853</td>\n",
       "      <td>-0.028131</td>\n",
       "      <td>0.010150</td>\n",
       "      <td>0.008699</td>\n",
       "      <td>0.069619</td>\n",
       "      <td>0.015815</td>\n",
       "      <td>0.035503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1819.720329</td>\n",
       "      <td>3758.195466</td>\n",
       "      <td>0.046866</td>\n",
       "      <td>0.264266</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1822.544171</td>\n",
       "      <td>-108.999489</td>\n",
       "      <td>88.477946</td>\n",
       "      <td>1.765099</td>\n",
       "      <td>...</td>\n",
       "      <td>15.495786</td>\n",
       "      <td>17.275785</td>\n",
       "      <td>15.989216</td>\n",
       "      <td>31.911001</td>\n",
       "      <td>0.005678</td>\n",
       "      <td>-0.000520</td>\n",
       "      <td>0.042118</td>\n",
       "      <td>-0.065317</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>-0.027502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1522.400676</td>\n",
       "      <td>3197.733275</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1469.451624</td>\n",
       "      <td>-335.279025</td>\n",
       "      <td>88.043952</td>\n",
       "      <td>-13.385408</td>\n",
       "      <td>...</td>\n",
       "      <td>16.119826</td>\n",
       "      <td>16.877280</td>\n",
       "      <td>20.289910</td>\n",
       "      <td>20.613310</td>\n",
       "      <td>-0.028095</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.065693</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.043067</td>\n",
       "      <td>-0.009393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1313.624225</td>\n",
       "      <td>1879.899364</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.456163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>877.905397</td>\n",
       "      <td>-152.596151</td>\n",
       "      <td>101.331179</td>\n",
       "      <td>4.128521</td>\n",
       "      <td>...</td>\n",
       "      <td>15.182697</td>\n",
       "      <td>15.116126</td>\n",
       "      <td>15.652804</td>\n",
       "      <td>29.761262</td>\n",
       "      <td>-0.014816</td>\n",
       "      <td>-0.004522</td>\n",
       "      <td>-0.071889</td>\n",
       "      <td>0.013992</td>\n",
       "      <td>0.016066</td>\n",
       "      <td>-0.004285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 173 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender            0            1         2         3     4            5  \\\n",
       "0       1  1998.167321  3826.669730  0.009790  0.176262   3.0  1542.501380   \n",
       "1       1  1465.930938  2964.031040  0.007881  0.146610   0.0  1412.481786   \n",
       "2       1  1819.720329  3758.195466  0.046866  0.264266  19.0  1822.544171   \n",
       "3       1  1522.400676  3197.733275  0.005549  0.054348  32.0  1469.451624   \n",
       "4       1  1313.624225  1879.899364  0.001486  0.456163   0.0   877.905397   \n",
       "\n",
       "            6           7          8  ...        162        163        164  \\\n",
       "0 -166.959571   83.464402  25.002596  ...  15.272717  14.990555  14.630045   \n",
       "1 -207.286773  100.778216  -3.030930  ...  16.908002  16.401552  18.983034   \n",
       "2 -108.999489   88.477946   1.765099  ...  15.495786  17.275785  15.989216   \n",
       "3 -335.279025   88.043952 -13.385408  ...  16.119826  16.877280  20.289910   \n",
       "4 -152.596151  101.331179   4.128521  ...  15.182697  15.116126  15.652804   \n",
       "\n",
       "         165       166       167       168       169       170       171  \n",
       "0  29.689999 -0.061420 -0.160284 -0.135946 -0.155298  0.039997  0.034367  \n",
       "1  28.853853 -0.028131  0.010150  0.008699  0.069619  0.015815  0.035503  \n",
       "2  31.911001  0.005678 -0.000520  0.042118 -0.065317  0.002688 -0.027502  \n",
       "3  20.613310 -0.028095  0.028000  0.065693  0.088900  0.043067 -0.009393  \n",
       "4  29.761262 -0.014816 -0.004522 -0.071889  0.013992  0.016066 -0.004285  \n",
       "\n",
       "[5 rows x 173 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gender'].unique()\n",
    "gender = ['Male', 'Female']\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "le = LabelEncoder()\n",
    "df['gender']= le.fit_transform(df['gender'].astype(str))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29cc7d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f4b7811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8354, 172)\n",
      "(8354,)\n"
     ]
    }
   ],
   "source": [
    "x = df.drop('gender', axis = 1)\n",
    "y = df['gender']\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc5909b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.752593</td>\n",
       "      <td>0.577458</td>\n",
       "      <td>0.028190</td>\n",
       "      <td>0.074934</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>0.357258</td>\n",
       "      <td>0.705326</td>\n",
       "      <td>0.544426</td>\n",
       "      <td>0.749863</td>\n",
       "      <td>0.550610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127597</td>\n",
       "      <td>0.091820</td>\n",
       "      <td>0.034299</td>\n",
       "      <td>0.602277</td>\n",
       "      <td>0.368806</td>\n",
       "      <td>0.216183</td>\n",
       "      <td>0.181971</td>\n",
       "      <td>0.318891</td>\n",
       "      <td>0.510266</td>\n",
       "      <td>0.596919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.464790</td>\n",
       "      <td>0.444985</td>\n",
       "      <td>0.022689</td>\n",
       "      <td>0.062321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324068</td>\n",
       "      <td>0.660315</td>\n",
       "      <td>0.640475</td>\n",
       "      <td>0.521267</td>\n",
       "      <td>0.761624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227348</td>\n",
       "      <td>0.176098</td>\n",
       "      <td>0.223503</td>\n",
       "      <td>0.573157</td>\n",
       "      <td>0.421728</td>\n",
       "      <td>0.425012</td>\n",
       "      <td>0.338256</td>\n",
       "      <td>0.578928</td>\n",
       "      <td>0.444943</td>\n",
       "      <td>0.600039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.656099</td>\n",
       "      <td>0.566943</td>\n",
       "      <td>0.135074</td>\n",
       "      <td>0.112367</td>\n",
       "      <td>0.215909</td>\n",
       "      <td>0.428746</td>\n",
       "      <td>0.770018</td>\n",
       "      <td>0.572239</td>\n",
       "      <td>0.560375</td>\n",
       "      <td>0.454430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141204</td>\n",
       "      <td>0.228315</td>\n",
       "      <td>0.093376</td>\n",
       "      <td>0.679627</td>\n",
       "      <td>0.475476</td>\n",
       "      <td>0.411938</td>\n",
       "      <td>0.374364</td>\n",
       "      <td>0.422922</td>\n",
       "      <td>0.409482</td>\n",
       "      <td>0.426998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.495325</td>\n",
       "      <td>0.480874</td>\n",
       "      <td>0.015965</td>\n",
       "      <td>0.023077</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.338611</td>\n",
       "      <td>0.517456</td>\n",
       "      <td>0.569831</td>\n",
       "      <td>0.436832</td>\n",
       "      <td>0.850436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179270</td>\n",
       "      <td>0.204513</td>\n",
       "      <td>0.280306</td>\n",
       "      <td>0.286170</td>\n",
       "      <td>0.421785</td>\n",
       "      <td>0.446882</td>\n",
       "      <td>0.399836</td>\n",
       "      <td>0.601219</td>\n",
       "      <td>0.518560</td>\n",
       "      <td>0.476733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.382431</td>\n",
       "      <td>0.278497</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>0.193992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187603</td>\n",
       "      <td>0.721358</td>\n",
       "      <td>0.643543</td>\n",
       "      <td>0.579648</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122106</td>\n",
       "      <td>0.099320</td>\n",
       "      <td>0.078754</td>\n",
       "      <td>0.604759</td>\n",
       "      <td>0.442895</td>\n",
       "      <td>0.407035</td>\n",
       "      <td>0.251183</td>\n",
       "      <td>0.514615</td>\n",
       "      <td>0.445620</td>\n",
       "      <td>0.490761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.752593  0.577458  0.028190  0.074934  0.034091  0.357258  0.705326   \n",
       "1  0.464790  0.444985  0.022689  0.062321  0.000000  0.324068  0.660315   \n",
       "2  0.656099  0.566943  0.135074  0.112367  0.215909  0.428746  0.770018   \n",
       "3  0.495325  0.480874  0.015965  0.023077  0.363636  0.338611  0.517456   \n",
       "4  0.382431  0.278497  0.004252  0.193992  0.000000  0.187603  0.721358   \n",
       "\n",
       "          7         8         9  ...       162       163       164       165  \\\n",
       "0  0.544426  0.749863  0.550610  ...  0.127597  0.091820  0.034299  0.602277   \n",
       "1  0.640475  0.521267  0.761624  ...  0.227348  0.176098  0.223503  0.573157   \n",
       "2  0.572239  0.560375  0.454430  ...  0.141204  0.228315  0.093376  0.679627   \n",
       "3  0.569831  0.436832  0.850436  ...  0.179270  0.204513  0.280306  0.286170   \n",
       "4  0.643543  0.579648  0.638889  ...  0.122106  0.099320  0.078754  0.604759   \n",
       "\n",
       "        166       167       168       169       170       171  \n",
       "0  0.368806  0.216183  0.181971  0.318891  0.510266  0.596919  \n",
       "1  0.421728  0.425012  0.338256  0.578928  0.444943  0.600039  \n",
       "2  0.475476  0.411938  0.374364  0.422922  0.409482  0.426998  \n",
       "3  0.421785  0.446882  0.399836  0.601219  0.518560  0.476733  \n",
       "4  0.442895  0.407035  0.251183  0.514615  0.445620  0.490761  \n",
       "\n",
       "[5 rows x 172 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range =(0,1))\n",
    "X = scaler.fit_transform(x)\n",
    "X = pd.DataFrame(X, columns = x.columns)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "456c5569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6683, 172)\n",
      "(1671, 172)\n",
      "(6683,)\n",
      "(1671,)\n"
     ]
    }
   ],
   "source": [
    "#Splitting Train & Testing Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d9d7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9faea064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score using LR model is :  81.03%\n",
      "\n",
      "\n",
      "confusion matrix using LR model is : \n",
      " [[663 166]\n",
      " [151 691]]\n",
      "\n",
      "\n",
      "classification using LR model is :\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.81       829\n",
      "           1       0.81      0.82      0.81       842\n",
      "\n",
      "    accuracy                           0.81      1671\n",
      "   macro avg       0.81      0.81      0.81      1671\n",
      "weighted avg       0.81      0.81      0.81      1671\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression model\n",
    "LR_model = LogisticRegression()\n",
    "LR_model.fit(x_train, y_train)\n",
    "LR_y_pred = LR_model.predict(x_test)\n",
    "\n",
    "Accuracy_LR_test = \"{0:.2f}%\".format(accuracy_score(y_test,LR_y_pred)*100)\n",
    "print(\"accuracy score using LR model is : \", Accuracy_LR_test)\n",
    "print('\\n')\n",
    "print(\"confusion matrix using LR model is : \\n\", confusion_matrix(y_test, LR_y_pred))\n",
    "print('\\n')\n",
    "print(\"classification using LR model is :\\n \", classification_report(y_test, LR_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4cdc40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score using DT model is :  85.52%\n",
      "\n",
      "\n",
      "confusion matrix using DT model is : \n",
      " [[710 119]\n",
      " [123 719]]\n",
      "\n",
      "\n",
      "classification using DT model is :\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.85       829\n",
      "           1       0.86      0.85      0.86       842\n",
      "\n",
      "    accuracy                           0.86      1671\n",
      "   macro avg       0.86      0.86      0.86      1671\n",
      "weighted avg       0.86      0.86      0.86      1671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision tree model\n",
    "DT_model = DecisionTreeClassifier(criterion = 'entropy')\n",
    "DT_model.fit(x_train, y_train)\n",
    "DT_y_pred = DT_model.predict(x_test)\n",
    "\n",
    "Accuracy_DT_test = \"{0:.2f}%\".format(accuracy_score(y_test,DT_y_pred)*100)\n",
    "print(\"accuracy score using DT model is : \", Accuracy_DT_test)\n",
    "print('\\n')\n",
    "print(\"confusion matrix using DT model is : \\n\", confusion_matrix(y_test, DT_y_pred))\n",
    "print('\\n')\n",
    "print(\"classification using DT model is :\\n \", classification_report(y_test, DT_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f098eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score using SVM model is :  81.69%\n",
      "\n",
      "\n",
      "confusion matrix using SVM model is :\n",
      "  [[667 162]\n",
      " [144 698]]\n",
      "\n",
      "\n",
      "classification using SVM model is : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81       829\n",
      "           1       0.81      0.83      0.82       842\n",
      "\n",
      "    accuracy                           0.82      1671\n",
      "   macro avg       0.82      0.82      0.82      1671\n",
      "weighted avg       0.82      0.82      0.82      1671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using SVM\n",
    "SVM_model = SVC(kernel = 'linear')\n",
    "SVM_model.fit(x_train, y_train)\n",
    "SVM_y_pred = SVM_model.predict(x_test)\n",
    "\n",
    "Accuracy_SVM_test = \"{0:.2f}%\".format(accuracy_score(y_test,SVM_y_pred)*100)\n",
    "print(\"accuracy score using SVM model is : \", Accuracy_SVM_test)\n",
    "print('\\n')\n",
    "print(\"confusion matrix using SVM model is :\\n \", confusion_matrix(y_test, SVM_y_pred))\n",
    "print('\\n')\n",
    "print(\"classification using SVM model is : \\n\", classification_report(y_test, SVM_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c295ac5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score using RF model is :  92.88%\n",
      "\n",
      "\n",
      "confusion matrix using RF model is :\n",
      "  [[769  60]\n",
      " [ 59 783]]\n",
      "\n",
      "\n",
      "classification using RF model is :\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       829\n",
      "           1       0.93      0.93      0.93       842\n",
      "\n",
      "    accuracy                           0.93      1671\n",
      "   macro avg       0.93      0.93      0.93      1671\n",
      "weighted avg       0.93      0.93      0.93      1671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using random forest \n",
    "RF_model = RandomForestClassifier(n_estimators = 100, criterion = 'entropy')\n",
    "RF_model.fit(x_train, y_train)\n",
    "RF_y_pred = RF_model.predict(x_test)\n",
    "\n",
    "Accuracy_RF_test = \"{0:.2f}%\".format(accuracy_score(y_test,RF_y_pred)*100)\n",
    "print(\"accuracy score using RF model is : \", Accuracy_RF_test)\n",
    "print('\\n')\n",
    "print(\"confusion matrix using RF model is :\\n \", confusion_matrix(y_test, RF_y_pred))\n",
    "print('\\n')\n",
    "print(\"classification using RF model is :\\n \", classification_report(y_test, RF_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a28ceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score using LR model is :  90.54%\n",
      "\n",
      "\n",
      "confusion matrix using LR model is :\n",
      "  [[813  16]\n",
      " [142 700]]\n",
      "\n",
      "\n",
      "classification using LR model is :\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91       829\n",
      "           1       0.98      0.83      0.90       842\n",
      "\n",
      "    accuracy                           0.91      1671\n",
      "   macro avg       0.91      0.91      0.91      1671\n",
      "weighted avg       0.91      0.91      0.90      1671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using KNN\n",
    "KNN_model = KNeighborsClassifier(n_neighbors = 7)\n",
    "KNN_model.fit(x_train, y_train)\n",
    "KNN_y_pred = KNN_model.predict(x_test)\n",
    "\n",
    "Accuracy_KNN_test = \"{0:.2f}%\".format(accuracy_score(y_test,KNN_y_pred)*100)\n",
    "print(\"accuracy score using LR model is : \", Accuracy_KNN_test)\n",
    "print('\\n')\n",
    "print(\"confusion matrix using LR model is :\\n \", confusion_matrix(y_test, KNN_y_pred))\n",
    "print('\\n')\n",
    "print(\"classification using LR model is :\\n \", classification_report(y_test, KNN_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a482681a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score using XGB model is :  95.45%\n",
      "\n",
      "\n",
      "confusion matrix using XGB model is :\n",
      "  [[788  41]\n",
      " [ 35 807]]\n",
      "\n",
      "\n",
      "classification using XGB model is :\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95       829\n",
      "           1       0.95      0.96      0.96       842\n",
      "\n",
      "    accuracy                           0.95      1671\n",
      "   macro avg       0.95      0.95      0.95      1671\n",
      "weighted avg       0.95      0.95      0.95      1671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using XGB\n",
    "XGB_model = XGBClassifier(learning_rate=0.8,random_state=0)\n",
    "XGB_model.fit(x_train,y_train)\n",
    "XGB_y_pred = XGB_model.predict(x_test)\n",
    "\n",
    "Accuracy_XGB_test = \"{0:.2f}%\".format(accuracy_score(y_test,XGB_y_pred)*100)\n",
    "\n",
    "print(\"accuracy score using XGB model is : \", Accuracy_XGB_test)\n",
    "print('\\n')\n",
    "print(\"confusion matrix using XGB model is :\\n \", confusion_matrix(y_test, XGB_y_pred))\n",
    "print('\\n')\n",
    "print(\"classification using XGB model is :\\n \", classification_report(y_test, XGB_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15e24e94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score using MLP model is :  92.40%\n",
      "\n",
      "\n",
      "confusion matrix using MLP model is :\n",
      "  [[774  55]\n",
      " [ 72 770]]\n",
      "\n",
      "\n",
      "classification using MLP model is :\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92       829\n",
      "           1       0.93      0.91      0.92       842\n",
      "\n",
      "    accuracy                           0.92      1671\n",
      "   macro avg       0.92      0.92      0.92      1671\n",
      "weighted avg       0.92      0.92      0.92      1671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using MLP\n",
    "MLP_model = MLPClassifier(alpha=0.01,activation='relu', batch_size=64,hidden_layer_sizes=(450,), max_iter=550)\n",
    "MLP_model.fit(x_train,y_train)\n",
    "MLP_y_pred = MLP_model.predict(x_test)\n",
    "\n",
    "Accuracy_MLP_test = \"{0:.2f}%\".format(accuracy_score(y_test,MLP_y_pred)*100)\n",
    "print(\"accuracy score using MLP model is : \", Accuracy_MLP_test)\n",
    "print('\\n')\n",
    "print(\"confusion matrix using MLP model is :\\n \", confusion_matrix(y_test, MLP_y_pred))\n",
    "print('\\n')\n",
    "print(\"classification using MLP model is :\\n \", classification_report(y_test, MLP_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28502d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xlstm_train = x_train.values.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "ylstm_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6818322a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM RNN model ...\n",
      "Compiling ...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 6683, 128)         154112    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                20608     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6683)              220539    \n",
      "=================================================================\n",
      "Total params: 395,259\n",
      "Trainable params: 395,259\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training ...\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 6683, 172) for input KerasTensor(type_spec=TensorSpec(shape=(None, 6683, 172), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 172).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 6683, 172) for input KerasTensor(type_spec=TensorSpec(shape=(None, 6683, 172), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 172).\n",
      "53/53 [==============================] - 10s 41ms/step - loss: 7.9986 - accuracy: 0.4874\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 2s 38ms/step - loss: 3.8121 - accuracy: 0.5010\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 2s 34ms/step - loss: 1.4735 - accuracy: 0.5010\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 0.9145 - accuracy: 0.5010\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 2s 41ms/step - loss: 0.8045 - accuracy: 0.5014\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 2s 36ms/step - loss: 0.7644 - accuracy: 0.4998\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 2s 30ms/step - loss: 0.7440 - accuracy: 0.4971\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 2s 32ms/step - loss: 0.7319 - accuracy: 0.4953\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 2s 40ms/step - loss: 0.7239 - accuracy: 0.4963\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 2s 35ms/step - loss: 0.7184 - accuracy: 0.4959\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 0.7144 - accuracy: 0.4969\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 0.7113 - accuracy: 0.5047\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 0.7089 - accuracy: 0.4920\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 2s 41ms/step - loss: 0.7069 - accuracy: 0.4923\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 0.7053 - accuracy: 0.4965\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.7041 - accuracy: 0.4948\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.7030 - accuracy: 0.5004\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.7023 - accuracy: 0.4845\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 2s 34ms/step - loss: 0.7015 - accuracy: 0.5016\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 2s 30ms/step - loss: 0.7006 - accuracy: 0.5010\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.7000 - accuracy: 0.5055\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6993 - accuracy: 0.5050\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 0.6996 - accuracy: 0.4941\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 0.6991 - accuracy: 0.4950\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6984 - accuracy: 0.5029\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6980 - accuracy: 0.4960\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6977 - accuracy: 0.4980\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6975 - accuracy: 0.4956\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6972 - accuracy: 0.4869\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6976 - accuracy: 0.4900\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6970 - accuracy: 0.5023\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6967 - accuracy: 0.5032\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6963 - accuracy: 0.4924\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6964 - accuracy: 0.4841\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6959 - accuracy: 0.5068\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6961 - accuracy: 0.4936\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6958 - accuracy: 0.4981\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6958 - accuracy: 0.4977\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6958 - accuracy: 0.4878\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6954 - accuracy: 0.4956\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6954 - accuracy: 0.4906\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6957 - accuracy: 0.4896\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6952 - accuracy: 0.4938\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6952 - accuracy: 0.5025\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 2s 30ms/step - loss: 0.6951 - accuracy: 0.5049\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6949 - accuracy: 0.5071\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6951 - accuracy: 0.5016\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.6959 - accuracy: 0.4923\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 2s 35ms/step - loss: 0.6949 - accuracy: 0.5001\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 2s 36ms/step - loss: 0.6951 - accuracy: 0.4905\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 2s 34ms/step - loss: 0.6951 - accuracy: 0.4995\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 0.6946 - accuracy: 0.5028\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 2s 30ms/step - loss: 0.6946 - accuracy: 0.4992\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6948 - accuracy: 0.4975\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6949 - accuracy: 0.4935\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6951 - accuracy: 0.4972\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 2s 32ms/step - loss: 0.6948 - accuracy: 0.4944\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 2s 32ms/step - loss: 0.6949 - accuracy: 0.4939\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 2s 32ms/step - loss: 0.6949 - accuracy: 0.4950\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 0.6944 - accuracy: 0.4914\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 0.6944 - accuracy: 0.4963\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 0.6944 - accuracy: 0.4947\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 2s 43ms/step - loss: 0.6945 - accuracy: 0.4902\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 2s 36ms/step - loss: 0.6948 - accuracy: 0.4912\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 2s 37ms/step - loss: 0.6942 - accuracy: 0.5058\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 0.6947 - accuracy: 0.4977\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6943 - accuracy: 0.4912\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6943 - accuracy: 0.5005\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6945 - accuracy: 0.5002\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6945 - accuracy: 0.4986\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6944 - accuracy: 0.4965\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6941 - accuracy: 0.4984\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6940 - accuracy: 0.4968\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6941 - accuracy: 0.4978\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6944 - accuracy: 0.4938\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6948 - accuracy: 0.5016\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6944 - accuracy: 0.4851\n",
      "Epoch 78/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6946 - accuracy: 0.4998\n",
      "Epoch 79/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.6947 - accuracy: 0.4892\n",
      "Epoch 80/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.6942 - accuracy: 0.4938\n",
      "Epoch 81/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6946 - accuracy: 0.4912\n",
      "Epoch 82/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6945 - accuracy: 0.4906\n",
      "Epoch 83/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6945 - accuracy: 0.4981\n",
      "Epoch 84/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6941 - accuracy: 0.4981\n",
      "Epoch 85/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6937 - accuracy: 0.5149\n",
      "Epoch 86/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.6901 - accuracy: 0.5414\n",
      "Epoch 87/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.6585 - accuracy: 0.6548\n",
      "Epoch 88/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.5994 - accuracy: 0.7197\n",
      "Epoch 89/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.5692 - accuracy: 0.7287\n",
      "Epoch 90/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.5430 - accuracy: 0.7456\n",
      "Epoch 91/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.5284 - accuracy: 0.7495\n",
      "Epoch 92/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.5113 - accuracy: 0.7624\n",
      "Epoch 93/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.5064 - accuracy: 0.7574\n",
      "Epoch 94/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.4940 - accuracy: 0.7708\n",
      "Epoch 95/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.4835 - accuracy: 0.7775\n",
      "Epoch 96/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.4780 - accuracy: 0.7812\n",
      "Epoch 97/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.4823 - accuracy: 0.7727\n",
      "Epoch 98/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.4827 - accuracy: 0.7724\n",
      "Epoch 99/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.4718 - accuracy: 0.7814\n",
      "Epoch 100/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.4633 - accuracy: 0.7905\n",
      "Epoch 101/200\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 0.4630 - accuracy: 0.7896\n",
      "Epoch 102/200\n",
      "53/53 [==============================] - 2s 30ms/step - loss: 0.4617 - accuracy: 0.7850\n",
      "Epoch 103/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.4598 - accuracy: 0.7893\n",
      "Epoch 104/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.4579 - accuracy: 0.7860\n",
      "Epoch 105/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.4491 - accuracy: 0.7972\n",
      "Epoch 106/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.4451 - accuracy: 0.7959\n",
      "Epoch 107/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.4495 - accuracy: 0.7889\n",
      "Epoch 108/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.4513 - accuracy: 0.7935\n",
      "Epoch 109/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.4391 - accuracy: 0.7990\n",
      "Epoch 110/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.4434 - accuracy: 0.7971\n",
      "Epoch 111/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.4300 - accuracy: 0.8023\n",
      "Epoch 112/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.4377 - accuracy: 0.7978\n",
      "Epoch 113/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.4327 - accuracy: 0.8022\n",
      "Epoch 114/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.4289 - accuracy: 0.8080\n",
      "Epoch 115/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.4233 - accuracy: 0.8101\n",
      "Epoch 116/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.4202 - accuracy: 0.8079\n",
      "Epoch 117/200\n",
      "53/53 [==============================] - 2s 30ms/step - loss: 0.4169 - accuracy: 0.8113\n",
      "Epoch 118/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.4099 - accuracy: 0.8189\n",
      "Epoch 119/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.4198 - accuracy: 0.8085\n",
      "Epoch 120/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.4073 - accuracy: 0.8143\n",
      "Epoch 121/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.4061 - accuracy: 0.8203\n",
      "Epoch 122/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.4088 - accuracy: 0.8173\n",
      "Epoch 123/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.4005 - accuracy: 0.8179\n",
      "Epoch 124/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.4069 - accuracy: 0.8171\n",
      "Epoch 125/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.4118 - accuracy: 0.8157\n",
      "Epoch 126/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.4004 - accuracy: 0.8198\n",
      "Epoch 127/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.4001 - accuracy: 0.8206\n",
      "Epoch 128/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.3940 - accuracy: 0.8272\n",
      "Epoch 129/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.3993 - accuracy: 0.8212\n",
      "Epoch 130/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.4001 - accuracy: 0.8171\n",
      "Epoch 131/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.3906 - accuracy: 0.8266\n",
      "Epoch 132/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.3866 - accuracy: 0.8281\n",
      "Epoch 133/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.3896 - accuracy: 0.8239\n",
      "Epoch 134/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.3836 - accuracy: 0.8285\n",
      "Epoch 135/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.3982 - accuracy: 0.8215\n",
      "Epoch 136/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.3850 - accuracy: 0.8282\n",
      "Epoch 137/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.3804 - accuracy: 0.8312\n",
      "Epoch 138/200\n",
      "53/53 [==============================] - 2s 34ms/step - loss: 0.3826 - accuracy: 0.8293\n",
      "Epoch 139/200\n",
      "53/53 [==============================] - 2s 32ms/step - loss: 0.3813 - accuracy: 0.8279\n",
      "Epoch 140/200\n",
      "53/53 [==============================] - 2s 34ms/step - loss: 0.3836 - accuracy: 0.8288\n",
      "Epoch 141/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.3826 - accuracy: 0.8288\n",
      "Epoch 142/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.3858 - accuracy: 0.8315\n",
      "Epoch 143/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.3790 - accuracy: 0.8294\n",
      "Epoch 144/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.3775 - accuracy: 0.8305\n",
      "Epoch 145/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.3742 - accuracy: 0.8353\n",
      "Epoch 146/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.3717 - accuracy: 0.8366\n",
      "Epoch 147/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.3745 - accuracy: 0.8360\n",
      "Epoch 148/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3695 - accuracy: 0.8387\n",
      "Epoch 149/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3689 - accuracy: 0.8357\n",
      "Epoch 150/200\n",
      "53/53 [==============================] - 1s 27ms/step - loss: 0.3608 - accuracy: 0.8426\n",
      "Epoch 151/200\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 0.3661 - accuracy: 0.8429\n",
      "Epoch 152/200\n",
      "53/53 [==============================] - 2s 30ms/step - loss: 0.3670 - accuracy: 0.8399\n",
      "Epoch 153/200\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 0.3728 - accuracy: 0.8303\n",
      "Epoch 154/200\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 0.3581 - accuracy: 0.8433\n",
      "Epoch 155/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3652 - accuracy: 0.8381\n",
      "Epoch 156/200\n",
      "53/53 [==============================] - 1s 27ms/step - loss: 0.3625 - accuracy: 0.8442\n",
      "Epoch 157/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.3646 - accuracy: 0.8372\n",
      "Epoch 158/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3574 - accuracy: 0.8430\n",
      "Epoch 159/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.3567 - accuracy: 0.8390\n",
      "Epoch 160/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.3579 - accuracy: 0.8424\n",
      "Epoch 161/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3607 - accuracy: 0.8435\n",
      "Epoch 162/200\n",
      "53/53 [==============================] - 1s 27ms/step - loss: 0.3559 - accuracy: 0.8463\n",
      "Epoch 163/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3615 - accuracy: 0.8460\n",
      "Epoch 164/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.3756 - accuracy: 0.8288\n",
      "Epoch 165/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3461 - accuracy: 0.8517\n",
      "Epoch 166/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3525 - accuracy: 0.8451\n",
      "Epoch 167/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3481 - accuracy: 0.8459\n",
      "Epoch 168/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3466 - accuracy: 0.8519\n",
      "Epoch 169/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3422 - accuracy: 0.8505\n",
      "Epoch 170/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3486 - accuracy: 0.8460\n",
      "Epoch 171/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3552 - accuracy: 0.8460\n",
      "Epoch 172/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3489 - accuracy: 0.8507\n",
      "Epoch 173/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3425 - accuracy: 0.8540\n",
      "Epoch 174/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3294 - accuracy: 0.8590\n",
      "Epoch 175/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3407 - accuracy: 0.8498\n",
      "Epoch 176/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3483 - accuracy: 0.8508\n",
      "Epoch 177/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.3377 - accuracy: 0.8549\n",
      "Epoch 178/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3497 - accuracy: 0.8442\n",
      "Epoch 179/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3379 - accuracy: 0.8534\n",
      "Epoch 180/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3365 - accuracy: 0.8544\n",
      "Epoch 181/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3508 - accuracy: 0.8411\n",
      "Epoch 182/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3402 - accuracy: 0.8516\n",
      "Epoch 183/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3367 - accuracy: 0.8544\n",
      "Epoch 184/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3409 - accuracy: 0.8489\n",
      "Epoch 185/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3403 - accuracy: 0.8537\n",
      "Epoch 186/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3249 - accuracy: 0.8596\n",
      "Epoch 187/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3410 - accuracy: 0.8562\n",
      "Epoch 188/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3368 - accuracy: 0.8490\n",
      "Epoch 189/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3272 - accuracy: 0.8577\n",
      "Epoch 190/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3376 - accuracy: 0.8552\n",
      "Epoch 191/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.3292 - accuracy: 0.8616\n",
      "Epoch 192/200\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 0.3274 - accuracy: 0.8595\n",
      "Epoch 193/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.3235 - accuracy: 0.8629\n",
      "Epoch 194/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.3259 - accuracy: 0.8616\n",
      "Epoch 195/200\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.3245 - accuracy: 0.8623\n",
      "Epoch 196/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3323 - accuracy: 0.8561\n",
      "Epoch 197/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3169 - accuracy: 0.8662\n",
      "Epoch 198/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3183 - accuracy: 0.8647\n",
      "Epoch 199/200\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.3194 - accuracy: 0.8634\n",
      "Epoch 200/200\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.3150 - accuracy: 0.8655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f81e62c86d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keras\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "input_shape = (x_train.shape[0], x_train.shape[1])\n",
    "print(\"Build LSTM RNN model ...\")\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=128, dropout=0.05, recurrent_dropout=0.35, return_sequences=True, input_shape=input_shape))\n",
    "model.add(LSTM(units=32,  dropout=0.05, recurrent_dropout=0.35, return_sequences=False))\n",
    "model.add(Dense(units=y_train.shape[0], activation=\"softmax\"))\n",
    "\n",
    "print(\"Compiling ...\")\n",
    "# Keras optimizer defaults:\n",
    "# Adam   : lr=0.001, beta_1=0.9,  beta_2=0.999, epsilon=1e-8, decay=0.\n",
    "# RMSprop: lr=0.001, rho=0.9,                   epsilon=1e-8, decay=0.\n",
    "# SGD    : lr=0.01,  momentum=0.,                             decay=0.\n",
    "opt = Adam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "print(\"Training ...\")\n",
    "batch_size = 128  # num of training examples per minibatch\n",
    "num_epochs = 200\n",
    "\n",
    "model.fit(Xlstm_train, ylstm_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f13e51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing ...\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 6683, 172) for input KerasTensor(type_spec=TensorSpec(shape=(None, 6683, 172), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 172).\n",
      "14/14 [==============================] - 1s 17ms/step - loss: 0.3071 - accuracy: 0.8743\n"
     ]
    }
   ],
   "source": [
    "x_test = x_test.values.reshape(x_test.shape[0], 1, x_test.shape[1])\n",
    "y_test = np.array(y_test)\n",
    "print(\"\\nTesting ...\")\n",
    "score, accuracy = model.evaluate(\n",
    "    x_test, y_test, batch_size=batch_size, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f6dbbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score using lstm model is :  87.43%\n"
     ]
    }
   ],
   "source": [
    "Accuracy_LSTM_test = \"{0:.2f}%\".format(accuracy*100)\n",
    "print(\"accuracy score using lstm model is : \", Accuracy_LSTM_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6481348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>81.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>81.69%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>85.52%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP</td>\n",
       "      <td>92.40%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN</td>\n",
       "      <td>90.54%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>92.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGB</td>\n",
       "      <td>95.45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>87.43%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model Accuracy\n",
       "0  Logistic Regression   81.03%\n",
       "1                  SVM   81.69%\n",
       "2        Decision Tree   85.52%\n",
       "3                  MLP   92.40%\n",
       "4                  KNN   90.54%\n",
       "5        Random Forest   92.88%\n",
       "6                  XGB   95.45%\n",
       "7                 LSTM   87.43%"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table=pd.DataFrame()\n",
    "table[\"Model\"]=('Logistic Regression','SVM','Decision Tree','MLP','KNN','Random Forest','XGB','LSTM')\n",
    "table[\"Accuracy\"]=(Accuracy_LR_test,Accuracy_SVM_test,Accuracy_DT_test,Accuracy_MLP_test,Accuracy_KNN_test,Accuracy_RF_test,Accuracy_XGB_test,Accuracy_LSTM_test)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a448faa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
