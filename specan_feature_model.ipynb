{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a38acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import librosa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34656eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>freq.median</th>\n",
       "      <th>freq.Q25</th>\n",
       "      <th>freq.Q75</th>\n",
       "      <th>freq.IQR</th>\n",
       "      <th>time.median</th>\n",
       "      <th>time.Q25</th>\n",
       "      <th>time.Q75</th>\n",
       "      <th>...</th>\n",
       "      <th>h1_freq</th>\n",
       "      <th>h1_width</th>\n",
       "      <th>h2_freq</th>\n",
       "      <th>h2_width</th>\n",
       "      <th>h3_freq</th>\n",
       "      <th>h3_width</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.303805</td>\n",
       "      <td>1.431251</td>\n",
       "      <td>0.660819</td>\n",
       "      <td>0.436412</td>\n",
       "      <td>1.444241</td>\n",
       "      <td>1.007829</td>\n",
       "      <td>1.596774</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>2.580645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571780</td>\n",
       "      <td>0.125789</td>\n",
       "      <td>1.694041</td>\n",
       "      <td>0.289940</td>\n",
       "      <td>2.757494</td>\n",
       "      <td>0.222735</td>\n",
       "      <td>0.213001</td>\n",
       "      <td>0.180885</td>\n",
       "      <td>0.248084</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.211847</td>\n",
       "      <td>2.047427</td>\n",
       "      <td>1.340397</td>\n",
       "      <td>0.494357</td>\n",
       "      <td>4.084861</td>\n",
       "      <td>3.590504</td>\n",
       "      <td>1.151351</td>\n",
       "      <td>0.518919</td>\n",
       "      <td>1.718919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520256</td>\n",
       "      <td>0.125211</td>\n",
       "      <td>1.708941</td>\n",
       "      <td>0.152071</td>\n",
       "      <td>2.954897</td>\n",
       "      <td>0.214618</td>\n",
       "      <td>0.183730</td>\n",
       "      <td>0.158824</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.851073</td>\n",
       "      <td>1.781392</td>\n",
       "      <td>1.073231</td>\n",
       "      <td>0.392609</td>\n",
       "      <td>3.262360</td>\n",
       "      <td>2.869751</td>\n",
       "      <td>1.492788</td>\n",
       "      <td>0.697716</td>\n",
       "      <td>2.093149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.707695</td>\n",
       "      <td>0.165588</td>\n",
       "      <td>2.132850</td>\n",
       "      <td>0.265299</td>\n",
       "      <td>3.162550</td>\n",
       "      <td>0.256742</td>\n",
       "      <td>0.194317</td>\n",
       "      <td>0.153460</td>\n",
       "      <td>0.222734</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.191388</td>\n",
       "      <td>1.164402</td>\n",
       "      <td>0.872694</td>\n",
       "      <td>0.560240</td>\n",
       "      <td>1.352710</td>\n",
       "      <td>0.792470</td>\n",
       "      <td>1.338710</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>2.225806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750893</td>\n",
       "      <td>0.124581</td>\n",
       "      <td>1.499483</td>\n",
       "      <td>0.238277</td>\n",
       "      <td>2.340988</td>\n",
       "      <td>0.159782</td>\n",
       "      <td>0.202065</td>\n",
       "      <td>0.183713</td>\n",
       "      <td>0.223518</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.737709</td>\n",
       "      <td>1.716717</td>\n",
       "      <td>1.067453</td>\n",
       "      <td>0.557074</td>\n",
       "      <td>2.564141</td>\n",
       "      <td>2.007067</td>\n",
       "      <td>2.385526</td>\n",
       "      <td>0.741447</td>\n",
       "      <td>3.529934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903343</td>\n",
       "      <td>0.215222</td>\n",
       "      <td>1.878694</td>\n",
       "      <td>0.193812</td>\n",
       "      <td>3.104261</td>\n",
       "      <td>0.237745</td>\n",
       "      <td>0.242657</td>\n",
       "      <td>0.218792</td>\n",
       "      <td>0.333028</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  meanfreq        sd  freq.median  freq.Q25  freq.Q75  freq.IQR  \\\n",
       "0           0  1.303805  1.431251     0.660819  0.436412  1.444241  1.007829   \n",
       "1           1  2.211847  2.047427     1.340397  0.494357  4.084861  3.590504   \n",
       "2           2  1.851073  1.781392     1.073231  0.392609  3.262360  2.869751   \n",
       "3           3  1.191388  1.164402     0.872694  0.560240  1.352710  0.792470   \n",
       "4           4  1.737709  1.716717     1.067453  0.557074  2.564141  2.007067   \n",
       "\n",
       "   time.median  time.Q25  time.Q75  ...   h1_freq  h1_width   h2_freq  \\\n",
       "0     1.596774  0.887097  2.580645  ...  0.571780  0.125789  1.694041   \n",
       "1     1.151351  0.518919  1.718919  ...  0.520256  0.125211  1.708941   \n",
       "2     1.492788  0.697716  2.093149  ...  0.707695  0.165588  2.132850   \n",
       "3     1.338710  0.806452  2.225806  ...  0.750893  0.124581  1.499483   \n",
       "4     2.385526  0.741447  3.529934  ...  0.903343  0.215222  1.878694   \n",
       "\n",
       "   h2_width   h3_freq  h3_width   meanfun    minfun    maxfun   label  \n",
       "0  0.289940  2.757494  0.222735  0.213001  0.180885  0.248084  Female  \n",
       "1  0.152071  2.954897  0.214618  0.183730  0.158824  0.210526  Female  \n",
       "2  0.265299  3.162550  0.256742  0.194317  0.153460  0.222734  Female  \n",
       "3  0.238277  2.340988  0.159782  0.202065  0.183713  0.223518  Female  \n",
       "4  0.193812  3.104261  0.237745  0.242657  0.218792  0.333028  Female  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/prem/Desktop/diver/r_label_use.csv')\n",
    "data_set=pd.DataFrame(data)\n",
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe80e3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>freq.median</th>\n",
       "      <th>freq.Q25</th>\n",
       "      <th>freq.Q75</th>\n",
       "      <th>freq.IQR</th>\n",
       "      <th>time.median</th>\n",
       "      <th>time.Q25</th>\n",
       "      <th>time.Q75</th>\n",
       "      <th>time.IQR</th>\n",
       "      <th>...</th>\n",
       "      <th>h1_freq</th>\n",
       "      <th>h1_width</th>\n",
       "      <th>h2_freq</th>\n",
       "      <th>h2_width</th>\n",
       "      <th>h3_freq</th>\n",
       "      <th>h3_width</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.303805</td>\n",
       "      <td>1.431251</td>\n",
       "      <td>0.660819</td>\n",
       "      <td>0.436412</td>\n",
       "      <td>1.444241</td>\n",
       "      <td>1.007829</td>\n",
       "      <td>1.596774</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>2.580645</td>\n",
       "      <td>1.693548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571780</td>\n",
       "      <td>0.125789</td>\n",
       "      <td>1.694041</td>\n",
       "      <td>0.289940</td>\n",
       "      <td>2.757494</td>\n",
       "      <td>0.222735</td>\n",
       "      <td>0.213001</td>\n",
       "      <td>0.180885</td>\n",
       "      <td>0.248084</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.211847</td>\n",
       "      <td>2.047427</td>\n",
       "      <td>1.340397</td>\n",
       "      <td>0.494357</td>\n",
       "      <td>4.084861</td>\n",
       "      <td>3.590504</td>\n",
       "      <td>1.151351</td>\n",
       "      <td>0.518919</td>\n",
       "      <td>1.718919</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520256</td>\n",
       "      <td>0.125211</td>\n",
       "      <td>1.708941</td>\n",
       "      <td>0.152071</td>\n",
       "      <td>2.954897</td>\n",
       "      <td>0.214618</td>\n",
       "      <td>0.183730</td>\n",
       "      <td>0.158824</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.851073</td>\n",
       "      <td>1.781392</td>\n",
       "      <td>1.073231</td>\n",
       "      <td>0.392609</td>\n",
       "      <td>3.262360</td>\n",
       "      <td>2.869751</td>\n",
       "      <td>1.492788</td>\n",
       "      <td>0.697716</td>\n",
       "      <td>2.093149</td>\n",
       "      <td>1.395433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.707695</td>\n",
       "      <td>0.165588</td>\n",
       "      <td>2.132850</td>\n",
       "      <td>0.265299</td>\n",
       "      <td>3.162550</td>\n",
       "      <td>0.256742</td>\n",
       "      <td>0.194317</td>\n",
       "      <td>0.153460</td>\n",
       "      <td>0.222734</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.191388</td>\n",
       "      <td>1.164402</td>\n",
       "      <td>0.872694</td>\n",
       "      <td>0.560240</td>\n",
       "      <td>1.352710</td>\n",
       "      <td>0.792470</td>\n",
       "      <td>1.338710</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>2.225806</td>\n",
       "      <td>1.419355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750893</td>\n",
       "      <td>0.124581</td>\n",
       "      <td>1.499483</td>\n",
       "      <td>0.238277</td>\n",
       "      <td>2.340988</td>\n",
       "      <td>0.159782</td>\n",
       "      <td>0.202065</td>\n",
       "      <td>0.183713</td>\n",
       "      <td>0.223518</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.737709</td>\n",
       "      <td>1.716717</td>\n",
       "      <td>1.067453</td>\n",
       "      <td>0.557074</td>\n",
       "      <td>2.564141</td>\n",
       "      <td>2.007067</td>\n",
       "      <td>2.385526</td>\n",
       "      <td>0.741447</td>\n",
       "      <td>3.529934</td>\n",
       "      <td>2.788487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903343</td>\n",
       "      <td>0.215222</td>\n",
       "      <td>1.878694</td>\n",
       "      <td>0.193812</td>\n",
       "      <td>3.104261</td>\n",
       "      <td>0.237745</td>\n",
       "      <td>0.242657</td>\n",
       "      <td>0.218792</td>\n",
       "      <td>0.333028</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7619</th>\n",
       "      <td>1.215431</td>\n",
       "      <td>1.839264</td>\n",
       "      <td>0.321182</td>\n",
       "      <td>0.092136</td>\n",
       "      <td>1.537583</td>\n",
       "      <td>1.445447</td>\n",
       "      <td>0.905372</td>\n",
       "      <td>0.274845</td>\n",
       "      <td>1.568234</td>\n",
       "      <td>1.293389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.690271</td>\n",
       "      <td>0.217558</td>\n",
       "      <td>1.580463</td>\n",
       "      <td>0.213196</td>\n",
       "      <td>2.852369</td>\n",
       "      <td>0.258661</td>\n",
       "      <td>0.120617</td>\n",
       "      <td>0.115490</td>\n",
       "      <td>0.124954</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7620</th>\n",
       "      <td>1.669696</td>\n",
       "      <td>1.820123</td>\n",
       "      <td>0.683231</td>\n",
       "      <td>0.231490</td>\n",
       "      <td>2.759960</td>\n",
       "      <td>2.528470</td>\n",
       "      <td>1.378345</td>\n",
       "      <td>0.470259</td>\n",
       "      <td>1.832388</td>\n",
       "      <td>1.362129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.488477</td>\n",
       "      <td>0.209534</td>\n",
       "      <td>1.786329</td>\n",
       "      <td>0.244668</td>\n",
       "      <td>2.730599</td>\n",
       "      <td>0.263459</td>\n",
       "      <td>0.196559</td>\n",
       "      <td>0.106652</td>\n",
       "      <td>0.330954</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7621</th>\n",
       "      <td>2.278545</td>\n",
       "      <td>2.300699</td>\n",
       "      <td>0.950154</td>\n",
       "      <td>0.338392</td>\n",
       "      <td>4.909204</td>\n",
       "      <td>4.570812</td>\n",
       "      <td>0.488955</td>\n",
       "      <td>0.228179</td>\n",
       "      <td>1.059403</td>\n",
       "      <td>0.831224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467912</td>\n",
       "      <td>0.169192</td>\n",
       "      <td>1.439881</td>\n",
       "      <td>0.152220</td>\n",
       "      <td>2.368497</td>\n",
       "      <td>0.212977</td>\n",
       "      <td>0.121343</td>\n",
       "      <td>0.113721</td>\n",
       "      <td>0.133018</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7622</th>\n",
       "      <td>0.933789</td>\n",
       "      <td>1.358711</td>\n",
       "      <td>0.396002</td>\n",
       "      <td>0.120030</td>\n",
       "      <td>0.987589</td>\n",
       "      <td>0.867559</td>\n",
       "      <td>3.040800</td>\n",
       "      <td>0.740089</td>\n",
       "      <td>5.438044</td>\n",
       "      <td>4.697956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500634</td>\n",
       "      <td>0.149385</td>\n",
       "      <td>1.714692</td>\n",
       "      <td>0.228120</td>\n",
       "      <td>2.689295</td>\n",
       "      <td>0.281016</td>\n",
       "      <td>0.134804</td>\n",
       "      <td>0.098534</td>\n",
       "      <td>0.564599</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7623</th>\n",
       "      <td>1.054806</td>\n",
       "      <td>1.546068</td>\n",
       "      <td>0.318053</td>\n",
       "      <td>0.113901</td>\n",
       "      <td>1.266919</td>\n",
       "      <td>1.153017</td>\n",
       "      <td>3.261311</td>\n",
       "      <td>1.590492</td>\n",
       "      <td>4.723278</td>\n",
       "      <td>3.132787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.462851</td>\n",
       "      <td>0.191590</td>\n",
       "      <td>1.884090</td>\n",
       "      <td>0.208419</td>\n",
       "      <td>2.778476</td>\n",
       "      <td>0.283865</td>\n",
       "      <td>0.108991</td>\n",
       "      <td>0.097199</td>\n",
       "      <td>0.116483</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7624 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      meanfreq        sd  freq.median  freq.Q25  freq.Q75  freq.IQR  \\\n",
       "0     1.303805  1.431251     0.660819  0.436412  1.444241  1.007829   \n",
       "1     2.211847  2.047427     1.340397  0.494357  4.084861  3.590504   \n",
       "2     1.851073  1.781392     1.073231  0.392609  3.262360  2.869751   \n",
       "3     1.191388  1.164402     0.872694  0.560240  1.352710  0.792470   \n",
       "4     1.737709  1.716717     1.067453  0.557074  2.564141  2.007067   \n",
       "...        ...       ...          ...       ...       ...       ...   \n",
       "7619  1.215431  1.839264     0.321182  0.092136  1.537583  1.445447   \n",
       "7620  1.669696  1.820123     0.683231  0.231490  2.759960  2.528470   \n",
       "7621  2.278545  2.300699     0.950154  0.338392  4.909204  4.570812   \n",
       "7622  0.933789  1.358711     0.396002  0.120030  0.987589  0.867559   \n",
       "7623  1.054806  1.546068     0.318053  0.113901  1.266919  1.153017   \n",
       "\n",
       "      time.median  time.Q25  time.Q75  time.IQR  ...   h1_freq  h1_width  \\\n",
       "0        1.596774  0.887097  2.580645  1.693548  ...  0.571780  0.125789   \n",
       "1        1.151351  0.518919  1.718919  1.200000  ...  0.520256  0.125211   \n",
       "2        1.492788  0.697716  2.093149  1.395433  ...  0.707695  0.165588   \n",
       "3        1.338710  0.806452  2.225806  1.419355  ...  0.750893  0.124581   \n",
       "4        2.385526  0.741447  3.529934  2.788487  ...  0.903343  0.215222   \n",
       "...           ...       ...       ...       ...  ...       ...       ...   \n",
       "7619     0.905372  0.274845  1.568234  1.293389  ...  0.690271  0.217558   \n",
       "7620     1.378345  0.470259  1.832388  1.362129  ...  0.488477  0.209534   \n",
       "7621     0.488955  0.228179  1.059403  0.831224  ...  0.467912  0.169192   \n",
       "7622     3.040800  0.740089  5.438044  4.697956  ...  0.500634  0.149385   \n",
       "7623     3.261311  1.590492  4.723278  3.132787  ...  0.462851  0.191590   \n",
       "\n",
       "       h2_freq  h2_width   h3_freq  h3_width   meanfun    minfun    maxfun  \\\n",
       "0     1.694041  0.289940  2.757494  0.222735  0.213001  0.180885  0.248084   \n",
       "1     1.708941  0.152071  2.954897  0.214618  0.183730  0.158824  0.210526   \n",
       "2     2.132850  0.265299  3.162550  0.256742  0.194317  0.153460  0.222734   \n",
       "3     1.499483  0.238277  2.340988  0.159782  0.202065  0.183713  0.223518   \n",
       "4     1.878694  0.193812  3.104261  0.237745  0.242657  0.218792  0.333028   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7619  1.580463  0.213196  2.852369  0.258661  0.120617  0.115490  0.124954   \n",
       "7620  1.786329  0.244668  2.730599  0.263459  0.196559  0.106652  0.330954   \n",
       "7621  1.439881  0.152220  2.368497  0.212977  0.121343  0.113721  0.133018   \n",
       "7622  1.714692  0.228120  2.689295  0.281016  0.134804  0.098534  0.564599   \n",
       "7623  1.884090  0.208419  2.778476  0.283865  0.108991  0.097199  0.116483   \n",
       "\n",
       "       label  \n",
       "0     Female  \n",
       "1     Female  \n",
       "2     Female  \n",
       "3     Female  \n",
       "4     Female  \n",
       "...      ...  \n",
       "7619    Male  \n",
       "7620    Male  \n",
       "7621    Male  \n",
       "7622    Male  \n",
       "7623    Male  \n",
       "\n",
       "[7624 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = data_set.drop(columns=['Unnamed: 0'])\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23965c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Female', 'Male'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888d8304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>freq.median</th>\n",
       "      <th>freq.Q25</th>\n",
       "      <th>freq.Q75</th>\n",
       "      <th>freq.IQR</th>\n",
       "      <th>time.median</th>\n",
       "      <th>time.Q25</th>\n",
       "      <th>time.Q75</th>\n",
       "      <th>time.IQR</th>\n",
       "      <th>...</th>\n",
       "      <th>h1_freq</th>\n",
       "      <th>h1_width</th>\n",
       "      <th>h2_freq</th>\n",
       "      <th>h2_width</th>\n",
       "      <th>h3_freq</th>\n",
       "      <th>h3_width</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.303805</td>\n",
       "      <td>1.431251</td>\n",
       "      <td>0.660819</td>\n",
       "      <td>0.436412</td>\n",
       "      <td>1.444241</td>\n",
       "      <td>1.007829</td>\n",
       "      <td>1.596774</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>2.580645</td>\n",
       "      <td>1.693548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571780</td>\n",
       "      <td>0.125789</td>\n",
       "      <td>1.694041</td>\n",
       "      <td>0.289940</td>\n",
       "      <td>2.757494</td>\n",
       "      <td>0.222735</td>\n",
       "      <td>0.213001</td>\n",
       "      <td>0.180885</td>\n",
       "      <td>0.248084</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.211847</td>\n",
       "      <td>2.047427</td>\n",
       "      <td>1.340397</td>\n",
       "      <td>0.494357</td>\n",
       "      <td>4.084861</td>\n",
       "      <td>3.590504</td>\n",
       "      <td>1.151351</td>\n",
       "      <td>0.518919</td>\n",
       "      <td>1.718919</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520256</td>\n",
       "      <td>0.125211</td>\n",
       "      <td>1.708941</td>\n",
       "      <td>0.152071</td>\n",
       "      <td>2.954897</td>\n",
       "      <td>0.214618</td>\n",
       "      <td>0.183730</td>\n",
       "      <td>0.158824</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.851073</td>\n",
       "      <td>1.781392</td>\n",
       "      <td>1.073231</td>\n",
       "      <td>0.392609</td>\n",
       "      <td>3.262360</td>\n",
       "      <td>2.869751</td>\n",
       "      <td>1.492788</td>\n",
       "      <td>0.697716</td>\n",
       "      <td>2.093149</td>\n",
       "      <td>1.395433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.707695</td>\n",
       "      <td>0.165588</td>\n",
       "      <td>2.132850</td>\n",
       "      <td>0.265299</td>\n",
       "      <td>3.162550</td>\n",
       "      <td>0.256742</td>\n",
       "      <td>0.194317</td>\n",
       "      <td>0.153460</td>\n",
       "      <td>0.222734</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.191388</td>\n",
       "      <td>1.164402</td>\n",
       "      <td>0.872694</td>\n",
       "      <td>0.560240</td>\n",
       "      <td>1.352710</td>\n",
       "      <td>0.792470</td>\n",
       "      <td>1.338710</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>2.225806</td>\n",
       "      <td>1.419355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750893</td>\n",
       "      <td>0.124581</td>\n",
       "      <td>1.499483</td>\n",
       "      <td>0.238277</td>\n",
       "      <td>2.340988</td>\n",
       "      <td>0.159782</td>\n",
       "      <td>0.202065</td>\n",
       "      <td>0.183713</td>\n",
       "      <td>0.223518</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.737709</td>\n",
       "      <td>1.716717</td>\n",
       "      <td>1.067453</td>\n",
       "      <td>0.557074</td>\n",
       "      <td>2.564141</td>\n",
       "      <td>2.007067</td>\n",
       "      <td>2.385526</td>\n",
       "      <td>0.741447</td>\n",
       "      <td>3.529934</td>\n",
       "      <td>2.788487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903343</td>\n",
       "      <td>0.215222</td>\n",
       "      <td>1.878694</td>\n",
       "      <td>0.193812</td>\n",
       "      <td>3.104261</td>\n",
       "      <td>0.237745</td>\n",
       "      <td>0.242657</td>\n",
       "      <td>0.218792</td>\n",
       "      <td>0.333028</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   meanfreq        sd  freq.median  freq.Q25  freq.Q75  freq.IQR  time.median  \\\n",
       "0  1.303805  1.431251     0.660819  0.436412  1.444241  1.007829     1.596774   \n",
       "1  2.211847  2.047427     1.340397  0.494357  4.084861  3.590504     1.151351   \n",
       "2  1.851073  1.781392     1.073231  0.392609  3.262360  2.869751     1.492788   \n",
       "3  1.191388  1.164402     0.872694  0.560240  1.352710  0.792470     1.338710   \n",
       "4  1.737709  1.716717     1.067453  0.557074  2.564141  2.007067     2.385526   \n",
       "\n",
       "   time.Q25  time.Q75  time.IQR  ...   h1_freq  h1_width   h2_freq  h2_width  \\\n",
       "0  0.887097  2.580645  1.693548  ...  0.571780  0.125789  1.694041  0.289940   \n",
       "1  0.518919  1.718919  1.200000  ...  0.520256  0.125211  1.708941  0.152071   \n",
       "2  0.697716  2.093149  1.395433  ...  0.707695  0.165588  2.132850  0.265299   \n",
       "3  0.806452  2.225806  1.419355  ...  0.750893  0.124581  1.499483  0.238277   \n",
       "4  0.741447  3.529934  2.788487  ...  0.903343  0.215222  1.878694  0.193812   \n",
       "\n",
       "    h3_freq  h3_width   meanfun    minfun    maxfun  label  \n",
       "0  2.757494  0.222735  0.213001  0.180885  0.248084      0  \n",
       "1  2.954897  0.214618  0.183730  0.158824  0.210526      0  \n",
       "2  3.162550  0.256742  0.194317  0.153460  0.222734      0  \n",
       "3  2.340988  0.159782  0.202065  0.183713  0.223518      0  \n",
       "4  3.104261  0.237745  0.242657  0.218792  0.333028      0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "le = LabelEncoder()\n",
    "dataframe['label']= le.fit_transform(dataframe['label'])\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e42a89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Features')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABdUAAALiCAYAAAA/9z/9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAB2bklEQVR4nOzde5RlV1ku/OchQYiIQSAIyMHmJiq3YAqUWwThgKigCIqSo4KXiDdA5XxH0Q9BPlFED6h4ISKgEhFFPSKoCSLhJgjVSZMEBBQTvMDRoAJyh+T9/ti7pSiqu3eT7q6qzu83xh577Tnnmutdu6szMp6aPVdnJgAAAAAAwKFdbbsLAAAAAACA3UKoDgAAAAAAKxKqAwAAAADAioTqAAAAAACwIqE6AAAAAACsSKgOAAAAAAArEqoDALDjtD2v7RzmOdP2vN187auCT+f73W5tH778M374YZxzw7a/2faf2l6+PP86R69KAACOlRO3uwAAAI6dTyPMfMTMPPdo1AJXRttLk2Rm9mxvJQf03CT3TfL8JH+XZJJ8+FhcuO1zk3xbkpvNzKXH4poAAFclQnUAgKuWJ27R9pgkJyf5hSTv2dS37+iWc0R9UZIPbncR0PYzkvz3JH8xM2dsdz0AABxZQnUAgKuQmXnC5rbllhYnJ3n6bl7VOjNv2e4aYOmGWWy1+c7tLgQAgCPPnuoAABxQ229s+8q27237obYXtf3RttfYYuyly9fJbZ/R9p/bfrjtm9s+qm0/jeuf2PZxbf+27Ufa/mPbpyxXAm8eu+W+5m1v1PY5bf91eQ/72n5b23suz3nClb32Fuf+zHLubz1A/2nL/j859LfwX+fct+2fLO9jfz1/3PY+m8Zdre0j276h7fvbfmB5/D1tP+X///d/b8s9wJ+1/HO7fP/+4YfqX4750rYvbPt/2350Wdsz2954xXv7jLbf3/ZP275jeX//3vYv2t5/09h7Lrcx+vwkn7+sb//ruZvGfmHb5y7r+Ujbf2n7O21vfYA6btn299v+x/J7+6u2X73KPWyY49Ik71h+/LaD1PbNbV++vNaH2/5N2x8/wN+tr2v7vLZvW9b1/rZ7l3+vrrZp7GSx9UuSXLLh+pdurHHj503nP2E5/p6b5z2SPwdtb972rLZ/18Xfy3/v4r8vv9b2egf4egEAdgQr1QEA2FLbJyf50STvTvI7Sd6f5P5Jnpzkfm3/+8x8bNNpn5HkL5JcJ8nvLj8/OIutZW6d5PsOs4zfSXKPJH+W5H1JvirJ/5PkBkkescI93CDJXyXZk+SVy+MbJvmVJOcexWv/WpL/meS7k/zWFv3fvXx/5qHuIUnaPjHJ47P4M/g/Sf4xyY2T3DXJ/8jiO9/vt5M8bDnmWVns5f2gLO757km22o7kuklet5z/D5NckeRfVulv+4gkv57kI0letLzurZJ8Z5IHtP2ymfmHQ9zidbP4GfmrJC9NclmSGyV5QJI/bftdM/Os5dhLs9jG6DHLz0/fMM++/Qdtv3JZ69WT/EkW+5rfJMnXJ/nqtveamfM3jL9VktcmuV4Wf+b7ktwyi+/7zw5R/0ZPz+Ln7dFJ3rg8f3Ntv5Hk25P807LG9yT5siRPSnLv5d+tj2+Y82ey+M7/Osk/Z/EvS74ii+/sTkm+ZcPYJyb5uiR3yCdv6fSeXHlH5Oeg7Y2SvCHJZyf50yR/kOSaSW62vJdnJPm3I1AvAMDRMTNeXl5eXl5eXl5X4VcWIeUk2bOh7S7Ltn9IcsMN7SdmEVBOkscdYJ5XJ7nGhvbrJnn7su/0FWs6bzl+b5Lrbmi/Vhbh6OUb61r2TZLzNrX9xrL9KZva75BF+DdJnnCUrv3iZfvtNrV/VpL/XH63J6zwXdx3Oc/fJ/m8LfpvsuH4m5djz0/yWZtqX1/2PWyL2ieL8P/ELeY/YH+SL0jy0eX38nmb+r5i+V390Vbf76a2a2y8jw3tJye5OMm/Jzlpi5+3Sw/wnX1Okv/I4hdCX7yp7zZZhMLnb2o/d3mfj97U/rUbvoOHr/jzu2c5/rlb9D182feHW9zTEw5Qwy22mOdqSX5zOf5LN/U9N5v+Th/Gd7e/hnserZ+DJD+w1X1u+Fk9aavavLy8vLy8vLx2ysv2LwAAbOXbl+//38z83/2Ns1g9+8NZrFD9zgOc+6Mz85EN5/x7FitwkxVWl2/yv5bn75/rA0nOziJQXDvYiV1s0/LNSd6b5P/b2Dczb8zWK8iPyLWXfnX5fuam9jOyCNafNTOXrzDPDyzff3hm/nlz58z804aP+//cfmRm3r+p9v+1/LjVn9tHkzx2Pnl19Cr935PFSvBHb65tZv4yixXLD2h77QPMu3/sRzbdx/729yZ5dhYh+Z0ONscm35rFv5b4iZl586Y535TFiuo7tv3iJGl7kyweLHpJFqukN47/4ySvOIxrH8qjk3w8ybfPzIc29T0pixXan/SvCWbm7ZsnmZkrsliJniT3O4L1HcyR/jnYfP+ZmQ9s8b0AAOwotn8BAGArX7J8/8vNHTPztrb/lORmba8zM+/Z0P3xLLbw2Oy85fsdD7OO9S3a/nH5/jmHOPfWSU5Ksj4z/7lF/6tz4F8MXNlrJ4stQy5J8i1t/9fMfHDZfmYWK3efdcAzP9mXZbGq989XGPslWfzC47wt+l6xvO5WfwaXzsy/HmTeA/XfZfn+5W23Cr1vkOSELFYy7z3I/Gl7myy2zDk9i61frrlpyOcd7PwD1HWHbr1n/hcs378oyZvzie/k1Qf4Rcd5Sb78MK6/pbafmcW/knh3ksd068cMfGRZ18bzrpfFd/NVSW6exWrujQ7nu7kyjtTPwYuy2Ebql9veL8k5SV6T5M0zM0e+bACAI0uoDgDAVk5evr/rAP3vSnLT5bj3bGh/9wFCyf2r3U/eou+ANgX2++1fJXvCIU7ff61/OUD/gdqPxLUzM1e0fWYW+2E/NMlz2p6WRfD9f2bmnYeaY+k6Sf5jxdW7Jyf595n56Bb1fLztu7MIODf7v1u0rdK//4GS//MQ53/WwTrbflkWv8A5McnLsghd35fFLwhOzWILlk95gOdB7K/ru1as61A/K4f6flb1OUma5JQkP7HKCW2vk8X+4zdL8vos/oXFv2fxs3idLFa+H853c2UckZ+DmXlH2ztnsdXMV2axz32S/GPbn5uZX7yyhQIAHE1CdQAAtvLe5fsNs9gPfbMbbRq33/XbnrBFsH7DA4w/mt63fP/cA/QfqP1IenYWD4787iTPyWE+oHTpPUmu1/akFYL19ya5bturz6aHyLY9Mcn184nvZaNDrQ4+UP/+P8+TZ2areVf141n8q4J7zcx5Gzva/mgWofrh2F/XHWbmwsMYf6CfiRseoP1w7b/OBTPzJQcd+QnfmUWg/sSZecLGjrZ3ySJUP1xXZPEQ4a1c5yDnHbGfg5n5myQPXf5c3iHJfbLY6ugX2n5gZn5jlXkAALaDPdUBANjKBcv3e27uaHvLJDdJcskWq7lPTHLXLebbP88FW/QdLW/JYs/m2x9gT++7H+0CZuayJC9M8qVt75bFHu+XZvFQzFW9LovVzV+5wtgLsvh//NO36Ds9ixX25x/GtQ/ldcv3e1zJeW6ZxQr787boO9C2K5fnwP9i4HDr2v9zefe2W815zxXnOajlPvdvSnKbttdd8bRbLt//YIu+g303yYG/n/9I8rltr75F3yrPC9js0/45mJmPz8zemXlKFn8/kuTrPo0aAACOGaE6AABbefby/cfbnrK/cRk4/lwW/x95oJWkP932GhvOuW4WK5GTxWrtY2K5BcoLstja48c39rW9QxYPszwW9j+w9AVZbH9x1vIhk6v6peX7z7f9lL2zN7Xt/3P76eX+3fvHfGYW29AkB/5z+3Q8I8nHkjyt7Rds7mz7GW1XCVovzWKF/e03nf8dOfBDOP8tySltT9qi7zlZrPD/ieU2I5vrulrbe+7/vHxI6kuzWBH+/ZvGfm2OwH7qG/zvLFaJP3u5tcvm2j6n7cZV7Jcu3++5adwdk/zoAa7xb8v3mx6g//VZ/ALskx4c3PbhSe52wMoP7LB+Dtreue1W/ypgf9sHt+gDANgxbP8CAMCnmJm/avuzSf6fJBe3fWGSDyS5f5LbZvGQz6duceq7stjf+eK2L0py9SQPyWK7mF+ZmVcei/o3+JEkX5Hk/2n7pVk8RPVGSb4xyZ9msSL2cALuwzYzr2n7xiy2uPhYPhF8r3r+uW2flOT/TfI3bf9PFg9M/dwsVtu/LsnDl2N/ZxkCf2OSNy3HThb3ebMkvzczZ1/5u/qv2t7S9tuzuKc3tf3zJG/L4s/9plmsXL4syRceYqqnZxGev7rt72Wxncja8v5emMXP0GYvS3KnJH/e9pVZPODzjTPzJzPzb20fkuSPkryu7cuyWCF+xbKuu2SxD/jGh6F+X5LXJnl62/smeWMWq8QflORPkjxg1e/lYGbm2cu99b83ydvbnpPkH5JcN4s/o9Oz+KXAI5en/FYWe5U/ve29kvxtklsl+Zokf5jFfv2bvWx5zq8v/+6+P8l7ZuYZy/5fyiJQ/9W2987i5+kOWfwrkxcv5z6cezrcn4OHJfm+tq9I8ndZrJy/RRbf8Uey+HkAANixhOoAAGxpZv5X2wuyWLn7rVkEZG/PYtX3z2/1MMwkH81ib+QnJ/mmLPbw/vssVkn/0hbjj6qZ+Ze2d13W81VJvjTJW7MIND+QRdh8ZfYCX9VzsggK/3hmDvqA1K3MzOPbvi7Jo7IIPK+V5F+TrGcRum70zUlekeTb84k93P8myc/nE6vmj5iZed7ylwY/nOReSe6bxXf7ziwC8ResMMeft31AFj9bD81i+5LXL+e7ebYO1f+/LPb/fkAWq6tPSPKbWQTgmZmXLVe+PzaLwP4eWfx8vjOLh6J+0nYqM/O3ywem/kwWP8P3THJhFj8jp+QIherLa31f2z/LIji/z/I+/j2LcP2pSZ63Yew7l6u8fyaLXzLcL4utjb43yV9ki1B9Zs5p+8NZPKj1B7NYGf+OLFaUZ2be3Hb/39MHZPHQ01dl8cuGr89hhurLOQ/n5+D5Wfzy7a5ZPLj3pCT/nOR3s/hvy8WHe30AgGOpM4d6JhEAABxa20uTZGb2bG8lq2n7U0kel+QrZ+aco3yt5yb5tiT3mZmXHc1rAQAAR5dQHQCAI2Knhuptbzwz79zUdrsstoL5aJLPm5kPH8Xr/7cstuz4+yS3Gf8DDgAAu5rtXwAAON6tt/27JBdnsR3FrZJ8dRYPW33k0QrU2z4syRdksQ3ONZL8vwJ1AADY/YTqAAAc756Zxb7Y35zk2knek+ScJD83M+cdxeuemcVDJ/8xyQ/OzB8cYjwAALAL2P4FAAAAAABWdLXtLgAAAAAAAHYLoToAAAAAAKxIqA4AAAAAACsSqgMAAAAAwIqE6gAAAAAAsCKhOgAAAAAArEioDgAAAAAAKxKqAwAAAADAioTqAAAAAACwIqE6AAAAAACsSKgOAAAAAAArEqoDAAAAAMCKhOoAAAAAALAioToAAAAAAKxIqA4AAAAAACsSqgMAAAAAwIqE6gAAAAAAsCKhOgAAAAAArEioDgAAAAAAKxKqAwAAAADAik7c7gKuSq5//evPnj17trsMAAAAAAAOYu/eve+emVO26hOqH0N79uzJ+vr6dpcBAAAAAMBBtH3Hgfps/wIAAAAAACsSqgMAAAAAwIqE6gAAAAAAsCJ7qh9Le/cm7XZXAQAAAACwtZntrmDHs1IdAAAAAABWtK2hetv3b/r88LbPWB4/oe0H295gq/FtL2+7r+3Fbf+k7XWW5z9/05zXb3tZ22scoIZHtv3WI3AvT2j72Cs7DwAAAAAAO9dOX6n+7iQ/fIC+D83MqTNz2yT/nuT7kvxhkv/e9jM3jHtIkhfNzEe2mmRmfm1mfutIFg0AAAAAwPFpp4fqz07y0LbXPcS41yb5vJl5X5JXJnnAhr5vSvL8tp/f9mVtL1y+3zT55BXmbW/Z9i/avrHt+W1vsWz/n23fsDz3ifsnbvtjbd/a9i+S3PrI3TYAAAAAADvRdofqJy23cNnXdl+Sn9zU//4sgvVHH2iCtickuXeSFy2bnp9FkJ62N07yBUlenuQZSX5rZm6f5Owkv7jFdGcn+eWZuUOSuyZ5V9v7JrlVkjsnOTXJaW1Pb3va8jp3TPL1Se50gPrObLvedv2yg30TAAAAAADseCdu8/U/NDOn7v/Q9uFJ1jaN+cUk+9r+/Kb2k5ZB/J4ke5O8dNn+4iS/0vazk3xjkhfOzOVt75JF+J0kv53kZzdO1vbaWax2/6MkmZkPL9vvm+S+SS5YDv2sLEL2ayf5o5n54HLci7KFmTkryVlJstZ6dC4AAAAAwC623SvVD2lm3pPkd5J876au/YH85yf5jCz2VM/MfCjJnyd5UJZbvxxo6k2fe4BxTfLTy/3bT52ZW87MbxxgDgAAAAAAjmM7PlRf+t9JvjtbrKyfmfcmeVSSx7a9+rL5+Ul+KMnnJnndsu2vstwWJskZSV69aZ73Jfmntl+XJG2vsXzg6TlJvr3tZy3bP6/tDbLYu/1BbU9arnLfuI87AAAAAADHoV0Rqs/Mu5P8UZJrHKD/giRvzCdC83OT3DjJC2Zm/2ryRyV5RNsLk3xLtt6n/VuSPGo55q+S3HBmzs1ipfxr216U5IVJrj0z5yd5QZJ9Sf4gyauu7H0CAAAAALCz9ROZM0fb2trarK+vb3cZAAAAAAAcRNu9M7P5+Z9JdslKdQAAAAAA2AmE6gAAAAAAsCKhOgAAAAAArEioDgAAAAAAKxKqAwAAAADAioTqAAAAAACwIqE6AAAAAACsSKgOAAAAAAArOnG7C7hK2bs3abe7CgAAAACuSma2uwI4rlipDgAAAAAAK7rKh+pt37/p88PbPmN5/IS2H2x7g63Gt7287b62F7f9k7bXOWaFAwAAAABwzF3lQ/UVvDvJDx+g70Mzc+rM3DbJvyf5vmNXFgAAAAAAx5pQ/dCeneShba97iHGvTfJ5x6AeAAAAAAC2iVA9OWm5hcu+tvuS/OSm/vdnEaw/+kATtD0hyb2TvGiLvjPbrrddv+wIFg0AAAAAwLEnVP/EFi6nzsypSR6/xZhfTPJtbT97U/tJyyD+35JcN8lLN584M2fNzNrMrJ1yhAsHAAAAAODYEqqvYGbek+R3knzvpq4PLYP4z0/yGbGnOgAAAADAcU2ovrr/neS7k5y4uWNm3pvkUUke2/bqx7owAAAAAACODaH6imbm3Un+KMk1DtB/QZI3JvmmY1kXAAAAAADHTmdmu2u4ylhbW5v19fXtLgMAAAAAgINou3dm1rbqs1IdAAAAAABWJFQHAAAAAIAVCdUBAAAAAGBFQnUAAAAAAFiRUB0AAAAAAFYkVAcAAAAAgBUJ1QEAAAAAYEVCdQAAAAAAWJFQHQAAAAAAVnTidhdwlbJ3b9JudxUAAACsama7KwAAdhgr1QEAAAAAYEU7PlRvu6ftxVu0P7XtW9pe2PaP2l7nSlzjxm1feIC+89quLY8fd6i6AAAAAAA4fu34UP0gXprktjNz+yRvS/Kjn+5EM/POmXnICkMfd+ghAAAAAAAcr3ZLqH5C219v+6a257Y9aWbOnZmPL/tfl+QmBzq57Z+2vf3y+IK2j18eP6ntd25cdd72pLa/u1wB/4IkJy3bfybJSW33tT37QHVtce0z2663Xb/siH0dAAAAAABsh90Sqt8qyS/PzG2SvCfJgzf1f3uSPzvI+a9Mco+2n53k40nutmy/e5JXbRr7PUk+uFwB/1NJTkuSmfmRJB+amVNn5owV68rMnDUzazOzdsoqdwoAAAAAwI61W0L1S2Zm3/J4b5I9+zva/lgWQfnZn3raf3lVktOzCNFfkuSz2n5mkj0z89ZNY09P8rwkmZkLk1z46dQFAAAAAMDx58TtLmBFH9lwfHk+sSXLtyX5miT3npk5yPlvSLKW5O+z2Iv9+km+K4sgfCsHm+uQdQEAAAAAcHzaLSvVP0Xbr0zyv5I8cGY+eLCxM/PRJP+Y5Buz2H/9VUkem0/d+iVZbBVzxvIat01y+w19H2t79StfPQAAAAAAu9FuWam+lWckuUaSl7ZNktfNzCMPMv5VWaxo/2DbV2XxYNOtQvVfTfKcthcm2Zfk9Rv6zkpyYdvzk/zYYVd82mnJ+vphnwYAAAAAwM7Qg++awpG0trY260J1AAAAAIAdre3emVnbqm/Xbv8CAAAAAADH2m7e/uVTtL1fkqdsar5kZh60HfUAAAAAAHB8Oa5C9Zk5J8k5210HAAAAAADHJ9u/AAAAAADAioTqAAAAAACwIqE6AAAAAACsSKgOAAAAAAArEqoDAAAAAMCKTtzuAq5S9u5N2u2uAgCAI2VmuysAAACOMSvVAQAAAABgRbs+VG+7p+3FW7Q/qe2Fbfe1PbftjQ8xz/OX43/w6FULAAAAAMBututD9YN46szcfmZOTfLiJI8/0MC2N0xy1+X4p23qs0UOAAAAAABJjp9Q/YS2v972TctV6SfNzPs29F8rycE2vDw3yQ2Wq9rv0fa8tk9u+4okj257WttXtN3b9py2N0qSZfsb27627VO3WjEPAAAAAMDx43gJ1W+V5Jdn5jZJ3pPkwUnS9qfa/mOSM3KQlepJHpjk7TNz6sy8atl2nZn58iS/mOSXkjxkZk5L8uwkP7Uc85wkj5qZuxxo4rZntl1vu37Zp39/AAAAAADsAMdLqH7JzOxbHu9NsidJZubHZua/JTk7yfcf5pwvWL7fOsltk7y07b4kP57kJm1PziJ4f8Vy3G9vNcnMnDUzazOzdsphFgAAAAAAwM5yvOwX/pENx5cnOWlT/+8keUmSnziMOT+wfG+SN21ejd72Ojn4ljIAAAAAABxnjpeV6p+i7a02fHxgkrd8mlO9Nckpbe+ynPfqbW8zM+9J8t62d1+OO+PTLhYAAAAAgF3heFmpvpWfaXvrJFckeUeSR346k8zMR9s+JMkvLrd8OTHJ05O8Kckjkjy77QeTnHPIyU47LVlf/3TKAAAAAABgB+iMHUyOhLZ7krx4Zm57oDFra2uzLlQHAAAAANjR2u6dmbWt+o7b7V8AAAAAAOBIO563f/kUbe+X5Cmbmi+ZmQdd2bln5tIkB1ylDgAAAADA7neVCtVn5pyssvc5AAAAAABswfYvAAAAAACwIqE6AAAAAACsSKgOAAAAAAArEqoDAAAAAMCKhOoAAAAAALCiE7e7gKuUvXuTdrurAAAOZma7KwAAAGAHs1IdAAAAAABWtOtD9bZ72l68RftT276l7YVt/6jtdQ4xz/OXY3/wqBULAAAAAMCututD9YN4aZLbzsztk7wtyY8eaGDbGya568zcfmaetqnPFjkAAAAAACQ5fkL1E9r+ets3tT237Ukzc+7MfHzZ/7okNznI+ecmuUHbfW3v0fa8tk9u+4okj257WttXtN3b9py2N0qSZfsb2752uTL+U1bMAwAAAABw/DheQvVbJfnlmblNkvckefCm/m9P8mcHOf+BSd4+M6fOzKuWbdeZmS9P8otJfinJQ2bmtCTPTvJTyzHPSfKombnLgSZue2bb9bbrlx3uXQEAAAAAsKMcL1ubXDIz+5bHe5Ps2d/R9seSfDzJ2Yc55wuW77dOctskL22bJCckeVfbk7MI3l+xHPfbSe6/eZKZOSvJWUmy1s5h1gAAAAAAwA5yvITqH9lwfHmSk5Kk7bcl+Zok956Zww20P7B8b5I3bV6NvnzwqZAcAAAAAOAq5HjZ/uVTtP3KJP8ryQNn5oNXYqq3Jjml7V2W81697W1m5j1J3tv27stxZ1ypggEAAAAA2PGO21A9yTOSXDuLbVv2tf21T2eSmflokockeUrbNybZl+Suy+5HJPnltq9N8qErXzIAAAAAADtZD39XFLbSdk+SF8/MbQ80Zm1tbdbX149dUQAAAAAAHLa2e2dmbau+43mlOgAAAAAAHFHHy4NKV9L2fkmesqn5kpl50JWde2YuTXLAVeoAAAAAAOx+V6lQfWbOSXLOdtcBAAAAAMDuZPsXAAAAAABYkVAdAAAAAABWJFQHAAAAAIAVCdUBAAAAAGBFQnUAAAAAAFjRidtdwFXK3r1Ju91VAGyvme2uAAAAAODTZqU6AAAAAACsSKgOAAAAAAAr2vGhets9bS/eov1JbS9su6/tuW1vfCWuceO2LzxA33lt15bHjztUXQAAAAAAHL92fKh+EE+dmdvPzKlJXpzk8Z/uRDPzzpl5yApDH3foIQAAAAAAHK92S6h+Qttfb/um5ar0k2bmfRv6r5XkgE++a/unbW+/PL6g7eOXx09q+50bV523Pant7y5Xwb8gyUnL9p9JctJyZfzZB6pri2uf2Xa97fplR+SrAAAAAABgu+yWUP1WSX55Zm6T5D1JHpwkbX+q7T8mOSMHX6n+yiT3aPvZST6e5G7L9rsnedWmsd+T5IMzc/skP5XktCSZmR9J8qGZOXVmzjhYXRvNzFkzszYza6cc3j0DAAAAALDD7JZQ/ZKZ2bc83ptkT5LMzI/NzH9LcnaS7z/I+a9KcnoWIfpLknxW289Msmdm3rpp7OlJnrec/8IkFx5uXQAAAAAAHJ92S6j+kQ3Hlyc5cVP/72SLVeIbvCHJWpJ7ZLFq/YIk35VFEL6VA24lc5h1AQAAAABwHNktofqnaHurDR8fmOQtBxo7Mx9N8o9JvjHJ67JYuf7YfOrWL8kidD9jeY3bJrn9hr6Ptb36lascAAAAAIDdajevrP6ZtrdOckWSdyR55CHGvyrJvWfmg21fleQm2TpU/9Ukz2l7YZJ9SV6/oe+sJBe2PT/Jjx12xaedlqyvH/ZpAAAAAADsDJ1ZdacTrqy1tbVZF6oDAAAAAOxobffOzNpWfbt2+xcAAAAAADjWdvP2L5+i7f2SPGVT8yUz86DtqAcAAAAAgOPLcRWqz8w5Sc7Z7joAAAAAADg+2f4FAAAAAABWJFQHAAAAAIAVCdUBAAAAAGBFQnUAAAAAAFjRcfWg0h1v796k3e4qAI6sme2uAAAAAOCYsVIdAAAAAABWJFQHAAAAAIAV7fhQve2ethdv0f4Nbd/U9oq2a1fyGjdu+8ID9J23f/62jztUXQAAAAAAHL92fKh+EBcn+fokr7yyE83MO2fmISsMfdyhhwAAAAAAcLzaLaH6CW1/fbky/dy2J83M38zMW1c5ue2ftr398viCto9fHj+p7XduXHXe9qS2v9v2wrYvSHLSsv1nkpzUdl/bsw9U1xbXPrPtetv1y47AFwEAAAAAwPbZLaH6rZL88szcJsl7kjz4MM9/ZZJ7tP3sJB9Pcrdl+92TvGrT2O9J8sGZuX2Sn0pyWpLMzI8k+dDMnDozZ6xa18ycNTNrM7N2ymEWDQAAAADAzrJbQvVLZmbf8nhvkj2Hef6rkpyeRYj+kiSf1fYzk+zZYrX76UmelyQzc2GSC49iXQAAAAAA7CInbncBK/rIhuPLs9yS5TC8Iclakr9P8tIk10/yXVkE4VuZY1QXAAAAAAC7yG5ZqX6lzMxHk/xjkm9M8rosVq4/Np+69Uuy2CrmjCRpe9skt9/Q97G2Vz+61QIAAAAAsFPtlpXqn6Ltg5L8UpJTkryk7b6Zud9BTnlVknvPzAfbvirJTbJ1qP6rSZ7T9sIk+5K8fkPfWUkubHt+kh877KJPOy1ZXz/s0wAAAAAA2Bk6s+pOJ1xZa2trsy5UBwAAAADY0drunZm1rfquEtu/AAAAAADAkbBrt3/ZStv7JXnKpuZLZuZB21EPAAAAAADHl+MqVJ+Zc5Kcs911AAAAAABwfLL9CwAAAAAArEioDgAAAAAAKxKqAwAAAADAioTqAAAAAACwIqE6AAAAAACs6MTtLuAqZe/epN3uKoCjYWa7KwAAAADgGLBSHQAAAAAAVrTrQ/W2e9pevEX7N7R9U9sr2q6tMM/z217Y9gePTqUAAAAAAOx2x/P2Lxcn+fokzzzUwLY3THLXmfn8LfpOnJmPH4X6AAAAAADYZXb9SvWlE9r++nJl+rltT5qZv5mZt654/rlJbtB2X9t7tD2v7ZPbviLJo9ue1vYVbfe2PaftjZJk2f7Gtq9t+9QDrJg/s+162/XLjuANAwAAAABw7B0vofqtkvzyzNwmyXuSPPgwz39gkrfPzKkz86pl23Vm5suT/GKSX0rykJk5Lcmzk/zUcsxzkjxqZu5yoIln5qyZWZuZtVMOsygAAAAAAHaW42X7l0tmZt/yeG+SPUdgzhcs32+d5LZJXto2SU5I8q62J2cRvL9iOe63k9z/CFwXAAAAAIAd6ngJ1T+y4fjyJCcdgTk/sHxvkjdtXo3e9jpJ5ghcBwAAAACAXeJ42f7laHprklPa3iVJ2l697W1m5j1J3tv27stxZ2xXgQAAAAAAHBvHy0r1T9H2QVnshX5Kkpe03Tcz9zvceWbmo20fkuQXl1u+nJjk6UnelOQRSZ7d9oNJzjnkZKedlqyvH24JAAAAAADsEJ2xg8mR0HZPkhfPzG0PNGZtbW3WheoAAAAAADta270zs7ZVn+1fAAAAAABgRcft9i9baXu/JE/Z1HzJzDzoys49M5cmOeAqdQAAAAAAdr+rVKg+M+dklb3PAQAAAABgC7Z/AQAAAACAFQnVAQAAAABgRUJ1AAAAAABYkVAdAAAAAABWJFQHAAAAAIAVnbjdBVyl7N2btNtdBWyPme2uAAAAAACuNCvVj4C292z74u2uAwAAAACAo0uoDgAAAAAAK7L9ywraXivJ7yW5SZITkjwpyXuTPD3Ju5Ocv23FAQAAAABwzAjVV/OVSd45M1+dJG1PTnJxkq9I8ndJXrCNtQEAAAAAcIzY/mU1FyW5T9untL1HkpsluWRm/nZmJsnzDnRi2zPbrrddv+xYVQsAAAAAwFEhVF/BzLwtyWlZhOs/neSBSWbFc8+ambWZWTvlKNYIAAAAAMDRZ/uXFbS9cZJ/n5nntX1/kkcmuVnbW8zM25N88/ZWCAAAAADAsSBUX83tkjy17RVJPpbke5JcP8lL2r47yauT3HYb6wMAAAAA4BgQqq9gZs5Jcs4WXV94WBOddlqyvn5EagIAAAAA4NizpzoAAAAAAKxIqA4AAAAAACsSqgMAAAAAwIqE6gAAAAAAsCKhOgAAAAAArEioDgAAAAAAKxKqAwAAAADAioTqAAAAAACwIqE6AAAAAACs6MTtLuAqZe/epN3uKkiSme2uAAAAAADYhaxUT9L2Hm3f1HZf25O2ux4AAAAAAHYmofrCGUl+bmZOnZkP7W9se8I21gQAAAAAwA6zI0P1tnvavqXts9pe3Pbstvdp+5q2f9v2zm2v1fbZbd/Q9oK2X7vh3Fe1PX/5uuuy/Z5tz2v7wuXcZ3fhO5N8Y5LHL9vu2fblbX8nyUVtT2j71OV1Lmz73cv52vYZbd/c9iVt/7TtQ7btSwMAAAAA4KjbyXuq3zLJNyQ5M8kbkjwsyd2TPDDJ45K8Oclfzsy3t71Okte3/Ysk/5rkv8/Mh9veKsnzk6wt57xjktskeWeS1yS528w8q+3dk7x4Zl7Y9p5J7pzktjNzSdszk7x3Zu7U9hpJXtP23OVct05yuySfu6zn2Uf1GwEAAAAAYFvt5FD9kpm5KEnavinJy2Zm2l6UZE+SmyR5YNvHLsdfM8lNswjMn9H21CSXJ/mCDXO+fmb+aTnnvuU8r97i2q+fmUuWx/dNcvsNq9BPTnKrJKcnef7MXJ7knW3/cqubWIbyZ2ZZHAAAAAAAu9dODtU/suH4ig2fr8ii7suTPHhm3rrxpLZPSPIvSe6QxfY2Hz7AnJfnwPf/gY1TJvmBmTln03W+Kskc6iZm5qwkZyXJWnvI8QAAAAAA7Fw7ck/1FZ2T5AfaNkna3nHZfnKSd83MFUm+JcmVfdjoOUm+p+3Vl9f5grbXSvLKJN+03HP9RknudSWvAwAAAADADreTV6ofypOSPD3Jhctg/dIkX5PkV5L8QdtvSPLyfPKq80/Hs7LYJub85XUuS/J1Sf4oyVckuSjJ25K84kpeBwAAAACAHa4zdiQ5Eto+N8uHnR5ozFo768euJA7Gzz0AAAAAcABt987M2lZ9u3ml+u5z2mnJulgdAAAAAGC3EqofITPz8O2uAQAAAACAo2s3P6gUAAAAAACOKaE6AAAAAACsSKgOAAAAAAArEqoDAAAAAMCKhOoAAAAAALAioToAAAAAAKxIqA4AAAAAACs6cbsLuErZuzdpt7uK3WdmuysAAAAAAEhipToAAAAAAKzsuFmp3vYJSd6f5MVJfjfJJHnIzLx9O+sCAAAAAOD4cTyuVP+6JH88M3fcGKh34Xi8XwAAAAAAjpFdHTK3/bG2b237F0luneQzkzwmyXe2fXnbPW3/pu2vJDk/yX9r+6tt19u+qe0TN8x1adsntj2/7UVtv3DZfkrbly7bn9n2HW2vv+z7H21f33bfsu+EY/4lAAAAAABwzOzaUL3taUm+Kckdk3x9kjsl+WCSX0vytJm513LorZP81nLl+juS/NjMrCW5fZIvb3v7DdO+e2a+JMmvJnnssu0nkvzlsv2Pktx0ef0vSvLQJHebmVOTXJ7kjC3qPHMZ4q9fduRuHwAAAACAbbCb91S/R5I/mpkPJknbFx1g3Dtm5nUbPn9j2zOzuPcbJfniJBcu+/5w+b43i6A+Se6e5EFJMjN/3vY/lu33TnJakje0TZKTkvzr5ovPzFlJzkqStXYO8x4BAAAAANhBdnOoniweRnooH9h/0PZmWaxAv9PM/Efb5ya55oaxH1m+X55PfDc9wLxN8psz86OHVTEAAAAAALvWrt3+Jckrkzyo7Ultr53kASuc89lZhOzvbfu5Se6/wjmvTvKNSdL2vkk+Z9n+siQPaXuDZd91237+Yd4DAAAAAAC7yK5dqT4z57d9QZJ9Sd6R5FUrnPPGthckeVOSv0/ymhUu9cQkz2/70CSvSPKuJP85M+9u++NJzm17tSQfS/J9y1oAAAAAADgOdcY23wfT9hpJLp+Zj7e9S5JfXT6Y9LCtra3N+vr6Ea0PAAAAAIAjq+3emVnbqm/XrlQ/hm6a5PeWq9E/muS7trkeAAAAAAC2iVD9EGbmb5PccbvrAAAAAABg++3mB5UCAAAAAMAxJVQHAAAAAIAVCdUBAAAAAGBFQnUAAAAAAFiRUB0AAAAAAFYkVAcAAAAAgBWduN0FXKXs3Zu0213F7jOz3RUAAAAAACSxUh0AAAAAAFYmVN+g7aVtr7/ddQAAAAAAsDMJ1QEAAAAAYEW7JlRvu6ftW9o+q+3Fbc9ue5+2r2n7t23vvHz9VdsLlu+3Xp77Q22fvTy+3fL8z2x7vbbnLsc/M0k3XO+HluMubvuYVWvYju8GAAAAAIBjY9eE6ku3TPILSW6f5AuTPCzJ3ZM8Nsnjkrwlyekzc8ckj0/y5OV5T09yy7YPSvKcJN89Mx9M8hNJXr0c/6IkN02StqcleUSSL03yZUm+q+0dV6zhk7Q9s+162/XLjtz3AAAAAADANjhxuws4TJfMzEVJ0vZNSV42M9P2oiR7kpyc5Dfb3irJJLl6kszMFW0fnuTCJM+cmdcs5zs9ydcvx7yk7X8s2++e5I9m5gPLa/1hkntkEbwfqoZPMjNnJTkrSdbaOYLfBQAAAAAAx9huW6n+kQ3HV2z4fEUWvyB4UpKXz8xtkzwgyTU3jL9VkvcnufGmObcKurtF26o1AAAAAABwnNptofqhnJzkn5fHD9/f2PbkLLZsOT3J9do+ZNn1yiRnLMfcP8nnbGj/uuW+69dK8qAkrzrq1QMAAAAAsKMdb6H6zyb56bavSXLChvanJfmVmXlbku9I8jNtb5DkiUlOb3t+kvsm+YckmZnzkzw3yeuT/HWSZ83MBcfsLgAAAAAA2JE6Y5vvY2VtbW3W19e3uwwAAAAAAA6i7d6ZWduq73hbqQ4AAAAAAEeNUB0AAAAAAFYkVAcAAAAAgBUJ1QEAAAAAYEVCdQAAAAAAWJFQHQAAAAAAViRUBwAAAACAFQnVAQAAAABgRUJ1AAAAAABY0YnbXcBVyt69SbvdVeweM9tdAQAAAADAJ7FS/TC0fXjbZ2zRfkrbv257Qdt7bEdtAAAAAAAcfVaqHxn3TvKWmfm27S4EAAAAAICjZ1tWqrfd0/YtbZ/V9uK2Z7e9T9vXtP3btndue622z277huUK8K/dcO6r2p6/fN112X7Ptue1feFy7rPbxV4rbS9t+5S2r1++brlsP6XtHyyv8Ya2d1u237ntXy2v+1dtb73FPXx129e2XUvys0m+qu2+ticdq+8RAAAAAIBjaztXqt8yyTckOTPJG5I8LMndkzwwyeOSvDnJX87Mt7e9TpLXt/2LJP+a5L/PzIfb3irJ85OsLee8Y5LbJHlnktckuVuSVy/73jczd277rUmenuRrkvxCkqfNzKvb3jTJOUm+KMlbkpw+Mx9ve58kT07y4P2Ft31Qkh9K8lUz8x9tH59kbWa+f/NNtj1zeY+56ZX8wgAAAAAA2F7bGapfMjMXJUnbNyV52cxM24uS7ElykyQPbPvY5fhrZpFLvzPJM9qemuTyJF+wYc7Xz8w/Lefct5xnf6j+/A3vT1se3yfJF/cTDw/97LbXTnJykt9chvaT5OobrnGvLEL8+87M+w51kzNzVpKzkmSt9eRNAAAAAIBdbDtD9Y9sOL5iw+crsqjr8iQPnpm3bjyp7ROS/EuSO2Sxfc2HDzDn5fnk+5stjq+W5C4z86FN1/ilJC+fmQe13ZPkvA3df5/k5lmE+esHu0EAAAAAAI4v27Kn+orOSfIDG/ZFv+Oy/eQk75qZK5J8S5ITVpzvoRveX7s8PjfJf23Zslz9vv8a/7w8fvimed6R5OuT/Fbb26x4bQAAAAAAjgM7OVR/UhbbrlzY9uLl5yT5lSTf1vZ1WawW/8CK812j7V8neXSSH1y2PSrJWtsL2745ySOX7T+b5KfbviZbhPbL1fNnJPn9trc4/FsDAAAAAGA36szxv81320uzeJDou7ezjrW1tVlft2MMAAAAAMBO1nbvzKxt1beTV6oDAAAAAMCOsp0PKj1mZmbPdtcAAAAAAMDuZ6U6AAAAAACsSKgOAAAAAAArEqoDAAAAAMCKhOoAAAAAALAioToAAAAAAKxIqA4AAAAAACsSqgMAAAAAwIpO3O4CrlL27k3a7a5iZ5nZ7goAAAAAAFZmpfomba/R9i/a7mv70O2uBwAAAACAncNK9U91xyRXn5lTt7sQAAAAAAB2ll29Ur3tnrZvafusthe3Pbvtfdq+pu3ftr3z8vVXbS9Yvt96ee4PtX328vh2y/NvmuR5SU5drlS/RdtL215/OW6t7XnL4ye0fXbb89r+fdtHbdPXAAAAAADAMbKrQ/WlWyb5hSS3T/KFSR6W5O5JHpvkcUnekuT0mbljkscnefLyvKcnuWXbByV5TpLvnpl/SPKdSV41M6fOzNsPce0vTHK/JHdO8hNtr755QNsz2663Xb/syt0nAAAAAADb7HjY/uWSmbkoSdq+KcnLZmbaXpRkT5KTk/xm21slmSRXT5KZuaLtw5NcmOSZM/OaT+PaL5mZjyT5SNt/TfK5Sf5p44CZOSvJWUmy1noqJwAAAADALnY8rFT/yIbjKzZ8viKLXxo8KcnLZ+a2SR6Q5Jobxt8qyfuT3Pgg8388n/ierrmpb+O1L8/x8UsKAAAAAAAO4HgI1Q/l5CT/vDx++P7GtidnsW3M6Umu1/YhBzj/0iSnLY8ffHRKBAAAAABgN7gqhOo/m+Sn274myQkb2p+W5Fdm5m1JviPJz7S9wRbnPzHJL7R9VRar0QEAAAAAuIrqjG2+j5W1tbVZX1/f7jIAAAAAADiItntnZm2rvqvCSnUAAAAAADgihOoAAAAAALAioToAAAAAAKxIqA4AAAAAACsSqgMAAAAAwIqE6gAAAAAAsCKhOgAAAAAArEioDgAAAAAAKxKqAwAAAADAik7c7gKuUvbuTdrtrmJnmdnuCgAAAAAAVrZrVqq3fVTbv2l79hGe9zPaPr3t29v+XdsXt73psu/WbfdteL2v7WOWfU9o+88b+r7qSNYFAAAAAMDOs5tWqn9vkvvPzCX7G9qeODMfv5LzPjnJtZN8wcxc3vYRSf647Wkz89Ykpy6vdUKSf07yRxvOfdrM/NyVvD4AAAAAALvErlip3vbXktw8yYvavrftWW3PTfJbbU9p+wdt37B83W15zvXantv2grbPbPuOttffNO9nJnlEkh+cmcuTZGaek+T9Se6zqYx7J3n7zLzjKN8uAAAAAAA71K4I1WfmkUnemeReSZ6W5LQkXzszD0vyC1msGL9TkgcnedbytJ9I8uqZuWOSFyW56RZT3zLJP8zM+za1ryf54k1t35Tk+Zvavr/thW2f3fZzPr27AwAAAABgt9gVofoWXjQzH1oe3yfJM9ruyyI8/+y2105yepLnJcnMvCTJf2wxT5Ns9aTMT3qaaNvPSPLAJL+/oflXk9wii+1h3pXk57cqtO2Zbdfbrl+20q0BAAAAALBT7aY91Tf6wIbjqyW5y4aQPUnSNtk6MN/o75J8fttrz8x/bmj/kiQv3PD5/knOn5l/2d+w8bjtryd58VYXmJmzkpyVJGvtoeoBAAAAAGAH260r1Tc6N8n37//Q9tTl4SuTnLFsu3+ST9meZWY+kOQ3k/zv5YNI0/Zbk3w4yWs2DP3mbNr6pe2NNnx8UJKLr+R9AAAAAACwwx0Pofqjkqwt9zZ/c5JHLtufmOT0tucnuW+Sf9h/Qts/bXvj5ccfTfKhJG9t+89JfiiL/dpnOfYzk/z3JH+46bo/2/aithdmsdf7Dx6d2wMAAAAAYKfoMjs+7rW9NMnazLz7IGNumOTPk/zKctuWI2ptbW3W19eP9LQAAAAAABxBbffOzNpWfbt1T/WjYmb+bxYPHgUAAAAAgE9xlQnVZ2bPdtcAAAAAAMDudjzsqQ4AAAAAAMeEUB0AAAAAAFYkVAcAAAAAgBUJ1QEAAAAAYEVCdQAAAAAAWJFQHQAAAAAAViRUBwAAAACAFZ243QVcpezdm7TbXcXOMbPdFQAAAAAAHBYr1QEAAAAAYEW7MlRv+6i2f9P27CM873lt15bHJ7f9rbZvX77Obvs5y749bT/Udl/bNy/HXf1I1gIAAAAAwM6zK0P1JN+b5Ktm5oz9DW2P9FY2v5Hk72fmFjNziyR/l+S5G/rfPjOnJrldkpsk+cYjfH0AAAAAAHaYXReqt/21JDdP8qK27217Vttzk/xW21Pa/kHbNyxfd1uec72257a9oO0z276j7fUPco1bJjktyZM2NP9kkju0vfXGsTNzeZLXJ/m8I3yrAAAAAADsMLsuVJ+ZRyZ5Z5J7JXlaFuH3187Mw5L8QpKnzcydkjw4ybOWp/1EklfPzB2TvCjJTQ9xmS9Osm8ZmO+/7uVJLkjyRRsHtr1mki9N8udbTdT2zLbrbdcvO6w7BQAAAABgpznSW6ZshxfNzIeWx/dJ8sVt9/d9dttrJzk9ydcnycy8pO1/HGLOJpkDtO93i7b7ktwqyQtn5sKtJpqZs5KclSRr7VZzAgAAAACwSxwPofoHNhxfLcldNoTsSZJlyH44gfabktyx7dVm5orlHFdLcvsk5y+v8/aZObXtjZKc1/aBM/OiK3EfAAAAAADscLtu+5dDODfJ9+//0PbU5eErk5yxbLt/ks852CQz83dZbPXy4xuafzzJy2bmHzaNfVeSH0nyo1eydgAAAAAAdrjjLVR/VJK1the2fXOSRy7bn5jk9LbnJ7lvkv8Kxtv+adsbbzHXtye5Vdu/a3tZki/bMN9m/yfJZ7a9xxG6DwAAAAAAdqDOXPW2+W57aZK1mXn3iuNvneRPk/zAzPzpp3vdtbW1WV9f/3RPBwAAAADgGGi7d2bWtuo7HvZUP+pm5q1JbrHddQAAAAAAsL2ukqH6zOzZ7hoAAAAAANh9jrc91QEAAAAA4KgRqgMAAAAAwIpWCtXb3qLtNZbH92z7qLbXOaqVAQAAAADADrPqSvU/SHJ521sm+Y0kN0vyO0etKgAAAAAA2IFWDdWvmJmPJ3lQkqfPzA8mudHRKwsAAAAAAHaeVUP1j7X95iTfluTFy7arH52SAAAAAABgZzpxxXGPSPLIJD81M5e0vVmS5x29so5Te/cm7XZXsf1mtrsCAAAAAIBPS2fFgLPtSUluOjNvPbolHb/W2lnf7iJ2AqE6AAAAALCDtd07M2tb9a20/UvbByTZl+TPl59PbfuiI1bhMdT2/Qdof27bhxzregAAAAAA2D1W3VP9CUnunOQ9STIz+5Lc7KhUBAAAAAAAO9SqofrHZ+a9m9q2bQ+Ptv+j7evb7mv7zLYntH1/259q+8a2r2v7ucuxN2v72rZvaPukDXO07TPavrntS5LcYEPfvdte0Paits9ue41l+6Vtn7ycb73tl7Q9p+3b2z7ymH8RAAAAAAAcU6uG6he3fViSE9requ0vJfmro1jXAbX9oiQPTXK3mTk1yeVJzkhyrSSvm5k7JHllku9anvILSX51Zu6U5P9umOpBSW6d5HbLsXddzn/NJM9N8tCZuV0WD3P9ng3n/ePM3CXJq5bjHpLky5L85AHqPXMZwK9fdqXuHAAAAACA7bZqqP4DSW6T5CNJfifJe5M85ijVdCj3TnJakje03bf8fPMkH03y4uWYvUn2LI/vluT5y+Pf3jDP6UmePzOXz8w7k/zlsv3WSS6ZmbctP//mcux++/eSvyjJX8/Mf87MZUk+3PY6m4udmbNmZm1m1k75NG4WAAAAAICd48RDDWh7QpIXzcx9kvzY0S/pkJrkN2fmRz+psX3szOzfkubyfPK9HWirmq3ae4jrf2T5fsWG4/2fD/l9AgAAAACwex1ypfrMXJ7kg21PPgb1rOJlSR7S9gZJ0va6bT//IONfk+SblsdnbGh/ZZJvWu7HfqMk91q2vyXJnra3XH7+liSvOGLVAwAAAACwa626svrDSS5q+9IkH9jfODOPOipVHcTMvLntjyc5t+3Vknwsyfcd5JRHJ/mdto9O8gcb2v8oyVdksY3L27IMzmfmw20fkeT3256Y5A1Jfu3I3wkAAAAAALtNP7FjykEGtd+2VfvM/OYRr+g4tra2Nuvr69tdBgAAAAAAB9F278ysbdW30kp14TkAAAAAAKwYqre9JFs81HNmbn7EKwIAAAAAgB1q1T3VNy5zv2aSb0hy3SNfDgAAAAAA7FxXW2XQzPzbhtc/z8zTs3jIJwAAAAAAXGWsuv3Ll2z4eLUsVq5f+6hUBAAAAAAAO9Sq27/8/Ibjjye5JMk3HvlyAAAAAABg51o1VP+Omfn7jQ1tb3YU6gEAAAAAgB1rpT3Vk7xwxTYAAAAAADhuHXSletsvTHKbJCe3/foNXZ+d5JpHs7Dj0t69SbvdVWxtZrsrAAAAAADY8Q61/cutk3xNkuskecCG9v9M8l1HqSYAAAAAANiRDhqqz8wfJ/njtneZmdceo5p2nbaPSXLWzHxwu2sBAAAAAODoWfVBpRe0/b4stoL5r21fZubbj0pVu89jkjwviVAdAAAAAOA4tuqDSn87yQ2T3C/JK5LcJIstYHasttdq+5K2b2x7cduHtr207VPavn75uuUW553Q9qlt39D2wrbfvWy/Z9vz2r6w7Vvant2FRyW5cZKXt335sb5PAAAAAACOnVVD9VvOzP+b5AMz85tJvjrJ7Y5eWUfEVyZ558zcYWZum+TPl+3vm5k7J3lGkqdvcd53JHnvzNwpyZ2SfFfbmy377pjFqvQvTnLzJHebmV9M8s4k95qZe22erO2Zbdfbrl925O4NAAAAAIBtsGqo/rHl+3va3jbJyUn2HJWKjpyLktxnuTL9HjPz3mX78ze832WL8+6b5Fvb7kvy10mul+RWy77Xz8w/zcwVSfZlhe9gZs6ambWZWTvl074VAAAAAAB2glX3VD+r7eck+X+TvCjJZyV5/FGr6giYmbe1PS3JVyX56bbn7u/aOGyLU5vkB2bmnE9qbO+Z5CMbmi7P6t8fAAAAAADHgZVWqs/Ms2bmP2bmFTNz85m5wcz82tEu7spoe+MkH5yZ5yX5uSRfsux66Ib3125x6jlJvqft1ZfzfEHbax3icv+Z5NpXvmoAAAAAAHaylVZat/3cJE9OcuOZuX/bL05yl5n5jaNa3ZVzuyRPbXtFFtvXfE+SFya5Rtu/zuIXCt+cJG0fmGRtZh6f5FlZbOtyftsmuSzJ1x3iWmcl+bO279pqX3UAAAAAAI4PndlqB5RNg9o/S/KcJD82M3doe2KSC2Zmpz+s9JO0vTSL8Pzd23H9tbW1WV9f345LAwAAAACworZ7Z2Ztq75VH1R6/Zn5vSRXJMnMfDyLPcUBAAAAAOAqY9UHbX6g7fWyfLBn2y9L8t6jVtVRMjN7trsGAAAAAAB2r1VD9R9K8qIkt2j7miSnJHnIUasKAAAAAAB2oIOG6m1vOjP/MDPnt/3yJLdO0iRvnZmPHZMKAQAAAABghzjUnur/Z8PxC2bmTTNzsUAdAAAAAICrokOF6t1wfPOjWQgAAAAAAOx0hwrV5wDHAAAAAABwlXOoB5Xeoe37slixftLyOMvPMzOffVSrAwAAAACAHeSgofrMnHCsCgEAAAAAgJ3uUCvVOZL27k3aQ4/bDmN3HwAAAACAQznUnuo7StvrtP3e5fGN275wu2tKkraPafuZ210HAAAAAABH164K1ZNcJ8n3JsnMvHNmHrK95fyXxyQRqgMAAAAAHOd2W6j+M0lu0XZf299ve3GStH142//T9k/aXtL2+9v+UNsL2r6u7XWX427R9s/b7m37qrZfuPkCbU9o+9S2b2h7YdvvXrbfs+15bV/Y9i1tz+7Co5LcOMnL2778GH4XAAAAAAAcY7stVP+RJG+fmVOT/M9NfbdN8rAkd07yU0k+ODN3TPLaJN+6HHNWkh+YmdOSPDbJr2xxje9I8t6ZuVOSOyX5rrY3W/bdMYtV6V+c5OZJ7jYzv5jknUnuNTP32jxZ2zPbrrddv+zTu2cAAAAAAHaI4+lBpS+fmf9M8p9t35vkT5btFyW5fdvPSnLXJL/fTzws9BpbzHPf5fj9W8ucnORWST6a5PUz809J0nZfkj1JXn2wombmrCzC/Ky1ngYKAAAAALCLHU+h+kc2HF+x4fMVWdzn1ZK8Z7nK/WCaxWr2cz6psb3npmtcnuPr+wMAAAAA4BB22/Yv/5nk2p/OiTPzviSXtP2GJFnuh36HLYaek+R72l59Oe4L2l7raNUFAAAAAMDusatC9Zn5tySvWT6g9KmfxhRnJPmOtm9M8qYkX5skbR/Y9ieXY56V5M1Jzl9e55k59Ir0s5L8mQeVAgAAAAAc3zpjm+9jZW1tbdbX17e7DAAAAAAADqLt3plZ26pvV61UBwAAAACA7SRUBwAAAACAFQnVAQAAAABgRUJ1AAAAAABYkVAdAAAAAABWJFQHAAAAAIAVCdUBAAAAAGBFQnUAAAAAAFiRUB0AAAAAAFZ04nYXcJWyd2/SbncVn2pmuysAAAAAANgVrFTfQtt7tH1T231tT9ruegAAAAAA2BmE6ls7I8nPzcypM/Oh7S4GAAAAAICdYVeE6m33tH1L22e1vbjt2W3v0/Y1bf+27Z3bXqvts9u+oe0Fbb92w7mvanv+8nXXZfs9257X9oXLuc/uwncm+cYkj1+23bPtizfU8oy2D18eX9r2ict5L2r7hdvw9QAAAAAAcIzspj3Vb5nkG5KcmeQNSR6W5O5JHpjkcUnenOQvZ+bb214nyevb/kWSf03y32fmw21vleT5SdaWc94xyW2SvDPJa5LcbWae1fbuSV48My9se89D1PXumfmStt+b5LFJvvNI3TAAAAAAADvLbgrVL5mZi5Kk7ZuSvGxmpu1FSfYkuUmSB7Z97HL8NZPcNIvA/BltT01yeZIv2DDn62fmn5Zz7lvO8+rDrOsPl+97k3z95s62Z2bxi4Dc9DAnBgAAAABgZ9lNofpHNhxfseHzFVncx+VJHjwzb914UtsnJPmXJHfIYrubDx9gzsuz9ffx8XzyNjnXPEBdW54/M2clOStJ1trZYn4AAAAAAHaJXbGn+orOSfIDbZskbe+4bD85ybtm5ook35LkhMOc9x1JvrjtNdqenOTeR6pgAAAAAAB2l+MpVH9SkqsnubDtxcvPSfIrSb6t7euy2PrlA4cz6cz8Y5LfS3JhkrOTXHDEKgYAAAAAYFfpjB1JjpW1tbVZX1/f7jIAAAAAADiItntnZm2rvuNppToAAAAAABxVQnUAAAAAAFiRUB0AAAAAAFYkVAcAAAAAgBUJ1QEAAAAAYEVCdQAAAAAAWJFQHQAAAAAAViRUBwAAAACAFQnVAQAAAABgRSdudwFXKXv3Ju12V/HJZra7AgAAAACAXcNKdQAAAAAAWJFQ/RDa3qPtm9rua3vSdtcDAAAAAMD2Eaof2hlJfm5mTp2ZD213MQAAAAAAbB97qm/Q9lpJfi/JTZKckOS3k3xjkvu1vU+SX0/yxCT/kuTUJH+Y5KIkj05yUpKvm5m3H/vKAQAAAAA4FoTqn+wrk7xzZr46SdqenOSLkrx4Zl7Y9p5J7rBs+/ckf5/kWTNz57aPTvIDSR6zccK2ZyY5M0luemzuAQAAAACAo8T2L5/soiT3afuUtveYmfduMeYNM/OumflIkrcnOXfDuXs2D56Zs2ZmbWbWTjlqZQMAAAAAcCxYqb7BzLyt7WlJvirJT7c9d4thH9lwfMWGz1fE9wkAAAAAcFwTAm/Q9sZJ/n1mntf2/UkenuQ921oUAAAAAAA7hlD9k90uyVPbXpHkY0m+J8n3b29JAAAAAADsFJ2Z7a7hKmNtbW3W19e3uwwAAAAAAA6i7d6ZWduqz4NKAQAAAABgRUJ1AAAAAABYkVAdAAAAAABWJFQHAAAAAIAVCdUBAAAAAGBFQnUAAAAAAFiRUB0AAAAAAFYkVAcAAAAAgBWduN0FXKXs3Zu0213Fwsx2VwAAAAAAsOtYqQ4AAAAAACs67kL1tk9o+9i2X9h2X9sL2t7iAGPPa7t2rGsEAAAAAGB3Ou5C9Q2+Lskfz8wdZ+bt210MAAAAAAC733ERqrf9sbZvbfsXSW6d5DOTPCbJd7Z9edtrtX1J2ze2vbjtQ7eY45vbXrTsf8qG9ve3/fm257d9WdtTlu23aPvnbfe2fVXbLzxGtwsAAAAAwDbZ9aF629OSfFOSOyb5+iR3SvLBJL+W5Gkzc68kX5nknTNzh5m5bZI/3zTHjZM8JclXJDk1yZ3aft2y+1pJzp+ZL0nyiiQ/sWw/K8kPzMxpSR6b5FcOUN+Zbdfbrl92ZG4ZAAAAAIBtsutD9ST3SPJHM/PBmXlfkhdtMeaiJPdp+5S295iZ927qv1OS82bmspn5eJKzk5y+7LsiyQuWx89Lcve2n5Xkrkl+v+2+JM9McqOtipuZs2ZmbWbWTrkSNwkAAAAAwPY7cbsLOELmoJ0zb1uuaP+qJD/d9tyZ+ckNQ3qY17pakvfMzKmHXSkAAAAAALvW8bBS/ZVJHtT2pLbXTvKAzQOW27t8cGael+TnknzJpiF/neTL216/7QlJvjmLrV6SxXf0kOXxw5K8erki/pK237Ccv23vcKRvDAAAAACAnWXXr1SfmfPbviDJviTvSPKqLYbdLslT216R5GNJvmfTHO9q+6NJXp7FqvU/nZk/XnZ/IMlt2u5N8t4k+x9yekaSX23740munuR3k7zxSN4bAAAAAAA7S2cOunPKVV7b98/MZx2JudbW1mZ9ff1ITAUAAAAAwFHSdu/MrG3Vdzxs/wIAAAAAAMeEUP0QjtQqdQAAAAAAdj+hOgAAAAAArEioDgAAAAAAKxKqAwAAAADAioTqAAAAAACwIqE6AAAAAACsSKgOAAAAAAArOnG7C7hK2bs3abe7imRmuysAAAAAANiVrFQHAAAAAIAVXSVD9baXtr3+8vivDvPce7Z98dGpDAAAAACAnewqGapvNDN33e4aAAAAAADYHXZNqN52T9u3tH1W24vbnt32Pm1f0/Zv29657XXb/p+2F7Z9XdvbL8+9Xttz217Q9plJumHe9y/f79n2vLYvXF7n7HaxAXrbr1y2vTrJ12849xfbPn55fL+2r2y7a75TAAAAAAAOz24LgG+Z5BeS3D7JFyZ5WJK7J3lskscleWKSC2bm9svPv7U87yeSvHpm7pjkRUlueoD575jkMUm+OMnNk9yt7TWT/HqSByS5R5Ibbhj/I0ke2vZeSX4xySNm5oqNE7Y9s+162/XLrsSNAwAAAACw/XZbqH7JzFy0DK7flORlMzNJLkqyJ4uA/beTZGb+Msn12p6c5PQkz1u2vyTJfxxg/tfPzD8t59+3nPMLl9f92+W1nrd/8Mx8MMl3JXlpkmfMzNs3TzgzZ83M2sysnXJl7x4AAAAAgG2120L1j2w4vmLD5yuSnJgN27psMJveV53/8uWchzr3dkn+LcmNV5gfAAAAAIBdbLeF6ofyyiRnJIs90pO8e2bet6n9/kk+5zDmfEuSm7W9xfLzN+/vaPv5SX44i21j7t/2S69k/QAAAAAA7GDHW6j+hCRrbS9M8jNJvm3Z/sQkp7c9P8l9k/zDqhPOzIeTnJnkJcsHlb4jSZYPMf2NJI+dmXcm+Y4kz1ruwQ4AAAAAwHGoi23CORbW1tZmfX19u8sAAAAAAOAg2u6dmbWt+o63leoAAAAAAHDUCNUBAAAAAGBFQnUAAAAAAFiRUB0AAAAAAFYkVAcAAAAAgBUJ1QEAAAAAYEVCdQAAAAAAWJFQHQAAAAAAViRUBwAAAACAFZ243QVcpezdm7TbW8PM9l4fAAAAAGAXs1I9Sds9bS/+NM+9TtvvPdI1AQAAAACw8wjVr4S2JyS5ThKhOgAAAADAVYBQfZO2N297QduXtH3Ihvb3L9/v2fblbX8nyUVJfibJLdrua/vUbSobAAAAAIBjwJ7qG7S9dZLfTfKIJI85yNA7J7ntzFzSds/y+NQDzHlmkjOT5KZHslgAAAAAAI45K9U/4ZQkf5zkf8zMvkOMff3MXLLKpDNz1syszczaKVe2QgAAAAAAtpVQ/RPem+Qfk9xt+fnjWX4/bZvkMzaM/cCxLQ0AAAAAgJ3A9i+f8NEkX5fknOX+6ZcmOS3J7yX52iRXP8B5/5nk2segPgAAAAAAtpmV6hvMzAeSfE2SH8xi1fqXt319ki/NAVanz8y/JXlN24s9qBQAAAAA4PjWmdnuGq4y1tbWZn19fbvLAAAAAADgINrunZm1rfqsVAcAAAAAgBUJ1QEAAAAAYEVCdQAAAAAAWJFQHQAAAAAAViRUBwAAAACAFQnVAQAAAABgRUJ1AAAAAABYkVAdAAAAAABWJFQHAAAAAIAVnbjdBVyl7N2btNtz7ZntuS4AAAAAwHHkKrVSve3Xtf3i7a4DAAAAAIDd6SoVqif5uiRbhuptrdoHAAAAAOCgdn2o3vZ/tH19231tn9n2hLbvb/tTbd/Y9nVtP7ftXZM8MMlTl2Nv0fa8tk9u+4okj25777YXtL2o7bPbXmN5jUvbPmV5nde3vWXba7e9pO3Vl2M+eznu6tv4dQAAAAAAcBTt6lC97RcleWiSu83MqUkuT3JGkmsled3M3CHJK5N818z8VZIXJfmfM3PqzLx9Oc11ZubLk/xykucmeejM3C6L/ea/Z8Pl3jczd07yjCRPn5n/THJekq9e9n9Tkj+YmY8drfsFAAAAAGB77epQPcm9k5yW5A1t9y0/3zzJR5O8eDlmb5I9B5njBcv3Wye5ZGbetvz8m0lO3zDu+Rve77I8flaSRyyPH5HkOZsnb3tm2/W265etdk8AAAAAAOxQu30f8Sb5zZn50U9qbB87M7P8eHkOfp8f2DDXwczm45l5Tds9bb88yQkzc/GnnDRzVpKzkmStnc39AAAAAADsHrt9pfrLkjyk7Q2SpO11237+Qcb/Z5JrH6DvLUn2tL3l8vO3JHnFhv6Hbnh/7Yb238pi9fqnrFIHAAAAAOD4sqtD9Zl5c5IfT3Ju2wuTvDTJjQ5yyu8m+Z/Lh5HeYtNcH85iC5ffb3tRkiuS/NqGIddo+9dJHp3kBze0n53kc/KJ7WEAAAAAADhO9RO7pHAgbS9NsjYz796i7yFJvnZmvuVQ86ytrc36+vpRqBAAAAAAgCOl7d6ZWduqb7fvqb6t2v5Skvsn+artrgUAAAAAgKNPqL6CmdlzgPYfOMalAAAAAACwjXb1nuoAAAAAAHAsCdUBAAAAAGBFQnUAAAAAAFiRUB0AAAAAAFYkVAcAAAAAgBUJ1QEAAAAAYEVCdQAAAAAAWNGJ213AVcrevUl77K87c+yvCQAAAABwHNqVK9XbPqbtZ34a5z287Y0P0HfPti++8tUBAAAAAHC82pWhepLHJDmsUL3tCUkenmTLUB0AAAAAAA5lx2//0vZaSX4vyU2SnJDk97MIxl/e9t0zc6+2v5rkTklOSvLCmfmJ5bmXJnl2kvsm+bUka0nObvuhJHdJ8uVJnp7k3UnO33DN6y7Pu3mSDyY5c2YubPuEJDdLcqMkX5Dkh5J8WZL7J/nnJA+YmY8dre8CAAAAAIDttRtWqn9lknfOzB1m5rZZhODvTHKvmbnXcsyPzcxaktsn+fK2t99w/odn5u4z87wk60nOmJlTk0ySX0/ygCT3SHLDDec8MckFM3P7JI9L8lsb+m6R5KuTfG2S5yV5+czcLsmHlu0AAAAAAByndkOoflGS+7R9Stt7zMx7txjzjW3PT3JBktsk+eINfS84wLxfmOSSmfnbmZksAvL97p7kt5NkZv4yyfXanrzs+7PlavSLslg5/+cb6tyz+SJtz2y73nb9shVuFgAAAACAnWvHb/8yM29re1qSr0ry023P3djf9mZJHpvkTjPzH22fm+SaG4Z84GDTH6C9Bxn7kWVdV7T92DKQT5IrssX3OTNnJTkrSdbaA10PAAAAAIBdYMevVG974yQfXG7f8nNJviTJfya59nLIZ2cRnL+37edmsb/5gWw87y1Jbtb2FsvP37xh3CuTnLG8/j2TvHtm3nelbwYAAAAAgF1tx69UT3K7JE9te0WSjyX5niweMvpnbd+1fFDpBUnelOTvk7zmIHM9N8mvbXhQ6ZlJXtL23UleneS2y3FPSPKcthdm8aDSbzvidwUAAAAAwK7TT+xewtG21s76dlzYnzEAAAAAwMra7p2Zta36dvz2L8eV005bBNzH+gUAAAAAwBEhVAcAAAAAgBUJ1QEAAAAAYEVCdQAAAAAAWJFQHQAAAAAAViRUBwAAAACAFQnVAQAAAABgRUJ1AAAAAABYkVAdAAAAAABWdOJ2F3CVsndv0h67680cu2sBAAAAAFwFWKkOAAAAAAAr2jWhettHtf2btmcf4Xk/o+3T27697d+1fXHbmy77/lvbly+v+6a2j95w3hPa/nPbfcvXVx3JugAAAAAA2Hl20/Yv35vk/jNzyf6GtifOzMev5LxPTnLtJF8wM5e3fUSSP257WpKPJ/nhmTm/7bWT7G370pl58/Lcp83Mz13J6wMAAAAAsEvsipXqbX8tyc2TvKjte9ue1fbcJL/V9pS2f9D2DcvX3ZbnXK/tuW0vaPvMtu9oe/1N835mkkck+cGZuTxJZuY5Sd6f5D4z866ZOX/Z/p9J/ibJ5x2zGwcAAAAAYEfZFaH6zDwyyTuT3CvJ05KcluRrZ+ZhSX4hixXjd0ry4CTPWp72E0lePTN3TPKiJDfdYupbJvmHmXnfpvb1JF+8saHtniR3TPLXG5q/v+2FbZ/d9nO2qr3tmW3X265ftvIdAwAAAACwE+2KUH0LL5qZDy2P75PkGW33ZRGef/Zyq5bTkzwvSWbmJUn+Y4t5mmQO0P6JD+1nJfmDJI/ZEMD/apJbJDk1ybuS/PxWhc7MWTOzNjNrp6x8ewAAAAAA7ES7aU/1jT6w4fhqSe6yIWRPkrRNtg7MN/q7JJ/f9trL7V32+5IkL1zOc/UsAvWzZ+YP9w+YmX/ZcK1fT/LiT+M+AAAAAADYRXbrSvWNzk3y/fs/tD11efjKJGcs2+6f5FO2Z5mZDyT5zST/u+0Jy7HfmuTDSV7TRTL/G0n+Zmb+98Zz295ow8cHJbn4CN0PAAAAAAA71PEQqj8qydpyb/M3J3nksv2JSU5ve36S+yb5h/0ntP3TtjdefvzRJB9K8ta2/5zkh7LYr32S3C3JtyT5irb7lq+vWp73s20vanthFnu9/+BRvk8AAAAAALZZF9nx8a/tpUnWZubdBxlzwyR/nuRXZuasI13D2trarK+vH+lpAQAAAAA4gtrunZm1rfp2657qR8XM/N8sHjwKAAAAAACf4ioTqs/Mnu2uAQAAAACA3e142FMdAAAAAACOCaE6AAAAAACsSKgOAAAAAAArEqoDAAAAAMCKhOoAAAAAALAioToAAAAAAKzoxO0u4Cpl796kPTpzzxydeQEAAAAA+C9WqgMAAAAAwIqOSaje9lFt/6bt2cfiep+Otk9o+9jl8U+2vc921wQAAAAAwM5yrLZ/+d4k95+ZS/Y3tD1xZj5+jK5/WGbm8dtdAwAAAAAAO89RX6ne9teS3DzJi9q+t+1Zbc9N8lttT2n7B23fsHzdbXnO9dqe2/aCts9s+462199i7kvbPrnta9uut/2Stue0fXvbR24Y9z+X81/Y9okb2n+s7Vvb/kWSW29of27bhyyPH7889+Jl7V22n9f2KW1f3/Ztbe9xtL5DAAAAAAB2hqMeqs/MI5O8M8m9kjwtyWlJvnZmHpbkF5I8bWbulOTBSZ61PO0nkrx6Zu6Y5EVJbnqQS/zjzNwlyauSPDfJQ5J8WZKfTJK2901yqyR3TnJqktPant72tCTflOSOSb4+yZ0OMP8zZuZOM3PbJCcl+ZoNfSfOzJ2TPGZZ86doe+Yy8F+/7CA3AQAAAADA/9/evQdZVlV5Hv/+pBAUaBBB8TFQiA9EhEIuqICA3WgoYwNKGSD4oHVklEcPdJQOoR2KEs5g0xMzMohaEg7areJACwM+gB55Ng8hE4oqCgEFHCWgBWyE4lUNlWv+uCfbS5JZeSorM28+vp+IjDzn7H32XucSm0ut2qwz801X+ZdeF1bVk83xAcBOzeZvgD9JshmwL91EN1X14yQPr2285vcKYNOqWgWsSvJUki2AdzY/Nzf9NqWbZN8MOL+qngBIciGje3uSTwMvBLYEVgIXNW0/bH4PAgtHu7mqlgJLATpJreU5JEmSJEmSJEkzXD+S6o/3HD8PeGtPkh2AJsneNgG9uvk91HM8fL4ACPBfq+obI+Y4Ybw5kmwMnAl0quq3SU4GNh5l7jX057OUJEmSJEmSJE2jKS//Mo5LgeOGT5Isag6vAo5srr0beNF6zHEJ8NEkmzbjvSLJS5o53pvkBc3u+D8f5d7hBPpDzf2L1yMOSZIkSZIkSdIs1++k+l8CneYForcBwy8X/QKwb5Kb6JZu+c3wDUl+kuTlbSeoqkuB7wHXJVkBnAdsVlU3AT8AlgH/QLcm+8h7/wB8k25pmQuAG9fx+SRJkiRJkiRJc0iqZn6Z7yS/pluC5aF+x7I+Op1ODQwM9DsMSZIkSZIkSdJaJBmsqs5obf3eqS5JkiRJkiRJ0qwxK16uWVUL+x2DJEmSJEmSJEnuVJckSZIkSZIkqSWT6pIkSZIkSZIktWRSXZIkSZIkSZKklkyqS5IkSZIkSZLUkkl1SZIkSZIkSZJaMqkuSZIkSZIkSVJLC/odwLwyOAjJ5I9bNfljSpIkSZIkSZKew53qkiRJkiRJkiS1NGOT6km2SHJMc/zyJOdN0Tz7JLkhye1J7khybE/bXyW5LcnyJD9Lsl1P25oky5qfC6ciNkmSJEmSJEnSzDKTy79sARwDnFlV9wGLJ3uCJNsA3wMOqaqbkmwFXJLkvqo6H7gZ6FTVE0k+CfwNcFhz+5NVtWiyY5IkSZIkSZIkzVwzdqc6cCqwQ7MT/NwktwIkOSrJBUkuSnJPkuOaHeU3J7k+yZZNvx2SXJxkMMnVSXYcZY5jgbOr6iaAqnoI+DTwqeb88qp6oul7PfDKKX5mSZIkSZIkSdIMNpOT6icBdzW7wT81om1n4AhgT+BLwBNVtRtwHfDhps9S4Piq2h1YApw5yhxvAAZHXBsAdhql78eAn/acb5xkoEnkHzLWQyQ5uuk38OBYnSRJkiRJkiRJs8JMLv+yNpdX1SpgVZJHgIua6yuAXZJsCuwFnJtk+J6NRhknQI03WZIPAh1gv57L21bVfUleBVyWZEVV3TXy3qpaSjfBTycZdy5JkiRJkiRJ0sw1W5Pqq3uOh3rOh+g+0/OAP7Soeb6SbrK890Wju9PdrQ5AkgOAzwL7VdW/zdvUeaeq7k5yBbAb8JykuiRJkiRJkiRp7pjJ5V9WAZtN5MaqehS4J8n7AdK16yhdvwoclWRR0+/FdMvJnNKc7wZ8Azioqh4YvinJi5Js1BxvBewN3DaRWCVJkiRJkiRJs8eM3aleVb9Pck3zgtJfTGCII4GvJflrYEPgHOCWJAcBnar6XFXd35R2WZpkc2AhcFRVXdmMcRqwKX8sI/ObqjoIeD3wjSRDdP9i4tSqMqkuSZIkSZIkSXNcqizzPSzJscAngH2r6uHJHr/T6dTAwMD4HSVJkiRJkiRJfZNksKo6o7XN5PIv066qvlpVb5yKhLokSZIkSZIkafYzqS5JkiRJkiRJUksm1SVJkiRJkiRJasmkuiRJkiRJkiRJLZlUlyRJkiRJkiSpJZPqkiRJkiRJkiS1ZFJdkiRJkiRJkqSWTKpLkiRJkiRJktSSSXVJkiRJkiRJklpa0O8A5pXBQUgmf9yqyR9TkiRJkiRJkvQcc3KnepKDkpy0jvecnWTxVMUkSZIkSZIkSZr95uRO9aq6ELiw33FIkiRJkiRJkuaWWbdTPcnCJLcnOSvJrUm+m+SAJNck+WWSPZMcleSMpv/ZSU5Pcm2Su4d3o6frjCS3Jfkx8JKeOf4syc1JViT5VpKNmuu/TvJfklyXZCDJm5JckuSuJJ/oywciSZIkSZIkSZo2sy6p3ng18BVgF2BH4AhgH2AJ8JlR+r+saX8PcGpz7b3A64A3Ah8H9gJIsjFwNnBYVb2R7m7+T/aM9duqeitwddNvMfAW4IujBZrk6CYBP/DgxJ5VkiRJkiRJkjRDzNak+j1VtaKqhoCVwM+qqoAVwMJR+l9QVUNVdRvw0ubavsD3q2pNVd0HXNZcf10z/p3N+bebvsOGy8qsAH5eVauq6kHgqSRbjJy4qpZWVaeqOltP+HElSZIkSZIkSTPBbE2qr+45Huo5H2L0OvG9/dNzXKP0zSjXRhurd961zS1JkiRJkiRJmiNma1J9MlwFHJ5kgyQvA97eXL8dWJjk1c35h4Ar+xGgJEmSJEmSJGlmmc87q88H/pRuGZc7aRLnVfVUkr8Azk2yALgR+HrfopQkSZIkSZIkzRjpliLXdOh0OjUwMNDvMCRJkiRJkiRJa5FksKo6o7XN5/IvkiRJkiRJkiStE5PqkiRJkiRJkiS1ZFJdkiRJkiRJkqSWTKpLkiRJkiRJktSSSXVJkiRJkiRJkloyqS5JkiRJkiRJUksm1SVJkiRJkiRJasmkuiRJkiRJkiRJLZlUlyRJkiRJkiSppQX9DmBeGRyEZP3HqVr/MSRJkiRJkiRJ62xKdqon2SLJMc3xy5OcNxXzTKYkjzW/Z0W8kiRJkiRJkqTpN1XlX7YAjgGoqvuqavEUzTPpZlu8kiRJkiRJkqTpM1XlX04FdkiyDPgl8Pqq2jnJUcAhwAbAzsB/A54PfAhYDRxYVf+SZAfgq8DWwBPAx6vq9t4J1nesJNsD36P7GVzcM+5C4EdNvAuBvwM2aZqPq6prk+wPnAw81Mw9CHywyroskiRJkiRJkjSXTdVO9ZOAu6pqEfCpEW07A0cAewJfAp6oqt2A64APN32WAsdX1e7AEuDMMeZZn7G+AnytqvYA/nmM8R8A3lFVbwIOA07vadsNOAHYCXgVsPcYY0iSJEmSJEmS5oh+vKj08qpaBaxK8ghwUXN9BbBLkk2BvYBz88eXem40BWPtDRzaHP8d8OVRxt8QOCPJImAN8Nqethuq6l6AZkf+QuCfRg6Q5GjgaIBtx3gISZIkSZIkSdLs0I+k+uqe46Ge8yG68TwP+EOzy32qxxqvXMuJwO+AXZuxnhpj7jWM8VlW1VK6u+XpJJaHkSRJkiRJkqRZbKrKv6wCNpvIjVX1KHBPkvcDpGvXKRjrGuDw5vjIMYbYHLi/qobo1mrfYCJxSJIkSZIkSZLmhilJqlfV74FrktwKnDaBIY4EPpbkFmAlcDBAkoOSfHEyxgL+E3BskhvpJs9HcybwkSTX0y398vg6zi1JkiRJkiRJmkNSZUWS6dLpdGpgYKDfYUiSJEmSJEmS1iLJYFV1RmubqvIvkiRJkiRJkiTNOSbVJUmSJEmSJElqyaS6JEmSJEmSJEktmVSXJEmSJEmSJKklk+qSJEmSJEmSJLVkUl2SJEmSJEmSpJZMqkuSJEmSJEmS1JJJdUmSJEmSJEmSWjKpLkmSJEmSJElSSwv6HcC8MjgIybrfVzX5sUiSJEmSJEmS1pk71SVJkiRJkiRJamleJdWTHJTkpBb9TkuyMslp0xGXJEmSJEmSJGl2SFla5DmSPApsXVWrJ3PcTlIDE7nRf0aSJEmSJEmSNG2SDFZVZ7S2ObNTPcnCJLcnOSvJrUm+m+SAJNck+WWSPZMcleSMpv/ZSU5Pcm2Su5Msbq5fCGwC/DzJYU2/xT3zPNb83j/JFUnOa+b9bjKRgumSJEmSJEmSpNliziTVG68GvgLsAuwIHAHsAywBPjNK/5c17e8BTgWoqoOAJ6tqUVX9YJz5dgNOAHYCXgXsPbJDkqOTDCQZeHAiTyRJkiRJkiRJmjHmWlL9nqpaUVVDwErgZ9Wtb7MCWDhK/wuqaqiqbgNeOoH5bqiqe5v5lo02R1UtrapOVXW2nsAEkiRJkiRJkqSZY64l1XtroA/1nA8BC8bpP1bplmdoPqemvMvzx7h/zRhzSJIkSZIkSZLmiLmWVJ8KvwZ2b44PBjbsXyiSJEmSJEmSpH4yqT6+bwL7JbkBeDPweJ/jkSRJkiRJkiT1SbolxzUdOp1ODQwM9DsMSZIkSZIkSdJaJBmsqs5obe5UlyRJkiRJkiSpJZPqkiRJkiRJkiS1ZFJdkiRJkiRJkqSWTKpLkiRJkiRJktSSSXVJkiRJkiRJkloyqS5JkiRJkiRJUksm1SVJkiRJkiRJasmkuiRJkiRJkiRJLS3odwDzyuAgJO37V01dLJIkSZIkSZKkdeZOdUmSJEmSJEmSWpqxSfUkWyQ5pjl+eZLzpmiefZLckOT2JHckOban7b8nWdb83JnkDz1ta3raLpyK2CRJkiRJkiRJM8tMLv+yBXAMcGZV3QcsnuwJkmwDfA84pKpuSrIVcEmS+6rq/Ko6safv8cBuPbc/WVWLJjsmSZIkSZIkSdLMNWN3qgOnAjs0O8HPTXIrQJKjklyQ5KIk9yQ5LslfJbk5yfVJtmz67ZDk4iSDSa5OsuMocxwLnF1VNwFU1UPAp4FPjdL3A8D3p+RJJUmSJEmSJEmzwkxOqp8E3NXsBh+Z5N4ZOALYE/gS8ERV7QZcB3y46bMUOL6qdgeWAGeOMscbgMER1waAnXovJNkO2B64rOfyxkkGmkT+IWM9RJKjm34DD47VSZIkSZIkSZI0K8zk8i9rc3lVrQJWJXkEuKi5vgLYJcmmwF7AuUmG79lolHECVIv5DgfOq6o1Pde2rar7krwKuCzJiqq6a+SNVbWUboKfTtJmLkmSJEmSJEnSDDVbk+qre46Hes6H6D7T84A/tKh5vhLoAL0vGt2d7m71XofTLRXzb5o671TV3UmuoFtv/TlJdUmSJEmSJEnS3DGTy7+sAjabyI1V9ShwT5L3A6Rr11G6fhU4Ksmipt+L6ZaTOWW4Q5LXAS+iW1pm+NqLkmzUHG8F7A3cNpFYJUmSJEmSJEmzx4zdqV5Vv09yTfOC0l9MYIgjga8l+WtgQ+Ac4JYkBwGdqvpcVd2f5IPA0iSbAwuBo6rqyp5xPgCcU1W9pVteD3wjyRDdv5g4tapMqkuSJEmSJEnSHJdn54rntyTHAp8A9q2qhyd7/E6nUwMDIyvLSJIkSZIkSZJmkiSDVdUZrW0ml3+ZdlX11ap641Qk1CVJkiRJkiRJs59JdUmSJEmSJEmSWjKpLkmSJEmSJElSSybVJUmSJEmSJElqyaS6JEmSJEmSJEktmVSXJEmSJEmSJKklk+qSJEmSJEmSJLVkUl2SJEmSJEmSpJYW9DuAeWVwEJLnXq+a/lgkSZIkSZIkSevMneqSJEmSJEmSJLVkUr2RZP8kP+p3HJIkSZIkSZKkmcukuiRJkiRJkiRJLc2opHqShUluT3JWkluTfDfJAUmuSfLLJHsm2STJt5LcmOTmJAf33Ht1kpuan72a6/snuSLJec3Y3026hc2TvKu59k/A+3ri2DLJBUmWJ7k+yS7N9ZOTfDvJpUl+neR9Sf4myYokFyfZsA8fmyRJkiRJkiRpmsyopHrj1cBXgF2AHYEjgH2AJcBngM8Cl1XVHsDbgdOSbAI8ALyjqt4EHAac3jPmbsAJwE7Aq4C9k2wMfBP4c+BtwDY9/b8A3FxVuzRzfqenbQfg3wMHA38PXF5VbwSebK4/S5KjkwwkGXhwop+IJEmSJEmSJGlGWNDvAEZxT1WtAEiyEvhZVVWSFcBC4JXAQUmWNP03BrYF7gPOSLIIWAO8tmfMG6rq3mbMZc04jzVz/bK5/vfA0U3/fYBDAarqsiQvTrJ50/bTqnq6iWcD4OLm+nB8z1JVS4GlAJ2kJvaRSJIkSZIkSZJmgpmYVF/dczzUcz5EN941wKFVdUfvTUlOBn4H7Ep3B/5TY4y5hj8+91hJ7oxybbjvaoCqGkrydFUNXx+OT5IkSZIkSZI0R83E8i/juQQ4vqcu+m7N9c2B+6tqCPgQ3V3ka3M7sH2SHZrzD/S0XQUc2Yy/P/BQVT06KdFLkiRJkiRJkmat2ZhUPwXYEFie5NbmHOBM4CNJrqdb+uXxtQ1SVU/RLffy4+ZFpf+vp/lkoJNkOXAq8JFJfQJJkiRJkiRJ0qyUP1Yv0VTrdDo1MDDQ7zAkSZIkSZIkSWuRZLCqOqO1zcad6pIkSZIkSZIk9YVJdUmSJEmSJEmSWjKpLkmSJEmSJElSSybVJUmSJEmSJElqyReVTqMkq4A7+h2HNI9tBTzU7yCkecw1KPWXa1DqL9eg1F+uQam/ZuMa3K6qth6tYcF0RzLP3THWG2MlTb0kA65BqX9cg1J/uQal/nINSv3lGpT6a66tQcu/SJIkSZIkSZLUkkl1SZIkSZIkSZJaMqk+vZb2OwBpnnMNSv3lGpT6yzUo9ZdrUOov16DUX3NqDfqiUkmSJEmSJEmSWnKnuiRJkiRJkiRJLZlUlyRJkiRJkiSpJZPqUyDJu5LckeRXSU4apT1JTm/alyd5Uz/ilOaqFmtwxyTXJVmdZEk/YpTmshZr8Mjm+295kmuT7NqPOKW5qsUaPLhZf8uSDCTZpx9xSnPVeGuwp98eSdYkWTyd8UlzXYvvwf2TPNJ8Dy5L8rl+xCnNVW2+B5t1uCzJyiRXTneMk8Ga6pMsyQbAncA7gHuBG4EPVNVtPX0OBI4HDgTeDHylqt7ch3ClOaflGnwJsB1wCPBwVf1tH0KV5qSWa3Av4BdV9XCSdwMn+z0oTY6Wa3BT4PGqqiS7AP+7qnbsS8DSHNNmDfb0+0fgKeBbVXXedMcqzUUtvwf3B5ZU1Xv6EaM0l7Vcg1sA1wLvqqrfJHlJVT3Qj3jXhzvVJ9+ewK+q6u6q+lfgHODgEX0OBr5TXdcDWyR52XQHKs1R467Bqnqgqm4Enu5HgNIc12YNXltVDzen1wOvnOYYpbmszRp8rP64s2YTwF020uRp8+dB6G6y+gdg1iURpBmu7RqUNDXarMEjgB9W1W+gm6OZ5hgnhUn1yfcK4Lc95/c219a1j6SJcX1J/bWua/BjwE+nNCJpfmm1BpO8N8ntwI+Bj05TbNJ8MO4aTPIK4L3A16cxLmm+aPvfom9NckuSnyZ5w/SEJs0Lbdbga4EXJbkiyWCSD09bdJNoQb8DmIMyyrWRu3/a9JE0Ma4vqb9ar8Ekb6ebVLeeszR5Wq3BqjofOD/JvsApwAFTHZg0T7RZg/8D+M9VtSYZrbuk9dBmDd4EbFdVjzXleS8AXjPVgUnzRJs1uADYHfgz4AXAdUmur6o7pzq4yWRSffLdC/y7nvNXAvdNoI+kiXF9Sf3Vag02dZzPAt5dVb+fptik+WCdvger6qokOyTZqqoemvLopLmvzRrsAOc0CfWtgAOTPFNVF0xLhNLcNu4arKpHe45/kuRMvwelSdM2L/pQVT0OPJ7kKmBXurXYZw3Lv0y+G4HXJNk+yfOBw4ELR/S5EPhwut4CPFJV9093oNIc1WYNSpo6467BJNsCPwQ+NNt2I0izQJs1+Oo02bwkbwKeD/iXW9LkGHcNVtX2VbWwqhYC5wHHmFCXJk2b78Fter4H96SbG/N7UJocbXIy/wd4W5IFSV4IvBn4xTTHud7cqT7JquqZJMcBlwAb0H2T+8okn2javw78BDgQ+BXwBPAX/YpXmmvarMEk2wADwJ8AQ0lOAHbq3bEgaWJafg9+DngxcGbz55lnqqrTr5iluaTlGjyU7gaPp4EngcN6XlwqaT20XIOSpkjLNbgY+GSSZ+h+Dx7u96A0Odqswar6RZKLgeXAEHBWVd3av6gnJv57Q5IkSZIkSZKkdiz/IkmSJEmSJElSSybVJUmSJEmSJElqyaS6JEmSJEmSJEktmVSXJEmSJEmSJKklk+qSJEmSJEmSJLVkUl2SJEmaJkm2SXJOkruS3JbkJ0leOwXzLExya4s+R/Scd5KcPtmxrI8k30+yPMmJSXZMsizJzUl2SHLtOPd+MckBE5x3UZIDJxa1JEmS5rpUVb9jkCRJkua8JAGuBb5dVV9vri0CNquqq1vcv0FVrRnrfETfhcCPqmrntYy3P7Ckqt6zDo8xbZJsA/y8qrZrzk8CXlBVn5+GuY8COlV13FTPJUmSpNnHneqSJEnS9Hg78PRwQh2gqpZV1dXpOi3JrUlWJDkMuonvJJcn+R6wYpTzDZr7bmx2dP/HkZM2O9KvTnJT87NX03Qq8LZm9/eJzdg/au7ZMskFzZjXJ9mluX5ykm8luSLJ3Un+crQHTfKuZq5bkvxsnDE3aca8sdmFfnAzzKXAS5r4Pg+cAPyHJJc39z3WM9+nm8/tliSnNtfOTrK4Od49yZVJBpNckuRlzfUrknw5yQ1J7kzytiTPB74IHNbMfdi6/oOWJEnS3Lag3wFIkiRJ88TOwOAYbe8DFgG7AlsBNya5qmnbE9i5qu5pdpf3nh8NPFJVeyTZCLgmyaVA7/+O+gDwjqp6KslrgO8DHeAkenaqN2MP+wJwc1UdkuRPge808QHsSPcvCDYD7kjytap6evjGJFsD3wT2bWLccpwxPwtcVlUfTbIFcEOS/wscRHe3/aJm3ACPVdXf9n5wSd4NHAK8uaqe6JlvuH1D4H8CB1fVg02S/EvAR5suC6pqz6bcy+er6oAkn8Od6pIkSRqDSXVJkiSp//YBvt+Uc/ldkiuBPYBHgRuq6p6evr3n7wR2Gd6RDWwOvAa4s6f/hsAZTamZNUCbGu77AIcCVNVlSV6cZPOm7cdVtRpYneQB4KXAvT33vgW4ajjGqvqXccZ8J3BQkiVNv42BbYEnW8QJcADwv6rqiRHzDXsd3b/Q+MduXp4NgPt72n/Y/B4EFracU5IkSfOYSXVJkiRpeqwEFo/RlrXc9/hazgMcX1WXPGuwbk31YScCv6O7C/55wFMtYh0tnuHd76t7rq3huX+mCM/eKT/emAEOrao7ntX52c+wNmPN19u+sqreOkb78POM9iySJEnSc1hTXZIkSZoelwEbJfn48IUkeyTZD7iKbg3vDZryKfsCN7QY8xLgk02JE5K8NskmI/psDtxfVUPAh+ju1AZYRbeEy2iuAo5sxtwfeKiqHm0RD8B1wH5Jtm/uHy7HMtaYlwDHN+VdSLJby3mGXQp8NMkLR8w37A5g6yRvbdo3TPKGccZc22cjSZKkec6kuiRJkjQNqqqA9wLvSHJXkpXAycB9wPnAcuAWusn3T1fVP7cY9izgNuCmJLcC3+C5u63PBD6S5Hq6pV+Gd7ovB55pXu554oh7TgY6SZbTfaHpR9bhOR8EjgZ+mOQW4AfjjHkK3RI1y5tnOKXtXM18FwMXAgNJlgFLRrT/K93/Q+DLTTzLgL1Yu8uBnXxRqSRJkkaT7n/bS5IkSZIkSZKk8bhTXZIkSZIkSZKklkyqS5IkSZIkSZLUkkl1SZIkSZIkSZJaMqkuSZIkSZIkSVJLJtUlSZIkSZIkSWrJpLokSZIkSZIkSS2ZVJckSZIkSZIkqaX/DyBW8FR1kHWxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1800x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr_data = dataframe.corr()\n",
    "selected_pixel_features = corr_data['label'].apply(lambda x: abs(x)).sort_values(ascending=False).iloc[1:-2][::-1]\n",
    "plt.figure(figsize=(25,12))\n",
    "selected_pixel_features.plot(kind='barh',color='red')\n",
    "# calculating highest correlated faetures\n",
    "# with respect to target variable i.e. \"convert\"\n",
    "plt.title(\"Top highly correlated features\", size=20, pad=26)\n",
    "plt.xlabel(\"Correlation coefficient\")\n",
    "plt.ylabel(\"Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79aa2624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7624, 36)\n",
      "(7624,)\n"
     ]
    }
   ],
   "source": [
    "x = dataframe.drop('label', axis = 1)\n",
    "#x = dataframe[['HNRVoiced','HNR']]\n",
    "y = dataframe['label']\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c93664c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6099, 36)\n",
      "(1525, 36)\n",
      "(6099,)\n",
      "(1525,)\n"
     ]
    }
   ],
   "source": [
    "#Splitting Train & Testing Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 0)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dc12b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VarianceThreshold(threshold=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "constant_filter = VarianceThreshold(threshold=0)\n",
    "constant_filter.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b058200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding not constant feature\n",
    "len(x_train.columns[constant_filter.get_support()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fac0404c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>freq.median</th>\n",
       "      <th>freq.Q25</th>\n",
       "      <th>freq.Q75</th>\n",
       "      <th>freq.IQR</th>\n",
       "      <th>time.median</th>\n",
       "      <th>time.Q25</th>\n",
       "      <th>time.Q75</th>\n",
       "      <th>time.IQR</th>\n",
       "      <th>...</th>\n",
       "      <th>HNRVoiced</th>\n",
       "      <th>h1_freq</th>\n",
       "      <th>h1_width</th>\n",
       "      <th>h2_freq</th>\n",
       "      <th>h2_width</th>\n",
       "      <th>h3_freq</th>\n",
       "      <th>h3_width</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>meanfreq</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.795590</td>\n",
       "      <td>0.870539</td>\n",
       "      <td>0.603795</td>\n",
       "      <td>0.952023</td>\n",
       "      <td>0.882332</td>\n",
       "      <td>-0.048984</td>\n",
       "      <td>-0.052761</td>\n",
       "      <td>-0.050406</td>\n",
       "      <td>-0.043259</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074716</td>\n",
       "      <td>0.424484</td>\n",
       "      <td>0.228450</td>\n",
       "      <td>0.392023</td>\n",
       "      <td>-0.017194</td>\n",
       "      <td>0.287206</td>\n",
       "      <td>-0.017504</td>\n",
       "      <td>0.220436</td>\n",
       "      <td>0.166984</td>\n",
       "      <td>0.245190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sd</th>\n",
       "      <td>0.795590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.450117</td>\n",
       "      <td>0.123068</td>\n",
       "      <td>0.830397</td>\n",
       "      <td>0.865721</td>\n",
       "      <td>-0.002590</td>\n",
       "      <td>-0.023713</td>\n",
       "      <td>0.009214</td>\n",
       "      <td>0.026402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056090</td>\n",
       "      <td>0.280239</td>\n",
       "      <td>0.265789</td>\n",
       "      <td>0.388382</td>\n",
       "      <td>0.129362</td>\n",
       "      <td>0.409930</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>0.120606</td>\n",
       "      <td>0.068648</td>\n",
       "      <td>0.214819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq.median</th>\n",
       "      <td>0.870539</td>\n",
       "      <td>0.450117</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.720723</td>\n",
       "      <td>0.748930</td>\n",
       "      <td>0.635587</td>\n",
       "      <td>-0.065558</td>\n",
       "      <td>-0.059288</td>\n",
       "      <td>-0.075742</td>\n",
       "      <td>-0.076086</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115452</td>\n",
       "      <td>0.449191</td>\n",
       "      <td>0.216466</td>\n",
       "      <td>0.344145</td>\n",
       "      <td>-0.062363</td>\n",
       "      <td>0.187863</td>\n",
       "      <td>-0.051123</td>\n",
       "      <td>0.245461</td>\n",
       "      <td>0.194146</td>\n",
       "      <td>0.219835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq.Q25</th>\n",
       "      <td>0.603795</td>\n",
       "      <td>0.123068</td>\n",
       "      <td>0.720723</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.425503</td>\n",
       "      <td>0.220479</td>\n",
       "      <td>-0.048509</td>\n",
       "      <td>-0.035872</td>\n",
       "      <td>-0.061101</td>\n",
       "      <td>-0.068007</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145287</td>\n",
       "      <td>0.326888</td>\n",
       "      <td>0.004712</td>\n",
       "      <td>0.122033</td>\n",
       "      <td>-0.153735</td>\n",
       "      <td>-0.016222</td>\n",
       "      <td>-0.102060</td>\n",
       "      <td>0.202100</td>\n",
       "      <td>0.204851</td>\n",
       "      <td>0.090377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq.Q75</th>\n",
       "      <td>0.952023</td>\n",
       "      <td>0.830397</td>\n",
       "      <td>0.748930</td>\n",
       "      <td>0.425503</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976502</td>\n",
       "      <td>-0.049437</td>\n",
       "      <td>-0.054377</td>\n",
       "      <td>-0.049123</td>\n",
       "      <td>-0.040516</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067475</td>\n",
       "      <td>0.383044</td>\n",
       "      <td>0.237334</td>\n",
       "      <td>0.393738</td>\n",
       "      <td>0.009274</td>\n",
       "      <td>0.305765</td>\n",
       "      <td>-0.004775</td>\n",
       "      <td>0.198423</td>\n",
       "      <td>0.136846</td>\n",
       "      <td>0.261679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq.IQR</th>\n",
       "      <td>0.882332</td>\n",
       "      <td>0.865721</td>\n",
       "      <td>0.635587</td>\n",
       "      <td>0.220479</td>\n",
       "      <td>0.976502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.041733</td>\n",
       "      <td>-0.050066</td>\n",
       "      <td>-0.038395</td>\n",
       "      <td>-0.027475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038128</td>\n",
       "      <td>0.335012</td>\n",
       "      <td>0.254684</td>\n",
       "      <td>0.395322</td>\n",
       "      <td>0.046606</td>\n",
       "      <td>0.333426</td>\n",
       "      <td>0.019158</td>\n",
       "      <td>0.165738</td>\n",
       "      <td>0.098713</td>\n",
       "      <td>0.260523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time.median</th>\n",
       "      <td>-0.048984</td>\n",
       "      <td>-0.002590</td>\n",
       "      <td>-0.065558</td>\n",
       "      <td>-0.048509</td>\n",
       "      <td>-0.049437</td>\n",
       "      <td>-0.041733</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909278</td>\n",
       "      <td>0.966742</td>\n",
       "      <td>0.886561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064073</td>\n",
       "      <td>-0.033618</td>\n",
       "      <td>-0.016960</td>\n",
       "      <td>-0.012271</td>\n",
       "      <td>0.107722</td>\n",
       "      <td>0.044405</td>\n",
       "      <td>0.081702</td>\n",
       "      <td>-0.031837</td>\n",
       "      <td>-0.047124</td>\n",
       "      <td>0.031422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time.Q25</th>\n",
       "      <td>-0.052761</td>\n",
       "      <td>-0.023713</td>\n",
       "      <td>-0.059288</td>\n",
       "      <td>-0.035872</td>\n",
       "      <td>-0.054377</td>\n",
       "      <td>-0.050066</td>\n",
       "      <td>0.909278</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.863110</td>\n",
       "      <td>0.687182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062497</td>\n",
       "      <td>-0.036356</td>\n",
       "      <td>-0.018942</td>\n",
       "      <td>-0.011750</td>\n",
       "      <td>0.097033</td>\n",
       "      <td>0.030945</td>\n",
       "      <td>0.064165</td>\n",
       "      <td>-0.033286</td>\n",
       "      <td>-0.040538</td>\n",
       "      <td>0.021050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time.Q75</th>\n",
       "      <td>-0.050406</td>\n",
       "      <td>0.009214</td>\n",
       "      <td>-0.075742</td>\n",
       "      <td>-0.061101</td>\n",
       "      <td>-0.049123</td>\n",
       "      <td>-0.038395</td>\n",
       "      <td>0.966742</td>\n",
       "      <td>0.863110</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065248</td>\n",
       "      <td>-0.030774</td>\n",
       "      <td>-0.019669</td>\n",
       "      <td>-0.014312</td>\n",
       "      <td>0.107892</td>\n",
       "      <td>0.047378</td>\n",
       "      <td>0.085383</td>\n",
       "      <td>-0.033258</td>\n",
       "      <td>-0.049494</td>\n",
       "      <td>0.029995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time.IQR</th>\n",
       "      <td>-0.043259</td>\n",
       "      <td>0.026402</td>\n",
       "      <td>-0.076086</td>\n",
       "      <td>-0.068007</td>\n",
       "      <td>-0.040516</td>\n",
       "      <td>-0.027475</td>\n",
       "      <td>0.886561</td>\n",
       "      <td>0.687182</td>\n",
       "      <td>0.960001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059211</td>\n",
       "      <td>-0.024112</td>\n",
       "      <td>-0.017793</td>\n",
       "      <td>-0.014074</td>\n",
       "      <td>0.101408</td>\n",
       "      <td>0.050998</td>\n",
       "      <td>0.087251</td>\n",
       "      <td>-0.029388</td>\n",
       "      <td>-0.048723</td>\n",
       "      <td>0.031477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skew</th>\n",
       "      <td>-0.013687</td>\n",
       "      <td>0.083118</td>\n",
       "      <td>-0.039652</td>\n",
       "      <td>-0.133530</td>\n",
       "      <td>0.018379</td>\n",
       "      <td>0.051609</td>\n",
       "      <td>0.121077</td>\n",
       "      <td>0.099814</td>\n",
       "      <td>0.130949</td>\n",
       "      <td>0.133035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009044</td>\n",
       "      <td>0.135980</td>\n",
       "      <td>0.184477</td>\n",
       "      <td>0.171535</td>\n",
       "      <td>0.152450</td>\n",
       "      <td>0.178778</td>\n",
       "      <td>0.145871</td>\n",
       "      <td>0.084918</td>\n",
       "      <td>0.055314</td>\n",
       "      <td>0.093586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kurt</th>\n",
       "      <td>-0.007608</td>\n",
       "      <td>0.062675</td>\n",
       "      <td>-0.021509</td>\n",
       "      <td>-0.098251</td>\n",
       "      <td>0.016051</td>\n",
       "      <td>0.040699</td>\n",
       "      <td>0.176043</td>\n",
       "      <td>0.149828</td>\n",
       "      <td>0.185510</td>\n",
       "      <td>0.183794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006683</td>\n",
       "      <td>0.110511</td>\n",
       "      <td>0.138493</td>\n",
       "      <td>0.134121</td>\n",
       "      <td>0.107871</td>\n",
       "      <td>0.139481</td>\n",
       "      <td>0.110717</td>\n",
       "      <td>0.056589</td>\n",
       "      <td>0.033403</td>\n",
       "      <td>0.072811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp.ent</th>\n",
       "      <td>0.751073</td>\n",
       "      <td>0.557622</td>\n",
       "      <td>0.640518</td>\n",
       "      <td>0.491447</td>\n",
       "      <td>0.701176</td>\n",
       "      <td>0.638716</td>\n",
       "      <td>0.019966</td>\n",
       "      <td>0.011787</td>\n",
       "      <td>0.024987</td>\n",
       "      <td>0.029410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155043</td>\n",
       "      <td>0.203277</td>\n",
       "      <td>0.034005</td>\n",
       "      <td>0.131759</td>\n",
       "      <td>-0.206377</td>\n",
       "      <td>0.035375</td>\n",
       "      <td>-0.116142</td>\n",
       "      <td>0.106739</td>\n",
       "      <td>0.075615</td>\n",
       "      <td>0.116357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time.ent</th>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.059746</td>\n",
       "      <td>-0.029612</td>\n",
       "      <td>-0.018898</td>\n",
       "      <td>-0.007997</td>\n",
       "      <td>-0.004119</td>\n",
       "      <td>0.815033</td>\n",
       "      <td>0.721833</td>\n",
       "      <td>0.851911</td>\n",
       "      <td>0.825298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100071</td>\n",
       "      <td>-0.049107</td>\n",
       "      <td>-0.054674</td>\n",
       "      <td>-0.044909</td>\n",
       "      <td>0.044423</td>\n",
       "      <td>0.010155</td>\n",
       "      <td>0.023341</td>\n",
       "      <td>-0.059670</td>\n",
       "      <td>-0.066791</td>\n",
       "      <td>-0.002039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entropy</th>\n",
       "      <td>0.727108</td>\n",
       "      <td>0.549607</td>\n",
       "      <td>0.614122</td>\n",
       "      <td>0.471655</td>\n",
       "      <td>0.676730</td>\n",
       "      <td>0.617081</td>\n",
       "      <td>0.158355</td>\n",
       "      <td>0.134535</td>\n",
       "      <td>0.169493</td>\n",
       "      <td>0.169231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132786</td>\n",
       "      <td>0.188563</td>\n",
       "      <td>0.023839</td>\n",
       "      <td>0.120177</td>\n",
       "      <td>-0.191789</td>\n",
       "      <td>0.036447</td>\n",
       "      <td>-0.108136</td>\n",
       "      <td>0.093029</td>\n",
       "      <td>0.061686</td>\n",
       "      <td>0.112211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sfm</th>\n",
       "      <td>0.711753</td>\n",
       "      <td>0.638536</td>\n",
       "      <td>0.554883</td>\n",
       "      <td>0.370979</td>\n",
       "      <td>0.668689</td>\n",
       "      <td>0.632388</td>\n",
       "      <td>-0.056685</td>\n",
       "      <td>-0.061374</td>\n",
       "      <td>-0.052625</td>\n",
       "      <td>-0.041675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039685</td>\n",
       "      <td>0.257886</td>\n",
       "      <td>0.144412</td>\n",
       "      <td>0.250026</td>\n",
       "      <td>-0.061608</td>\n",
       "      <td>0.204779</td>\n",
       "      <td>-0.029545</td>\n",
       "      <td>0.097689</td>\n",
       "      <td>0.060142</td>\n",
       "      <td>0.129186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meandom</th>\n",
       "      <td>0.607234</td>\n",
       "      <td>0.309372</td>\n",
       "      <td>0.600181</td>\n",
       "      <td>0.632206</td>\n",
       "      <td>0.515540</td>\n",
       "      <td>0.405111</td>\n",
       "      <td>-0.060256</td>\n",
       "      <td>-0.044482</td>\n",
       "      <td>-0.069352</td>\n",
       "      <td>-0.075103</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115006</td>\n",
       "      <td>0.167407</td>\n",
       "      <td>-0.067714</td>\n",
       "      <td>0.068728</td>\n",
       "      <td>-0.156469</td>\n",
       "      <td>-0.034698</td>\n",
       "      <td>-0.147827</td>\n",
       "      <td>0.080314</td>\n",
       "      <td>0.096392</td>\n",
       "      <td>0.046154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mindom</th>\n",
       "      <td>-0.000163</td>\n",
       "      <td>-0.042509</td>\n",
       "      <td>0.021479</td>\n",
       "      <td>0.033141</td>\n",
       "      <td>-0.003762</td>\n",
       "      <td>-0.011947</td>\n",
       "      <td>-0.060566</td>\n",
       "      <td>-0.053839</td>\n",
       "      <td>-0.058794</td>\n",
       "      <td>-0.054727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007365</td>\n",
       "      <td>0.011951</td>\n",
       "      <td>-0.018563</td>\n",
       "      <td>-0.011234</td>\n",
       "      <td>0.041608</td>\n",
       "      <td>-0.048981</td>\n",
       "      <td>-0.004748</td>\n",
       "      <td>0.014688</td>\n",
       "      <td>0.018724</td>\n",
       "      <td>0.001852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maxdom</th>\n",
       "      <td>0.491478</td>\n",
       "      <td>0.478754</td>\n",
       "      <td>0.336458</td>\n",
       "      <td>0.253957</td>\n",
       "      <td>0.460502</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>0.074607</td>\n",
       "      <td>0.060053</td>\n",
       "      <td>0.072063</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000915</td>\n",
       "      <td>0.026850</td>\n",
       "      <td>-0.011226</td>\n",
       "      <td>0.078585</td>\n",
       "      <td>-0.074630</td>\n",
       "      <td>0.053762</td>\n",
       "      <td>-0.087187</td>\n",
       "      <td>-0.010599</td>\n",
       "      <td>-0.006543</td>\n",
       "      <td>0.030929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dfrange</th>\n",
       "      <td>0.491407</td>\n",
       "      <td>0.478998</td>\n",
       "      <td>0.336249</td>\n",
       "      <td>0.253674</td>\n",
       "      <td>0.460462</td>\n",
       "      <td>0.435890</td>\n",
       "      <td>0.075046</td>\n",
       "      <td>0.060444</td>\n",
       "      <td>0.072489</td>\n",
       "      <td>0.070765</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000860</td>\n",
       "      <td>0.026758</td>\n",
       "      <td>-0.011087</td>\n",
       "      <td>0.078657</td>\n",
       "      <td>-0.074928</td>\n",
       "      <td>0.054117</td>\n",
       "      <td>-0.087139</td>\n",
       "      <td>-0.010706</td>\n",
       "      <td>-0.006681</td>\n",
       "      <td>0.030911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modindx</th>\n",
       "      <td>0.142226</td>\n",
       "      <td>0.018217</td>\n",
       "      <td>0.183571</td>\n",
       "      <td>0.178362</td>\n",
       "      <td>0.120883</td>\n",
       "      <td>0.087817</td>\n",
       "      <td>0.359633</td>\n",
       "      <td>0.301041</td>\n",
       "      <td>0.380467</td>\n",
       "      <td>0.380408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045355</td>\n",
       "      <td>0.052035</td>\n",
       "      <td>-0.000546</td>\n",
       "      <td>-0.006268</td>\n",
       "      <td>0.010561</td>\n",
       "      <td>-0.017612</td>\n",
       "      <td>0.003043</td>\n",
       "      <td>0.027612</td>\n",
       "      <td>0.006547</td>\n",
       "      <td>0.049670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startdom</th>\n",
       "      <td>0.173138</td>\n",
       "      <td>0.080923</td>\n",
       "      <td>0.167023</td>\n",
       "      <td>0.182786</td>\n",
       "      <td>0.145317</td>\n",
       "      <td>0.113098</td>\n",
       "      <td>-0.046748</td>\n",
       "      <td>-0.047837</td>\n",
       "      <td>-0.049261</td>\n",
       "      <td>-0.044342</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053915</td>\n",
       "      <td>0.049976</td>\n",
       "      <td>-0.022826</td>\n",
       "      <td>0.029751</td>\n",
       "      <td>-0.054640</td>\n",
       "      <td>-0.021503</td>\n",
       "      <td>-0.054098</td>\n",
       "      <td>0.018647</td>\n",
       "      <td>0.038858</td>\n",
       "      <td>0.001237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enddom</th>\n",
       "      <td>0.222831</td>\n",
       "      <td>0.137847</td>\n",
       "      <td>0.207769</td>\n",
       "      <td>0.200875</td>\n",
       "      <td>0.190208</td>\n",
       "      <td>0.157175</td>\n",
       "      <td>0.035176</td>\n",
       "      <td>0.033147</td>\n",
       "      <td>0.034643</td>\n",
       "      <td>0.031458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023967</td>\n",
       "      <td>0.023863</td>\n",
       "      <td>-0.041264</td>\n",
       "      <td>0.029424</td>\n",
       "      <td>-0.030214</td>\n",
       "      <td>0.007857</td>\n",
       "      <td>-0.069693</td>\n",
       "      <td>0.012429</td>\n",
       "      <td>0.011340</td>\n",
       "      <td>0.019421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dfslope</th>\n",
       "      <td>0.069683</td>\n",
       "      <td>0.060405</td>\n",
       "      <td>0.061281</td>\n",
       "      <td>0.045171</td>\n",
       "      <td>0.061114</td>\n",
       "      <td>0.055114</td>\n",
       "      <td>0.059943</td>\n",
       "      <td>0.058986</td>\n",
       "      <td>0.061163</td>\n",
       "      <td>0.055282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015822</td>\n",
       "      <td>-0.013305</td>\n",
       "      <td>-0.019006</td>\n",
       "      <td>0.004650</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>0.020697</td>\n",
       "      <td>-0.021829</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.016306</td>\n",
       "      <td>0.015226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meanpeakf</th>\n",
       "      <td>0.157395</td>\n",
       "      <td>-0.076539</td>\n",
       "      <td>0.213082</td>\n",
       "      <td>0.451817</td>\n",
       "      <td>0.063382</td>\n",
       "      <td>-0.039282</td>\n",
       "      <td>-0.055589</td>\n",
       "      <td>-0.035854</td>\n",
       "      <td>-0.056765</td>\n",
       "      <td>-0.061781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052088</td>\n",
       "      <td>0.070148</td>\n",
       "      <td>-0.177781</td>\n",
       "      <td>-0.049197</td>\n",
       "      <td>-0.112825</td>\n",
       "      <td>-0.107520</td>\n",
       "      <td>-0.109978</td>\n",
       "      <td>0.064899</td>\n",
       "      <td>0.101624</td>\n",
       "      <td>-0.026067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HNR</th>\n",
       "      <td>-0.215435</td>\n",
       "      <td>0.020212</td>\n",
       "      <td>-0.286344</td>\n",
       "      <td>-0.343692</td>\n",
       "      <td>-0.164167</td>\n",
       "      <td>-0.095097</td>\n",
       "      <td>0.057705</td>\n",
       "      <td>0.052876</td>\n",
       "      <td>0.064460</td>\n",
       "      <td>0.063412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809671</td>\n",
       "      <td>0.105990</td>\n",
       "      <td>0.270024</td>\n",
       "      <td>0.280788</td>\n",
       "      <td>0.466794</td>\n",
       "      <td>0.371887</td>\n",
       "      <td>0.205238</td>\n",
       "      <td>0.045604</td>\n",
       "      <td>0.012477</td>\n",
       "      <td>0.077993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HNRVoiced</th>\n",
       "      <td>-0.074716</td>\n",
       "      <td>0.056090</td>\n",
       "      <td>-0.115452</td>\n",
       "      <td>-0.145287</td>\n",
       "      <td>-0.067475</td>\n",
       "      <td>-0.038128</td>\n",
       "      <td>0.064073</td>\n",
       "      <td>0.062497</td>\n",
       "      <td>0.065248</td>\n",
       "      <td>0.059211</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.181581</td>\n",
       "      <td>0.204008</td>\n",
       "      <td>0.428080</td>\n",
       "      <td>0.282699</td>\n",
       "      <td>0.155827</td>\n",
       "      <td>0.060323</td>\n",
       "      <td>0.071588</td>\n",
       "      <td>0.045593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h1_freq</th>\n",
       "      <td>0.424484</td>\n",
       "      <td>0.280239</td>\n",
       "      <td>0.449191</td>\n",
       "      <td>0.326888</td>\n",
       "      <td>0.383044</td>\n",
       "      <td>0.335012</td>\n",
       "      <td>-0.033618</td>\n",
       "      <td>-0.036356</td>\n",
       "      <td>-0.030774</td>\n",
       "      <td>-0.024112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.626050</td>\n",
       "      <td>0.698041</td>\n",
       "      <td>0.266442</td>\n",
       "      <td>0.616340</td>\n",
       "      <td>0.202439</td>\n",
       "      <td>0.482779</td>\n",
       "      <td>0.368188</td>\n",
       "      <td>0.415536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h1_width</th>\n",
       "      <td>0.228450</td>\n",
       "      <td>0.265789</td>\n",
       "      <td>0.216466</td>\n",
       "      <td>0.004712</td>\n",
       "      <td>0.237334</td>\n",
       "      <td>0.254684</td>\n",
       "      <td>-0.016960</td>\n",
       "      <td>-0.018942</td>\n",
       "      <td>-0.019669</td>\n",
       "      <td>-0.017793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181581</td>\n",
       "      <td>0.626050</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.585898</td>\n",
       "      <td>0.450659</td>\n",
       "      <td>0.599937</td>\n",
       "      <td>0.335051</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>0.245440</td>\n",
       "      <td>0.330699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h2_freq</th>\n",
       "      <td>0.392023</td>\n",
       "      <td>0.388382</td>\n",
       "      <td>0.344145</td>\n",
       "      <td>0.122033</td>\n",
       "      <td>0.393738</td>\n",
       "      <td>0.395322</td>\n",
       "      <td>-0.012271</td>\n",
       "      <td>-0.011750</td>\n",
       "      <td>-0.014312</td>\n",
       "      <td>-0.014074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204008</td>\n",
       "      <td>0.698041</td>\n",
       "      <td>0.585898</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.467980</td>\n",
       "      <td>0.810984</td>\n",
       "      <td>0.327377</td>\n",
       "      <td>0.395760</td>\n",
       "      <td>0.292860</td>\n",
       "      <td>0.381168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h2_width</th>\n",
       "      <td>-0.017194</td>\n",
       "      <td>0.129362</td>\n",
       "      <td>-0.062363</td>\n",
       "      <td>-0.153735</td>\n",
       "      <td>0.009274</td>\n",
       "      <td>0.046606</td>\n",
       "      <td>0.107722</td>\n",
       "      <td>0.097033</td>\n",
       "      <td>0.107892</td>\n",
       "      <td>0.101408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428080</td>\n",
       "      <td>0.266442</td>\n",
       "      <td>0.450659</td>\n",
       "      <td>0.467980</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.525319</td>\n",
       "      <td>0.514055</td>\n",
       "      <td>0.113884</td>\n",
       "      <td>0.082086</td>\n",
       "      <td>0.141332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h3_freq</th>\n",
       "      <td>0.287206</td>\n",
       "      <td>0.409930</td>\n",
       "      <td>0.187863</td>\n",
       "      <td>-0.016222</td>\n",
       "      <td>0.305765</td>\n",
       "      <td>0.333426</td>\n",
       "      <td>0.044405</td>\n",
       "      <td>0.030945</td>\n",
       "      <td>0.047378</td>\n",
       "      <td>0.050998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282699</td>\n",
       "      <td>0.616340</td>\n",
       "      <td>0.599937</td>\n",
       "      <td>0.810984</td>\n",
       "      <td>0.525319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.453399</td>\n",
       "      <td>0.296338</td>\n",
       "      <td>0.212166</td>\n",
       "      <td>0.318235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h3_width</th>\n",
       "      <td>-0.017504</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>-0.051123</td>\n",
       "      <td>-0.102060</td>\n",
       "      <td>-0.004775</td>\n",
       "      <td>0.019158</td>\n",
       "      <td>0.081702</td>\n",
       "      <td>0.064165</td>\n",
       "      <td>0.085383</td>\n",
       "      <td>0.087251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155827</td>\n",
       "      <td>0.202439</td>\n",
       "      <td>0.335051</td>\n",
       "      <td>0.327377</td>\n",
       "      <td>0.514055</td>\n",
       "      <td>0.453399</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071836</td>\n",
       "      <td>0.035671</td>\n",
       "      <td>0.102083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meanfun</th>\n",
       "      <td>0.220436</td>\n",
       "      <td>0.120606</td>\n",
       "      <td>0.245461</td>\n",
       "      <td>0.202100</td>\n",
       "      <td>0.198423</td>\n",
       "      <td>0.165738</td>\n",
       "      <td>-0.031837</td>\n",
       "      <td>-0.033286</td>\n",
       "      <td>-0.033258</td>\n",
       "      <td>-0.029388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060323</td>\n",
       "      <td>0.482779</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>0.395760</td>\n",
       "      <td>0.113884</td>\n",
       "      <td>0.296338</td>\n",
       "      <td>0.071836</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.841752</td>\n",
       "      <td>0.664872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minfun</th>\n",
       "      <td>0.166984</td>\n",
       "      <td>0.068648</td>\n",
       "      <td>0.194146</td>\n",
       "      <td>0.204851</td>\n",
       "      <td>0.136846</td>\n",
       "      <td>0.098713</td>\n",
       "      <td>-0.047124</td>\n",
       "      <td>-0.040538</td>\n",
       "      <td>-0.049494</td>\n",
       "      <td>-0.048723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071588</td>\n",
       "      <td>0.368188</td>\n",
       "      <td>0.245440</td>\n",
       "      <td>0.292860</td>\n",
       "      <td>0.082086</td>\n",
       "      <td>0.212166</td>\n",
       "      <td>0.035671</td>\n",
       "      <td>0.841752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.366812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maxfun</th>\n",
       "      <td>0.245190</td>\n",
       "      <td>0.214819</td>\n",
       "      <td>0.219835</td>\n",
       "      <td>0.090377</td>\n",
       "      <td>0.261679</td>\n",
       "      <td>0.260523</td>\n",
       "      <td>0.031422</td>\n",
       "      <td>0.021050</td>\n",
       "      <td>0.029995</td>\n",
       "      <td>0.031477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045593</td>\n",
       "      <td>0.415536</td>\n",
       "      <td>0.330699</td>\n",
       "      <td>0.381168</td>\n",
       "      <td>0.141332</td>\n",
       "      <td>0.318235</td>\n",
       "      <td>0.102083</td>\n",
       "      <td>0.664872</td>\n",
       "      <td>0.366812</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             meanfreq        sd  freq.median  freq.Q25  freq.Q75  freq.IQR  \\\n",
       "meanfreq     1.000000  0.795590     0.870539  0.603795  0.952023  0.882332   \n",
       "sd           0.795590  1.000000     0.450117  0.123068  0.830397  0.865721   \n",
       "freq.median  0.870539  0.450117     1.000000  0.720723  0.748930  0.635587   \n",
       "freq.Q25     0.603795  0.123068     0.720723  1.000000  0.425503  0.220479   \n",
       "freq.Q75     0.952023  0.830397     0.748930  0.425503  1.000000  0.976502   \n",
       "freq.IQR     0.882332  0.865721     0.635587  0.220479  0.976502  1.000000   \n",
       "time.median -0.048984 -0.002590    -0.065558 -0.048509 -0.049437 -0.041733   \n",
       "time.Q25    -0.052761 -0.023713    -0.059288 -0.035872 -0.054377 -0.050066   \n",
       "time.Q75    -0.050406  0.009214    -0.075742 -0.061101 -0.049123 -0.038395   \n",
       "time.IQR    -0.043259  0.026402    -0.076086 -0.068007 -0.040516 -0.027475   \n",
       "skew        -0.013687  0.083118    -0.039652 -0.133530  0.018379  0.051609   \n",
       "kurt        -0.007608  0.062675    -0.021509 -0.098251  0.016051  0.040699   \n",
       "sp.ent       0.751073  0.557622     0.640518  0.491447  0.701176  0.638716   \n",
       "time.ent     0.005055  0.059746    -0.029612 -0.018898 -0.007997 -0.004119   \n",
       "entropy      0.727108  0.549607     0.614122  0.471655  0.676730  0.617081   \n",
       "sfm          0.711753  0.638536     0.554883  0.370979  0.668689  0.632388   \n",
       "meandom      0.607234  0.309372     0.600181  0.632206  0.515540  0.405111   \n",
       "mindom      -0.000163 -0.042509     0.021479  0.033141 -0.003762 -0.011947   \n",
       "maxdom       0.491478  0.478754     0.336458  0.253957  0.460502  0.435866   \n",
       "dfrange      0.491407  0.478998     0.336249  0.253674  0.460462  0.435890   \n",
       "modindx      0.142226  0.018217     0.183571  0.178362  0.120883  0.087817   \n",
       "startdom     0.173138  0.080923     0.167023  0.182786  0.145317  0.113098   \n",
       "enddom       0.222831  0.137847     0.207769  0.200875  0.190208  0.157175   \n",
       "dfslope      0.069683  0.060405     0.061281  0.045171  0.061114  0.055114   \n",
       "meanpeakf    0.157395 -0.076539     0.213082  0.451817  0.063382 -0.039282   \n",
       "HNR         -0.215435  0.020212    -0.286344 -0.343692 -0.164167 -0.095097   \n",
       "HNRVoiced   -0.074716  0.056090    -0.115452 -0.145287 -0.067475 -0.038128   \n",
       "h1_freq      0.424484  0.280239     0.449191  0.326888  0.383044  0.335012   \n",
       "h1_width     0.228450  0.265789     0.216466  0.004712  0.237334  0.254684   \n",
       "h2_freq      0.392023  0.388382     0.344145  0.122033  0.393738  0.395322   \n",
       "h2_width    -0.017194  0.129362    -0.062363 -0.153735  0.009274  0.046606   \n",
       "h3_freq      0.287206  0.409930     0.187863 -0.016222  0.305765  0.333426   \n",
       "h3_width    -0.017504  0.086100    -0.051123 -0.102060 -0.004775  0.019158   \n",
       "meanfun      0.220436  0.120606     0.245461  0.202100  0.198423  0.165738   \n",
       "minfun       0.166984  0.068648     0.194146  0.204851  0.136846  0.098713   \n",
       "maxfun       0.245190  0.214819     0.219835  0.090377  0.261679  0.260523   \n",
       "\n",
       "             time.median  time.Q25  time.Q75  time.IQR  ...  HNRVoiced  \\\n",
       "meanfreq       -0.048984 -0.052761 -0.050406 -0.043259  ...  -0.074716   \n",
       "sd             -0.002590 -0.023713  0.009214  0.026402  ...   0.056090   \n",
       "freq.median    -0.065558 -0.059288 -0.075742 -0.076086  ...  -0.115452   \n",
       "freq.Q25       -0.048509 -0.035872 -0.061101 -0.068007  ...  -0.145287   \n",
       "freq.Q75       -0.049437 -0.054377 -0.049123 -0.040516  ...  -0.067475   \n",
       "freq.IQR       -0.041733 -0.050066 -0.038395 -0.027475  ...  -0.038128   \n",
       "time.median     1.000000  0.909278  0.966742  0.886561  ...   0.064073   \n",
       "time.Q25        0.909278  1.000000  0.863110  0.687182  ...   0.062497   \n",
       "time.Q75        0.966742  0.863110  1.000000  0.960001  ...   0.065248   \n",
       "time.IQR        0.886561  0.687182  0.960001  1.000000  ...   0.059211   \n",
       "skew            0.121077  0.099814  0.130949  0.133035  ...   0.009044   \n",
       "kurt            0.176043  0.149828  0.185510  0.183794  ...  -0.006683   \n",
       "sp.ent          0.019966  0.011787  0.024987  0.029410  ...  -0.155043   \n",
       "time.ent        0.815033  0.721833  0.851911  0.825298  ...   0.100071   \n",
       "entropy         0.158355  0.134535  0.169493  0.169231  ...  -0.132786   \n",
       "sfm            -0.056685 -0.061374 -0.052625 -0.041675  ...  -0.039685   \n",
       "meandom        -0.060256 -0.044482 -0.069352 -0.075103  ...  -0.115006   \n",
       "mindom         -0.060566 -0.053839 -0.058794 -0.054727  ...  -0.007365   \n",
       "maxdom          0.074607  0.060053  0.072063  0.070370  ...  -0.000915   \n",
       "dfrange         0.075046  0.060444  0.072489  0.070765  ...  -0.000860   \n",
       "modindx         0.359633  0.301041  0.380467  0.380408  ...  -0.045355   \n",
       "startdom       -0.046748 -0.047837 -0.049261 -0.044342  ...  -0.053915   \n",
       "enddom          0.035176  0.033147  0.034643  0.031458  ...  -0.023967   \n",
       "dfslope         0.059943  0.058986  0.061163  0.055282  ...   0.015822   \n",
       "meanpeakf      -0.055589 -0.035854 -0.056765 -0.061781  ...  -0.052088   \n",
       "HNR             0.057705  0.052876  0.064460  0.063412  ...   0.809671   \n",
       "HNRVoiced       0.064073  0.062497  0.065248  0.059211  ...   1.000000   \n",
       "h1_freq        -0.033618 -0.036356 -0.030774 -0.024112  ...   0.015702   \n",
       "h1_width       -0.016960 -0.018942 -0.019669 -0.017793  ...   0.181581   \n",
       "h2_freq        -0.012271 -0.011750 -0.014312 -0.014074  ...   0.204008   \n",
       "h2_width        0.107722  0.097033  0.107892  0.101408  ...   0.428080   \n",
       "h3_freq         0.044405  0.030945  0.047378  0.050998  ...   0.282699   \n",
       "h3_width        0.081702  0.064165  0.085383  0.087251  ...   0.155827   \n",
       "meanfun        -0.031837 -0.033286 -0.033258 -0.029388  ...   0.060323   \n",
       "minfun         -0.047124 -0.040538 -0.049494 -0.048723  ...   0.071588   \n",
       "maxfun          0.031422  0.021050  0.029995  0.031477  ...   0.045593   \n",
       "\n",
       "              h1_freq  h1_width   h2_freq  h2_width   h3_freq  h3_width  \\\n",
       "meanfreq     0.424484  0.228450  0.392023 -0.017194  0.287206 -0.017504   \n",
       "sd           0.280239  0.265789  0.388382  0.129362  0.409930  0.086100   \n",
       "freq.median  0.449191  0.216466  0.344145 -0.062363  0.187863 -0.051123   \n",
       "freq.Q25     0.326888  0.004712  0.122033 -0.153735 -0.016222 -0.102060   \n",
       "freq.Q75     0.383044  0.237334  0.393738  0.009274  0.305765 -0.004775   \n",
       "freq.IQR     0.335012  0.254684  0.395322  0.046606  0.333426  0.019158   \n",
       "time.median -0.033618 -0.016960 -0.012271  0.107722  0.044405  0.081702   \n",
       "time.Q25    -0.036356 -0.018942 -0.011750  0.097033  0.030945  0.064165   \n",
       "time.Q75    -0.030774 -0.019669 -0.014312  0.107892  0.047378  0.085383   \n",
       "time.IQR    -0.024112 -0.017793 -0.014074  0.101408  0.050998  0.087251   \n",
       "skew         0.135980  0.184477  0.171535  0.152450  0.178778  0.145871   \n",
       "kurt         0.110511  0.138493  0.134121  0.107871  0.139481  0.110717   \n",
       "sp.ent       0.203277  0.034005  0.131759 -0.206377  0.035375 -0.116142   \n",
       "time.ent    -0.049107 -0.054674 -0.044909  0.044423  0.010155  0.023341   \n",
       "entropy      0.188563  0.023839  0.120177 -0.191789  0.036447 -0.108136   \n",
       "sfm          0.257886  0.144412  0.250026 -0.061608  0.204779 -0.029545   \n",
       "meandom      0.167407 -0.067714  0.068728 -0.156469 -0.034698 -0.147827   \n",
       "mindom       0.011951 -0.018563 -0.011234  0.041608 -0.048981 -0.004748   \n",
       "maxdom       0.026850 -0.011226  0.078585 -0.074630  0.053762 -0.087187   \n",
       "dfrange      0.026758 -0.011087  0.078657 -0.074928  0.054117 -0.087139   \n",
       "modindx      0.052035 -0.000546 -0.006268  0.010561 -0.017612  0.003043   \n",
       "startdom     0.049976 -0.022826  0.029751 -0.054640 -0.021503 -0.054098   \n",
       "enddom       0.023863 -0.041264  0.029424 -0.030214  0.007857 -0.069693   \n",
       "dfslope     -0.013305 -0.019006  0.004650  0.011141  0.020697 -0.021829   \n",
       "meanpeakf    0.070148 -0.177781 -0.049197 -0.112825 -0.107520 -0.109978   \n",
       "HNR          0.105990  0.270024  0.280788  0.466794  0.371887  0.205238   \n",
       "HNRVoiced    0.015702  0.181581  0.204008  0.428080  0.282699  0.155827   \n",
       "h1_freq      1.000000  0.626050  0.698041  0.266442  0.616340  0.202439   \n",
       "h1_width     0.626050  1.000000  0.585898  0.450659  0.599937  0.335051   \n",
       "h2_freq      0.698041  0.585898  1.000000  0.467980  0.810984  0.327377   \n",
       "h2_width     0.266442  0.450659  0.467980  1.000000  0.525319  0.514055   \n",
       "h3_freq      0.616340  0.599937  0.810984  0.525319  1.000000  0.453399   \n",
       "h3_width     0.202439  0.335051  0.327377  0.514055  0.453399  1.000000   \n",
       "meanfun      0.482779  0.337911  0.395760  0.113884  0.296338  0.071836   \n",
       "minfun       0.368188  0.245440  0.292860  0.082086  0.212166  0.035671   \n",
       "maxfun       0.415536  0.330699  0.381168  0.141332  0.318235  0.102083   \n",
       "\n",
       "              meanfun    minfun    maxfun  \n",
       "meanfreq     0.220436  0.166984  0.245190  \n",
       "sd           0.120606  0.068648  0.214819  \n",
       "freq.median  0.245461  0.194146  0.219835  \n",
       "freq.Q25     0.202100  0.204851  0.090377  \n",
       "freq.Q75     0.198423  0.136846  0.261679  \n",
       "freq.IQR     0.165738  0.098713  0.260523  \n",
       "time.median -0.031837 -0.047124  0.031422  \n",
       "time.Q25    -0.033286 -0.040538  0.021050  \n",
       "time.Q75    -0.033258 -0.049494  0.029995  \n",
       "time.IQR    -0.029388 -0.048723  0.031477  \n",
       "skew         0.084918  0.055314  0.093586  \n",
       "kurt         0.056589  0.033403  0.072811  \n",
       "sp.ent       0.106739  0.075615  0.116357  \n",
       "time.ent    -0.059670 -0.066791 -0.002039  \n",
       "entropy      0.093029  0.061686  0.112211  \n",
       "sfm          0.097689  0.060142  0.129186  \n",
       "meandom      0.080314  0.096392  0.046154  \n",
       "mindom       0.014688  0.018724  0.001852  \n",
       "maxdom      -0.010599 -0.006543  0.030929  \n",
       "dfrange     -0.010706 -0.006681  0.030911  \n",
       "modindx      0.027612  0.006547  0.049670  \n",
       "startdom     0.018647  0.038858  0.001237  \n",
       "enddom       0.012429  0.011340  0.019421  \n",
       "dfslope     -0.002052 -0.016306  0.015226  \n",
       "meanpeakf    0.064899  0.101624 -0.026067  \n",
       "HNR          0.045604  0.012477  0.077993  \n",
       "HNRVoiced    0.060323  0.071588  0.045593  \n",
       "h1_freq      0.482779  0.368188  0.415536  \n",
       "h1_width     0.337911  0.245440  0.330699  \n",
       "h2_freq      0.395760  0.292860  0.381168  \n",
       "h2_width     0.113884  0.082086  0.141332  \n",
       "h3_freq      0.296338  0.212166  0.318235  \n",
       "h3_width     0.071836  0.035671  0.102083  \n",
       "meanfun      1.000000  0.841752  0.664872  \n",
       "minfun       0.841752  1.000000  0.366812  \n",
       "maxfun       0.664872  0.366812  1.000000  \n",
       "\n",
       "[36 rows x 36 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38f37b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the following function we can select highly correlated features\n",
    "# it will remove the first feature that is correlated with anything other feature\n",
    "\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "959f9cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_features = correlation(x_train, 0.85)\n",
    "len(set(corr_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8e79e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dfrange',\n",
       " 'entropy',\n",
       " 'freq.IQR',\n",
       " 'freq.Q75',\n",
       " 'freq.median',\n",
       " 'kurt',\n",
       " 'time.IQR',\n",
       " 'time.Q25',\n",
       " 'time.Q75',\n",
       " 'time.ent'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c55462f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.drop(corr_features,axis=1)\n",
    "x_test = x_test.drop(corr_features,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b0b1cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00111748, 0.00402493, 0.02228073, 0.01042847, 0.00374663,\n",
       "       0.        , 0.00247098, 0.00625105, 0.        , 0.02498412,\n",
       "       0.00851453, 0.04564673, 0.00118399, 0.02014027, 0.14053755,\n",
       "       0.19107041, 0.2424776 , 0.01836356, 0.01595631, 0.03806756,\n",
       "       0.09964806, 0.07133276, 0.0185774 , 0.29322841, 0.37485997,\n",
       "       0.18878063])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "# determine the mutual information\n",
    "mutual_info = mutual_info_classif(x_train, y_train)\n",
    "mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4be6ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minfun         0.374860\n",
       "meanfun        0.293228\n",
       "HNRVoiced      0.242478\n",
       "HNR            0.191070\n",
       "maxfun         0.188781\n",
       "meanpeakf      0.140538\n",
       "h2_width       0.099648\n",
       "h3_freq        0.071333\n",
       "startdom       0.045647\n",
       "h2_freq        0.038068\n",
       "maxdom         0.024984\n",
       "freq.Q25       0.022281\n",
       "dfslope        0.020140\n",
       "h3_width       0.018577\n",
       "h1_freq        0.018364\n",
       "h1_width       0.015956\n",
       "time.median    0.010428\n",
       "modindx        0.008515\n",
       "meandom        0.006251\n",
       "sd             0.004025\n",
       "skew           0.003747\n",
       "sfm            0.002471\n",
       "enddom         0.001184\n",
       "meanfreq       0.001117\n",
       "sp.ent         0.000000\n",
       "mindom         0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info = pd.Series(mutual_info)\n",
    "mutual_info.index = x_train.columns\n",
    "mutual_info.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c57adaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's plot the ordered mutual_info values per feature\n",
    "mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cb4e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19bb4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    " #select the  top 10 important features\n",
    "sel_ten_cols = SelectKBest(mutual_info_classif, k=10)\n",
    "sel_ten_cols.fit(x_train, y_train)\n",
    "cols_to_keep = x_train.columns[sel_ten_cols.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75755f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['startdom', 'meanpeakf', 'HNR', 'HNRVoiced', 'h2_freq', 'h2_width',\n",
       "       'h3_freq', 'meanfun', 'minfun', 'maxfun'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de39a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reindex(cols_to_keep, axis=1)\n",
    "x_test = x_test.reindex(cols_to_keep, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37c6aaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>startdom</th>\n",
       "      <th>meanpeakf</th>\n",
       "      <th>HNR</th>\n",
       "      <th>HNRVoiced</th>\n",
       "      <th>h2_freq</th>\n",
       "      <th>h2_width</th>\n",
       "      <th>h3_freq</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7186</th>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.110491</td>\n",
       "      <td>9.190898</td>\n",
       "      <td>9.485821</td>\n",
       "      <td>1.913113</td>\n",
       "      <td>0.257564</td>\n",
       "      <td>2.921711</td>\n",
       "      <td>0.334060</td>\n",
       "      <td>0.122327</td>\n",
       "      <td>2.566337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>0.265625</td>\n",
       "      <td>0.205357</td>\n",
       "      <td>6.179222</td>\n",
       "      <td>6.726197</td>\n",
       "      <td>1.720926</td>\n",
       "      <td>0.223096</td>\n",
       "      <td>2.699154</td>\n",
       "      <td>0.165580</td>\n",
       "      <td>0.104096</td>\n",
       "      <td>0.252809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5360</th>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.142113</td>\n",
       "      <td>7.078247</td>\n",
       "      <td>8.274101</td>\n",
       "      <td>1.280967</td>\n",
       "      <td>0.238637</td>\n",
       "      <td>2.275037</td>\n",
       "      <td>0.152522</td>\n",
       "      <td>0.115729</td>\n",
       "      <td>0.184783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3789</th>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.236979</td>\n",
       "      <td>9.179892</td>\n",
       "      <td>10.335894</td>\n",
       "      <td>2.104445</td>\n",
       "      <td>0.241160</td>\n",
       "      <td>3.214985</td>\n",
       "      <td>0.250374</td>\n",
       "      <td>0.211920</td>\n",
       "      <td>0.289992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5797</th>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.078869</td>\n",
       "      <td>10.807859</td>\n",
       "      <td>8.640348</td>\n",
       "      <td>2.220547</td>\n",
       "      <td>0.219394</td>\n",
       "      <td>3.367557</td>\n",
       "      <td>0.124791</td>\n",
       "      <td>0.114176</td>\n",
       "      <td>0.176689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4931</th>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.078869</td>\n",
       "      <td>7.105421</td>\n",
       "      <td>5.464837</td>\n",
       "      <td>1.942066</td>\n",
       "      <td>0.200263</td>\n",
       "      <td>2.568160</td>\n",
       "      <td>0.142758</td>\n",
       "      <td>0.112379</td>\n",
       "      <td>0.279667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>0.421875</td>\n",
       "      <td>0.521577</td>\n",
       "      <td>10.427519</td>\n",
       "      <td>10.499615</td>\n",
       "      <td>1.533689</td>\n",
       "      <td>0.241688</td>\n",
       "      <td>2.865996</td>\n",
       "      <td>0.241547</td>\n",
       "      <td>0.185603</td>\n",
       "      <td>0.295281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.173735</td>\n",
       "      <td>7.068535</td>\n",
       "      <td>8.915303</td>\n",
       "      <td>1.498886</td>\n",
       "      <td>0.331441</td>\n",
       "      <td>2.764725</td>\n",
       "      <td>0.192681</td>\n",
       "      <td>0.165049</td>\n",
       "      <td>0.227489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.142113</td>\n",
       "      <td>7.643514</td>\n",
       "      <td>8.143541</td>\n",
       "      <td>1.905914</td>\n",
       "      <td>0.204819</td>\n",
       "      <td>3.051079</td>\n",
       "      <td>0.169240</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.221152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.426711</td>\n",
       "      <td>8.850634</td>\n",
       "      <td>9.122717</td>\n",
       "      <td>1.479129</td>\n",
       "      <td>0.196188</td>\n",
       "      <td>2.568274</td>\n",
       "      <td>0.208588</td>\n",
       "      <td>0.172518</td>\n",
       "      <td>0.248258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6099 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      startdom  meanpeakf        HNR  HNRVoiced   h2_freq  h2_width   h3_freq  \\\n",
       "7186  0.171875   0.110491   9.190898   9.485821  1.913113  0.257564  2.921711   \n",
       "5753  0.265625   0.205357   6.179222   6.726197  1.720926  0.223096  2.699154   \n",
       "5360  0.546875   0.142113   7.078247   8.274101  1.280967  0.238637  2.275037   \n",
       "3789  0.078125   0.236979   9.179892  10.335894  2.104445  0.241160  3.214985   \n",
       "5797  0.015625   0.078869  10.807859   8.640348  2.220547  0.219394  3.367557   \n",
       "...        ...        ...        ...        ...       ...       ...       ...   \n",
       "4931  0.453125   0.078869   7.105421   5.464837  1.942066  0.200263  2.568160   \n",
       "3264  0.421875   0.521577  10.427519  10.499615  1.533689  0.241688  2.865996   \n",
       "1653  0.234375   0.173735   7.068535   8.915303  1.498886  0.331441  2.764725   \n",
       "2607  0.734375   0.142113   7.643514   8.143541  1.905914  0.204819  3.051079   \n",
       "2732  0.765625   0.426711   8.850634   9.122717  1.479129  0.196188  2.568274   \n",
       "\n",
       "       meanfun    minfun    maxfun  \n",
       "7186  0.334060  0.122327  2.566337  \n",
       "5753  0.165580  0.104096  0.252809  \n",
       "5360  0.152522  0.115729  0.184783  \n",
       "3789  0.250374  0.211920  0.289992  \n",
       "5797  0.124791  0.114176  0.176689  \n",
       "...        ...       ...       ...  \n",
       "4931  0.142758  0.112379  0.279667  \n",
       "3264  0.241547  0.185603  0.295281  \n",
       "1653  0.192681  0.165049  0.227489  \n",
       "2607  0.169240  0.142857  0.221152  \n",
       "2732  0.208588  0.172518  0.248258  \n",
       "\n",
       "[6099 rows x 10 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33bb7224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c9642ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score using LR model is :  80.33%\n",
      "\n",
      "\n",
      "confusion matrix using LR model is : \n",
      " [[652 137]\n",
      " [163 573]]\n",
      "\n",
      "\n",
      "classification using LR model is :\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.81       789\n",
      "           1       0.81      0.78      0.79       736\n",
      "\n",
      "    accuracy                           0.80      1525\n",
      "   macro avg       0.80      0.80      0.80      1525\n",
      "weighted avg       0.80      0.80      0.80      1525\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression model\n",
    "LR_model = LogisticRegression()\n",
    "LR_model.fit(x_train, y_train)\n",
    "LR_y_pred = LR_model.predict(x_test)\n",
    "\n",
    "Accuracy_LR_test = \"{0:.2f}%\".format(accuracy_score(y_test,LR_y_pred)*100)\n",
    "print(\"accuracy score using LR model is : \", Accuracy_LR_test)\n",
    "print('\\n')\n",
    "print(\"confusion matrix using LR model is : \\n\", confusion_matrix(y_test, LR_y_pred))\n",
    "print('\\n')\n",
    "print(\"classification using LR model is :\\n \", classification_report(y_test, LR_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6dc69e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score using XGB model is :  89.70%\n",
      "\n",
      "\n",
      "confusion matrix using XGB model is :\n",
      "  [[715  74]\n",
      " [ 83 653]]\n",
      "\n",
      "\n",
      "classification using XGB model is :\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.90       789\n",
      "           1       0.90      0.89      0.89       736\n",
      "\n",
      "    accuracy                           0.90      1525\n",
      "   macro avg       0.90      0.90      0.90      1525\n",
      "weighted avg       0.90      0.90      0.90      1525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using XGB\n",
    "XGB_model = XGBClassifier(learning_rate=0.8,random_state=0)\n",
    "XGB_model.fit(x_train,y_train)\n",
    "XGB_y_pred = XGB_model.predict(x_test)\n",
    "\n",
    "Accuracy_XGB_test = \"{0:.2f}%\".format(accuracy_score(y_test,XGB_y_pred)*100)\n",
    "\n",
    "print(\"accuracy score using XGB model is : \", Accuracy_XGB_test)\n",
    "print('\\n')\n",
    "print(\"confusion matrix using XGB model is :\\n \", confusion_matrix(y_test, XGB_y_pred))\n",
    "print('\\n')\n",
    "print(\"classification using XGB model is :\\n \", classification_report(y_test, XGB_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10fb5e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score using DT model is :  86.62%\n",
      "\n",
      "\n",
      "confusion matrix using DT model is : \n",
      " [[696  93]\n",
      " [111 625]]\n",
      "\n",
      "\n",
      "classification using DT model is :\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87       789\n",
      "           1       0.87      0.85      0.86       736\n",
      "\n",
      "    accuracy                           0.87      1525\n",
      "   macro avg       0.87      0.87      0.87      1525\n",
      "weighted avg       0.87      0.87      0.87      1525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision tree model\n",
    "DT_model = DecisionTreeClassifier(criterion = 'entropy')\n",
    "DT_model.fit(x_train, y_train)\n",
    "DT_y_pred = DT_model.predict(x_test)\n",
    "\n",
    "Accuracy_DT_test = \"{0:.2f}%\".format(accuracy_score(y_test,DT_y_pred)*100)\n",
    "print(\"accuracy score using DT model is : \", Accuracy_DT_test)\n",
    "print('\\n')\n",
    "print(\"confusion matrix using DT model is : \\n\", confusion_matrix(y_test, DT_y_pred))\n",
    "print('\\n')\n",
    "print(\"classification using DT model is :\\n \", classification_report(y_test, DT_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e671cff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score using SVM model is :  80.72%\n",
      "\n",
      "\n",
      "confusion matrix using SVM model is :\n",
      "  [[664 125]\n",
      " [169 567]]\n",
      "\n",
      "\n",
      "classification using SVM model is : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82       789\n",
      "           1       0.82      0.77      0.79       736\n",
      "\n",
      "    accuracy                           0.81      1525\n",
      "   macro avg       0.81      0.81      0.81      1525\n",
      "weighted avg       0.81      0.81      0.81      1525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using SVM\n",
    "SVM_model = SVC(kernel = 'linear')\n",
    "SVM_model.fit(x_train, y_train)\n",
    "SVM_y_pred = SVM_model.predict(x_test)\n",
    "\n",
    "Accuracy_SVM_test = \"{0:.2f}%\".format(accuracy_score(y_test,SVM_y_pred)*100)\n",
    "print(\"accuracy score using SVM model is : \", Accuracy_SVM_test)\n",
    "print('\\n')\n",
    "print(\"confusion matrix using SVM model is :\\n \", confusion_matrix(y_test, SVM_y_pred))\n",
    "print('\\n')\n",
    "print(\"classification using SVM model is : \\n\", classification_report(y_test, SVM_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d16417f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score using RF model is :  90.56%\n",
      "\n",
      "\n",
      "confusion matrix using RF model is :\n",
      "  [[719  70]\n",
      " [ 74 662]]\n",
      "\n",
      "\n",
      "classification using RF model is :\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       789\n",
      "           1       0.90      0.90      0.90       736\n",
      "\n",
      "    accuracy                           0.91      1525\n",
      "   macro avg       0.91      0.91      0.91      1525\n",
      "weighted avg       0.91      0.91      0.91      1525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using random forest \n",
    "RF_model = RandomForestClassifier(n_estimators = 100, criterion = 'entropy')\n",
    "RF_model.fit(x_train, y_train)\n",
    "RF_y_pred = RF_model.predict(x_test)\n",
    "\n",
    "Accuracy_RF_test = \"{0:.2f}%\".format(accuracy_score(y_test,RF_y_pred)*100)\n",
    "print(\"accuracy score using RF model is : \", Accuracy_RF_test)\n",
    "print('\\n')\n",
    "print(\"confusion matrix using RF model is :\\n \", confusion_matrix(y_test, RF_y_pred))\n",
    "print('\\n')\n",
    "print(\"classification using RF model is :\\n \", classification_report(y_test, RF_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af01f910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score using LR model is :  81.57%\n",
      "\n",
      "\n",
      "confusion matrix using LR model is :\n",
      "  [[657 132]\n",
      " [149 587]]\n",
      "\n",
      "\n",
      "classification using LR model is :\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.82       789\n",
      "           1       0.82      0.80      0.81       736\n",
      "\n",
      "    accuracy                           0.82      1525\n",
      "   macro avg       0.82      0.82      0.82      1525\n",
      "weighted avg       0.82      0.82      0.82      1525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using KNN\n",
    "KNN_model = KNeighborsClassifier(n_neighbors = 7)\n",
    "KNN_model.fit(x_train, y_train)\n",
    "KNN_y_pred = KNN_model.predict(x_test)\n",
    "\n",
    "Accuracy_KNN_test = \"{0:.2f}%\".format(accuracy_score(y_test,KNN_y_pred)*100)\n",
    "print(\"accuracy score using LR model is : \", Accuracy_KNN_test)\n",
    "print('\\n')\n",
    "print(\"confusion matrix using LR model is :\\n \", confusion_matrix(y_test, KNN_y_pred))\n",
    "print('\\n')\n",
    "print(\"classification using LR model is :\\n \", classification_report(y_test, KNN_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3778c10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score using MLP model is :  88.66%\n",
      "\n",
      "\n",
      "confusion matrix using MLP model is :\n",
      "  [[728  61]\n",
      " [112 624]]\n",
      "\n",
      "\n",
      "classification using MLP model is :\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89       789\n",
      "           1       0.91      0.85      0.88       736\n",
      "\n",
      "    accuracy                           0.89      1525\n",
      "   macro avg       0.89      0.89      0.89      1525\n",
      "weighted avg       0.89      0.89      0.89      1525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using MLP\n",
    "MLP_model = MLPClassifier(alpha=0.01,activation='relu', batch_size=64,hidden_layer_sizes=(450,), max_iter=550)\n",
    "MLP_model.fit(x_train,y_train)\n",
    "MLP_y_pred = MLP_model.predict(x_test)\n",
    "\n",
    "Accuracy_MLP_test = \"{0:.2f}%\".format(accuracy_score(y_test,MLP_y_pred)*100)\n",
    "print(\"accuracy score using MLP model is : \", Accuracy_MLP_test)\n",
    "print('\\n')\n",
    "print(\"confusion matrix using MLP model is :\\n \", confusion_matrix(y_test, MLP_y_pred))\n",
    "print('\\n')\n",
    "print(\"classification using MLP model is :\\n \", classification_report(y_test, MLP_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f67d9871",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xlstm_train = x_train.values.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "ylstm_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d17ec62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM RNN model ...\n",
      "Compiling ...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 6099, 128)         71168     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                20608     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6099)              201267    \n",
      "=================================================================\n",
      "Total params: 293,043\n",
      "Trainable params: 293,043\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training ...\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 6099, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, 6099, 10), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 10).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 6099, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, 6099, 10), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 10).\n",
      "48/48 [==============================] - 9s 27ms/step - loss: 7.9858 - accuracy: 0.5011\n",
      "Epoch 2/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 4.2072 - accuracy: 0.5225\n",
      "Epoch 3/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 1.7656 - accuracy: 0.5225\n",
      "Epoch 4/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.9937 - accuracy: 0.5225\n",
      "Epoch 5/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.8336 - accuracy: 0.5225\n",
      "Epoch 6/200\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.7800 - accuracy: 0.5225\n",
      "Epoch 7/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.7542 - accuracy: 0.5225\n",
      "Epoch 8/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.7389 - accuracy: 0.5225\n",
      "Epoch 9/200\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.7299 - accuracy: 0.5225\n",
      "Epoch 10/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.7225 - accuracy: 0.5225\n",
      "Epoch 11/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.7176 - accuracy: 0.5225\n",
      "Epoch 12/200\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.7138 - accuracy: 0.5225\n",
      "Epoch 13/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.7109 - accuracy: 0.5225\n",
      "Epoch 14/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.7085 - accuracy: 0.5225\n",
      "Epoch 15/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.7066 - accuracy: 0.5225\n",
      "Epoch 16/200\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.7053 - accuracy: 0.5225\n",
      "Epoch 17/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.7038 - accuracy: 0.5225\n",
      "Epoch 18/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.7029 - accuracy: 0.5225\n",
      "Epoch 19/200\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.7018 - accuracy: 0.5225\n",
      "Epoch 20/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.7009 - accuracy: 0.5225\n",
      "Epoch 21/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.7002 - accuracy: 0.5225\n",
      "Epoch 22/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.6996 - accuracy: 0.5225\n",
      "Epoch 23/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.6995 - accuracy: 0.5225\n",
      "Epoch 24/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.6986 - accuracy: 0.5225\n",
      "Epoch 25/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.6981 - accuracy: 0.5225\n",
      "Epoch 26/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 0.6978 - accuracy: 0.5225\n",
      "Epoch 27/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.6974 - accuracy: 0.5225\n",
      "Epoch 28/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.6973 - accuracy: 0.5225\n",
      "Epoch 29/200\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6968 - accuracy: 0.5225\n",
      "Epoch 30/200\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.6968 - accuracy: 0.5225\n",
      "Epoch 31/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.6965 - accuracy: 0.5225\n",
      "Epoch 32/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.6961 - accuracy: 0.5225\n",
      "Epoch 33/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 0.6962 - accuracy: 0.5125\n",
      "Epoch 34/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.6964 - accuracy: 0.5124\n",
      "Epoch 35/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.6954 - accuracy: 0.5225\n",
      "Epoch 36/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 0.6886 - accuracy: 0.5334\n",
      "Epoch 37/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.6643 - accuracy: 0.7070\n",
      "Epoch 38/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.6327 - accuracy: 0.7609\n",
      "Epoch 39/200\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.6060 - accuracy: 0.7659\n",
      "Epoch 40/200\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.5832 - accuracy: 0.7772\n",
      "Epoch 41/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.5652 - accuracy: 0.7823\n",
      "Epoch 42/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.5440 - accuracy: 0.7941\n",
      "Epoch 43/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.5332 - accuracy: 0.7921\n",
      "Epoch 44/200\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.5197 - accuracy: 0.7964\n",
      "Epoch 45/200\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.5077 - accuracy: 0.7993\n",
      "Epoch 46/200\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.4876 - accuracy: 0.8026\n",
      "Epoch 47/200\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.4652 - accuracy: 0.8116\n",
      "Epoch 48/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.4521 - accuracy: 0.8210\n",
      "Epoch 49/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.4398 - accuracy: 0.8196\n",
      "Epoch 50/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.4295 - accuracy: 0.8264\n",
      "Epoch 51/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.4231 - accuracy: 0.8273\n",
      "Epoch 52/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.4173 - accuracy: 0.8277\n",
      "Epoch 53/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.4119 - accuracy: 0.8308\n",
      "Epoch 54/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.4014 - accuracy: 0.8385\n",
      "Epoch 55/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.4022 - accuracy: 0.8367\n",
      "Epoch 56/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.4020 - accuracy: 0.8359\n",
      "Epoch 57/200\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.3910 - accuracy: 0.8382\n",
      "Epoch 58/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.3918 - accuracy: 0.8360\n",
      "Epoch 59/200\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.3940 - accuracy: 0.8344\n",
      "Epoch 60/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 0.3856 - accuracy: 0.8410\n",
      "Epoch 61/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.3881 - accuracy: 0.8360\n",
      "Epoch 62/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.3794 - accuracy: 0.8439\n",
      "Epoch 63/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 0.3825 - accuracy: 0.8411\n",
      "Epoch 64/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.3731 - accuracy: 0.8441\n",
      "Epoch 65/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.3772 - accuracy: 0.8413\n",
      "Epoch 66/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.3779 - accuracy: 0.8398\n",
      "Epoch 67/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.3683 - accuracy: 0.8477\n",
      "Epoch 68/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.3673 - accuracy: 0.8470\n",
      "Epoch 69/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.3721 - accuracy: 0.8480\n",
      "Epoch 70/200\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.3658 - accuracy: 0.8464\n",
      "Epoch 71/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.3692 - accuracy: 0.8449\n",
      "Epoch 72/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.3683 - accuracy: 0.8446\n",
      "Epoch 73/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.3602 - accuracy: 0.8511\n",
      "Epoch 74/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.3686 - accuracy: 0.8454\n",
      "Epoch 75/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.3619 - accuracy: 0.8500\n",
      "Epoch 76/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.3606 - accuracy: 0.8439\n",
      "Epoch 77/200\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.3590 - accuracy: 0.8508\n",
      "Epoch 78/200\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.3601 - accuracy: 0.8528\n",
      "Epoch 79/200\n",
      "48/48 [==============================] - 2s 37ms/step - loss: 0.3583 - accuracy: 0.8485\n",
      "Epoch 80/200\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.3540 - accuracy: 0.8510\n",
      "Epoch 81/200\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.3576 - accuracy: 0.8483\n",
      "Epoch 82/200\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.3635 - accuracy: 0.8467\n",
      "Epoch 83/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.3495 - accuracy: 0.8559\n",
      "Epoch 84/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 0.3522 - accuracy: 0.8554\n",
      "Epoch 85/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 0.3479 - accuracy: 0.8562\n",
      "Epoch 86/200\n",
      "48/48 [==============================] - 2s 37ms/step - loss: 0.3490 - accuracy: 0.8541\n",
      "Epoch 87/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 0.3540 - accuracy: 0.8487\n",
      "Epoch 88/200\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 0.3485 - accuracy: 0.8528\n",
      "Epoch 89/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 0.3460 - accuracy: 0.8529\n",
      "Epoch 90/200\n",
      "48/48 [==============================] - 2s 37ms/step - loss: 0.3486 - accuracy: 0.8536\n",
      "Epoch 91/200\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.3494 - accuracy: 0.8501\n",
      "Epoch 92/200\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.3435 - accuracy: 0.8552\n",
      "Epoch 93/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.3501 - accuracy: 0.8537\n",
      "Epoch 94/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.3420 - accuracy: 0.8629\n",
      "Epoch 95/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.3456 - accuracy: 0.8583\n",
      "Epoch 96/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.3435 - accuracy: 0.8577\n",
      "Epoch 97/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.3447 - accuracy: 0.8556\n",
      "Epoch 98/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.3368 - accuracy: 0.8613\n",
      "Epoch 99/200\n",
      "48/48 [==============================] - 2s 37ms/step - loss: 0.3449 - accuracy: 0.8587\n",
      "Epoch 100/200\n",
      "48/48 [==============================] - 2s 39ms/step - loss: 0.3379 - accuracy: 0.8585\n",
      "Epoch 101/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 0.3347 - accuracy: 0.8578\n",
      "Epoch 102/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.3298 - accuracy: 0.8654\n",
      "Epoch 103/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 0.3430 - accuracy: 0.8560\n",
      "Epoch 104/200\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 0.3472 - accuracy: 0.8519\n",
      "Epoch 105/200\n",
      "48/48 [==============================] - 2s 38ms/step - loss: 0.3312 - accuracy: 0.8603\n",
      "Epoch 106/200\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.3335 - accuracy: 0.8596\n",
      "Epoch 107/200\n",
      "48/48 [==============================] - 2s 31ms/step - loss: 0.3327 - accuracy: 0.8652\n",
      "Epoch 108/200\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 0.3352 - accuracy: 0.8608\n",
      "Epoch 109/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 0.3333 - accuracy: 0.8598\n",
      "Epoch 110/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 0.3317 - accuracy: 0.8631\n",
      "Epoch 111/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.3317 - accuracy: 0.8624\n",
      "Epoch 112/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.3306 - accuracy: 0.8603\n",
      "Epoch 113/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.3302 - accuracy: 0.8642\n",
      "Epoch 114/200\n",
      "48/48 [==============================] - 2s 31ms/step - loss: 0.3324 - accuracy: 0.8601\n",
      "Epoch 115/200\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.3284 - accuracy: 0.8611\n",
      "Epoch 116/200\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.3286 - accuracy: 0.8660\n",
      "Epoch 117/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 0.3282 - accuracy: 0.8621\n",
      "Epoch 118/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.3281 - accuracy: 0.8639\n",
      "Epoch 119/200\n",
      "48/48 [==============================] - 2s 39ms/step - loss: 0.3237 - accuracy: 0.8672\n",
      "Epoch 120/200\n",
      "48/48 [==============================] - 2s 37ms/step - loss: 0.3203 - accuracy: 0.8660\n",
      "Epoch 121/200\n",
      "48/48 [==============================] - 2s 37ms/step - loss: 0.3257 - accuracy: 0.8678\n",
      "Epoch 122/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.3211 - accuracy: 0.8633\n",
      "Epoch 123/200\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 0.3212 - accuracy: 0.8669\n",
      "Epoch 124/200\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.3222 - accuracy: 0.8687\n",
      "Epoch 125/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.3170 - accuracy: 0.8719\n",
      "Epoch 126/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.3249 - accuracy: 0.8621\n",
      "Epoch 127/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.3196 - accuracy: 0.8723\n",
      "Epoch 128/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.3128 - accuracy: 0.8723\n",
      "Epoch 129/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.3122 - accuracy: 0.8698\n",
      "Epoch 130/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.3065 - accuracy: 0.8726\n",
      "Epoch 131/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.3095 - accuracy: 0.8770\n",
      "Epoch 132/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 0.3046 - accuracy: 0.8774\n",
      "Epoch 133/200\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.3019 - accuracy: 0.8793\n",
      "Epoch 134/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.3099 - accuracy: 0.8764\n",
      "Epoch 135/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.3016 - accuracy: 0.8800\n",
      "Epoch 136/200\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.3062 - accuracy: 0.8746\n",
      "Epoch 137/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.2985 - accuracy: 0.8803\n",
      "Epoch 138/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2954 - accuracy: 0.8834\n",
      "Epoch 139/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2930 - accuracy: 0.8828\n",
      "Epoch 140/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.3030 - accuracy: 0.8828\n",
      "Epoch 141/200\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.2899 - accuracy: 0.8865\n",
      "Epoch 142/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.2892 - accuracy: 0.8887\n",
      "Epoch 143/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 0.2946 - accuracy: 0.8833\n",
      "Epoch 144/200\n",
      "48/48 [==============================] - 2s 39ms/step - loss: 0.2905 - accuracy: 0.8862\n",
      "Epoch 145/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 0.2902 - accuracy: 0.8854\n",
      "Epoch 146/200\n",
      "48/48 [==============================] - 1s 26ms/step - loss: 0.2847 - accuracy: 0.8910\n",
      "Epoch 147/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.2927 - accuracy: 0.8874\n",
      "Epoch 148/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.2821 - accuracy: 0.8880\n",
      "Epoch 149/200\n",
      "48/48 [==============================] - 2s 31ms/step - loss: 0.2846 - accuracy: 0.8900\n",
      "Epoch 150/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.2806 - accuracy: 0.8919\n",
      "Epoch 151/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 0.2827 - accuracy: 0.8883\n",
      "Epoch 152/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.2734 - accuracy: 0.8947\n",
      "Epoch 153/200\n",
      "48/48 [==============================] - 2s 31ms/step - loss: 0.2805 - accuracy: 0.8926\n",
      "Epoch 154/200\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.2793 - accuracy: 0.8880\n",
      "Epoch 155/200\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 0.2793 - accuracy: 0.8903\n",
      "Epoch 156/200\n",
      "48/48 [==============================] - 1s 27ms/step - loss: 0.2727 - accuracy: 0.8951\n",
      "Epoch 157/200\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 0.2764 - accuracy: 0.8916\n",
      "Epoch 158/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 0.2777 - accuracy: 0.8936\n",
      "Epoch 159/200\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.2773 - accuracy: 0.8938\n",
      "Epoch 160/200\n",
      "48/48 [==============================] - 2s 31ms/step - loss: 0.2753 - accuracy: 0.8923\n",
      "Epoch 161/200\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.2767 - accuracy: 0.8936\n",
      "Epoch 162/200\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.2743 - accuracy: 0.8931\n",
      "Epoch 163/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 0.2708 - accuracy: 0.8974\n",
      "Epoch 164/200\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.2758 - accuracy: 0.8928\n",
      "Epoch 165/200\n",
      "48/48 [==============================] - 2s 37ms/step - loss: 0.2785 - accuracy: 0.8875\n",
      "Epoch 166/200\n",
      "48/48 [==============================] - 2s 37ms/step - loss: 0.2753 - accuracy: 0.8908\n",
      "Epoch 167/200\n",
      "48/48 [==============================] - 2s 38ms/step - loss: 0.2726 - accuracy: 0.8893\n",
      "Epoch 168/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 0.2696 - accuracy: 0.8957\n",
      "Epoch 169/200\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 0.2642 - accuracy: 0.8933\n",
      "Epoch 170/200\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 0.2670 - accuracy: 0.8965\n",
      "Epoch 171/200\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 0.2680 - accuracy: 0.8957\n",
      "Epoch 172/200\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 0.2646 - accuracy: 0.8980\n",
      "Epoch 173/200\n",
      "48/48 [==============================] - 2s 31ms/step - loss: 0.2741 - accuracy: 0.8936\n",
      "Epoch 174/200\n",
      "48/48 [==============================] - 2s 38ms/step - loss: 0.2689 - accuracy: 0.8951\n",
      "Epoch 175/200\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.2611 - accuracy: 0.8979\n",
      "Epoch 176/200\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 0.2648 - accuracy: 0.8985\n",
      "Epoch 177/200\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.2707 - accuracy: 0.8969\n",
      "Epoch 178/200\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.2769 - accuracy: 0.8908\n",
      "Epoch 179/200\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 0.2699 - accuracy: 0.8983\n",
      "Epoch 180/200\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.2666 - accuracy: 0.8975\n",
      "Epoch 181/200\n",
      "48/48 [==============================] - 2s 43ms/step - loss: 0.2605 - accuracy: 0.9011\n",
      "Epoch 182/200\n",
      "48/48 [==============================] - 2s 38ms/step - loss: 0.2673 - accuracy: 0.8954\n",
      "Epoch 183/200\n",
      "48/48 [==============================] - 2s 41ms/step - loss: 0.2644 - accuracy: 0.8987\n",
      "Epoch 184/200\n",
      "48/48 [==============================] - 2s 52ms/step - loss: 0.2626 - accuracy: 0.9008\n",
      "Epoch 185/200\n",
      "48/48 [==============================] - 2s 50ms/step - loss: 0.2669 - accuracy: 0.8979\n",
      "Epoch 186/200\n",
      "48/48 [==============================] - 3s 57ms/step - loss: 0.2566 - accuracy: 0.9005\n",
      "Epoch 187/200\n",
      "48/48 [==============================] - 3s 55ms/step - loss: 0.2566 - accuracy: 0.9021\n",
      "Epoch 188/200\n",
      "48/48 [==============================] - 2s 52ms/step - loss: 0.2581 - accuracy: 0.9021\n",
      "Epoch 189/200\n",
      "48/48 [==============================] - 2s 49ms/step - loss: 0.2642 - accuracy: 0.8979\n",
      "Epoch 190/200\n",
      "48/48 [==============================] - 3s 57ms/step - loss: 0.2577 - accuracy: 0.9003\n",
      "Epoch 191/200\n",
      "48/48 [==============================] - 3s 56ms/step - loss: 0.2628 - accuracy: 0.9018\n",
      "Epoch 192/200\n",
      "48/48 [==============================] - 3s 58ms/step - loss: 0.2600 - accuracy: 0.8982\n",
      "Epoch 193/200\n",
      "48/48 [==============================] - 2s 49ms/step - loss: 0.2586 - accuracy: 0.9005\n",
      "Epoch 194/200\n",
      "48/48 [==============================] - 2s 48ms/step - loss: 0.2565 - accuracy: 0.9038\n",
      "Epoch 195/200\n",
      "48/48 [==============================] - 2s 45ms/step - loss: 0.2530 - accuracy: 0.9061\n",
      "Epoch 196/200\n",
      "48/48 [==============================] - 2s 51ms/step - loss: 0.2566 - accuracy: 0.9010\n",
      "Epoch 197/200\n",
      "48/48 [==============================] - 2s 51ms/step - loss: 0.2634 - accuracy: 0.8995\n",
      "Epoch 198/200\n",
      "48/48 [==============================] - 3s 53ms/step - loss: 0.2585 - accuracy: 0.8977\n",
      "Epoch 199/200\n",
      "48/48 [==============================] - 2s 46ms/step - loss: 0.2577 - accuracy: 0.9020\n",
      "Epoch 200/200\n",
      "48/48 [==============================] - 2s 48ms/step - loss: 0.2618 - accuracy: 0.9020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd7a1cb28e0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keras\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "input_shape = (x_train.shape[0], x_train.shape[1])\n",
    "print(\"Build LSTM RNN model ...\")\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=128, dropout=0.05, recurrent_dropout=0.35, return_sequences=True, input_shape=input_shape))\n",
    "model.add(LSTM(units=32,  dropout=0.05, recurrent_dropout=0.35, return_sequences=False))\n",
    "model.add(Dense(units=y_train.shape[0], activation=\"softmax\"))\n",
    "\n",
    "print(\"Compiling ...\")\n",
    "# Keras optimizer defaults:\n",
    "# Adam   : lr=0.001, beta_1=0.9,  beta_2=0.999, epsilon=1e-8, decay=0.\n",
    "# RMSprop: lr=0.001, rho=0.9,                   epsilon=1e-8, decay=0.\n",
    "# SGD    : lr=0.01,  momentum=0.,                             decay=0.\n",
    "opt = Adam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "print(\"Training ...\")\n",
    "batch_size = 128  # num of training examples per minibatch\n",
    "num_epochs = 200\n",
    "\n",
    "model.fit(Xlstm_train, ylstm_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dcea925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing ...\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 6099, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, 6099, 10), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 10).\n",
      "12/12 [==============================] - 2s 24ms/step - loss: 0.2828 - accuracy: 0.8898\n"
     ]
    }
   ],
   "source": [
    "x_test = x_test.values.reshape(x_test.shape[0], 1, x_test.shape[1])\n",
    "y_test = np.array(y_test)\n",
    "print(\"\\nTesting ...\")\n",
    "score, accuracy = model.evaluate(\n",
    "    x_test, y_test, batch_size=batch_size, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "055318f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score using lstm model is :  88.98%\n"
     ]
    }
   ],
   "source": [
    "Accuracy_LSTM_test = \"{0:.2f}%\".format(accuracy*100)\n",
    "print(\"accuracy score using lstm model is : \", Accuracy_LSTM_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a2e0ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>80.33%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>80.72%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>86.62%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP</td>\n",
       "      <td>88.66%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN</td>\n",
       "      <td>81.57%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>90.56%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGB</td>\n",
       "      <td>89.70%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>88.98%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model Accuracy\n",
       "0  Logistic Regression   80.33%\n",
       "1                  SVM   80.72%\n",
       "2        Decision Tree   86.62%\n",
       "3                  MLP   88.66%\n",
       "4                  KNN   81.57%\n",
       "5        Random Forest   90.56%\n",
       "6                  XGB   89.70%\n",
       "7                 LSTM   88.98%"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table=pd.DataFrame()\n",
    "table[\"Model\"]=('Logistic Regression','SVM','Decision Tree','MLP','KNN','Random Forest','XGB','LSTM')\n",
    "table[\"Accuracy\"]=(Accuracy_LR_test,Accuracy_SVM_test,Accuracy_DT_test,Accuracy_MLP_test,Accuracy_KNN_test,Accuracy_RF_test,Accuracy_XGB_test,Accuracy_LSTM_test)\n",
    "table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
